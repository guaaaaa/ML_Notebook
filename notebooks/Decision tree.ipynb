{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e332da",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "A decision tree is a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes that is used to make predictions.\n",
    "\n",
    "Decision nodes: evaluates and split the the data based on features\n",
    "\n",
    "Leaf nodes: represents all the possible the outputs of the model\n",
    "\n",
    "<img src=\"https://i0.wp.com/why-change.com/wp-content/uploads/2021/11/Decision-Tree-elements-2.png?resize=715%2C450&ssl=1\">\n",
    "\n",
    "## Measuring purity\n",
    "Entropy ($H(p)$): a value between 0 and 1 that measures the impurity of a set of data. A high entropy means the data set less homogenous\n",
    "\n",
    "$p_n$: the ratio of $n$th class in a set of data\n",
    "\n",
    "For binary classification\n",
    "$$H(p) = -p_0log_2(p_0)-p_1log_2(p_1)$$\n",
    "where,\n",
    "$$p_0 = 1 - p_1$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pLl6EiI4KRyf3ClAgFrqHA.png\" width=500>\n",
    "Note: $0log(0) = 0$ for simplicity\n",
    "\n",
    "\n",
    "## Training\n",
    "Decision:\n",
    "* Which feature to use at each node to maximize purity (aiming for only one class at each node)\n",
    "* When to stop splitting (decide based on purity score, tree depth, number of training examples in each node, etc)\n",
    "\n",
    "## Information gain\n",
    "Information gain measures the change in entropy after each split. For each split, we aims to reduce the entropy the most, which increases the purity at each node (highest information gain)\n",
    "\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- \\left(w^{\\text{left}}H\\left(p_1^\\text{left}\\right) + w^{\\text{right}}H\\left(p_1^\\text{right}\\right)\\right),$$\n",
    "\n",
    "$H(p_1^\\text{node})$: the entropy at the node above\n",
    "\n",
    "$w$: the fraction of training example that has been splitted into each node\n",
    "$$w = \\frac{\\text{number of training examples in the current node}}{\\text{number of training examples in the node above}}$$\n",
    "\n",
    "$H\\left(p_1^\\text{left}\\right), H\\left(p_1^\\text{right}\\right)$: entropy at the right and left nodes\n",
    "\n",
    "\n",
    "## Stopping criteria\n",
    "We can stop splitting if\n",
    "* When a node if 100% pure\n",
    "* When a max depth of the tree is reached\n",
    "* When information gain from a split is too small (no significant improvement on the model)\n",
    "* When the number of example in a node is below a threshold\n",
    "\n",
    "## Building decision tree\n",
    "Steps:\n",
    "1. Starting with all training examples at the root node\n",
    "2. Calculate the information gain for each feature and picked the one that has the highest information gain as the spllting feature\n",
    "3. Split the training examples based on the feature chosen\n",
    "4. Repeat from step 1 at each node until a stoping criteria is met\n",
    "\n",
    "This is a recursive algorithm by building a large decision tree from smaller ones, stopping criteria will be the base case\n",
    "\n",
    "## One hot encoding\n",
    "One hot encoding resolves the case when a feature can take on more than two values. This method converts textual features to binary features, which can be feed as inputs to other models as well\n",
    "\n",
    "One hot encoding: if a feature can take on $k$ different values, we create $k$ binary features that can only be true or false (0 or 1) to replace the old feature\n",
    "\n",
    "## Continuous valued features\n",
    "Some features can take in a range of values. In order to split data based on continuous values, we need to pick a threshold value for splitting. This can be done by selecting a midpoint between two adjacent values in the training examples and calcualte the information gain. Repeat this process for each two adjacent value in the trianing examples and split at the value that has the highest information gain\n",
    "\n",
    "# Regression tree\n",
    "Instead of predicting a specific class, regression tree predicts a nubmer as output.\n",
    "\n",
    "Splitting: instead aiming to reduce the entropy, each split aims to reduce the variance of the data as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75ec63",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56feb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0244ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80fd6568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate entropy\n",
    "def entropy(p):\n",
    "    # if a class is pure, entropy is 0\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    \n",
    "    e = - p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "    return e\n",
    "\n",
    "entropy(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0b25cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 4, 5, 7], [1, 2, 6, 8, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training examples\n",
    "def split(X, feature_index):\n",
    "    left = []\n",
    "    right = []\n",
    "    \n",
    "    for i, x in enumerate(X): # enumerate return in (index, content) form\n",
    "        if x[feature_index] == 1:\n",
    "          left.append(i) # splitting training examples based on their index\n",
    "        else:\n",
    "          right.append(i)\n",
    "    return left, right\n",
    "          \n",
    "split(X_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a1717aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted entropy\n",
    "def weighted_entropy(X, y, left, right):\n",
    "    w_l = len(left) / len(X)\n",
    "    w_r = len(right) / len(X)\n",
    "    p_l = sum(y[left]) / len(left)   # get the ratio of a class on the left\n",
    "    p_r = sum(y[right]) / len(right) # get the ratio of a class on the right\n",
    "    \n",
    "    w_entropy = w_l * entropy(p_l) + w_r * entropy(p_r)\n",
    "    return w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b13bcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "left_indices, right_indices = split(X_train, 0)\n",
    "weighted_entropy(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53743ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(X, y, left, right):\n",
    "    entropy_reduction = entropy(sum(y)/len(y)) - weighted_entropy(X, y, left, right)\n",
    "    \n",
    "    return entropy_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16ad9800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2780719051126377"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "info_gain(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbd6de",
   "metadata": {},
   "source": [
    "# Tree ensembles\n",
    "Issues: a single decision tree is highly sensitive to small changes to data\n",
    "\n",
    "Tree ensembles: multiple decision trees that will be used for prediction, each tree will make a prediction and vote for a final result \n",
    "\n",
    "## Sampling with replacement\n",
    "\n",
    "## Random forest algorithm\n",
    "For a training set with $m$ training examples, use sampling with replacement to create $B$ number of similar training set, all with $m$ training examples. For each training set, build a decision tree. Then, form a tree emsemble with $B$ trees\n",
    "\n",
    "During training, when choosing feature for splitting at each node, if $n$ features are available, pick a random subset of $k < n$ (eg. $k = \\sqrt n$) features, and select the feature with the highest information gain within the subset with $k$ features\n",
    "\n",
    "This ensures a change in the training exmaple will not impact the prediction as much\n",
    "\n",
    "## XGBoost\n",
    "When picking a training example with sampling with replacment, instead of picking all examples with equal probability, we increase the probability of picking missclassified examples by previous trees. This improves the algorithm by focusing on the examples that the model is not doing well on\n",
    "\n",
    "XGBoost is\n",
    "* Fast to implement\n",
    "* Automatically choose splitting and stopping criteria\n",
    "* Use regularization to prevent overfitting\n",
    "\n",
    "# Decision tree vs Neural network\n",
    "* Decision trees: perform well on structured data (spread sheet format), but not well on unstructured data (eg. images, audios, texts, etc). Also, decision trees are faster to train than neural network\n",
    "* Neural network: perform well on all types of data, can be used for transfer learning, and stringed together, but is slower to train in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510f992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
