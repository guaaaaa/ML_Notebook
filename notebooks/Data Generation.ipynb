{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931305d6",
   "metadata": {},
   "source": [
    "# The Data Generation Problem\n",
    "Data comes in many modalities, such as text, images, audio, and video. We denote a generic data sample by the mathematical object $x$.\n",
    "\n",
    "### Data Space\n",
    "A data sample $x$ belongs to a known data space, denoted as $\\mathcal{X}$ (i.e., $x \\in \\mathcal{X}$). Essentially, the data space is defined as a high dimensional space that contains all possible value combinations of the raw data representation. However, not all points within this high-dimensional space represent valid or meaningful data.\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset of $5 \\times 5$ black & white images of digits. If each pixel can take a binary value $\\{0, 1\\}$, the data space is $\\mathcal{X} = \\{0, 1\\}^{5 \\times 5}$. This space contains $2^{25}$ possible combinations. However, the vast majority of these points look like random noise; only a tiny subset actually forms coherent, recognizable digits, which are what we considered as valid data points.\n",
    "\n",
    "### Data Distribution\n",
    "The data distribution, denoted as $p(x)$, describes the probability of observing a specific sample $x$ from the space $\\mathcal{X}$. This distribution is the key to differentiating meaningful data from noise in the data space\n",
    "* Valid/Meaningful samples are assigned high probability.\n",
    "* Invalid samples are assigned a probability near or equal to zero.\n",
    "\n",
    "In essence, $p(x)$ mathematically represents the hidden patterns and structure governing the data.\n",
    "\n",
    "### Dataset\n",
    "A dataset is a finite collection of samples $\\{x_1, x_2, ..., x_n\\}$ that have been drawn (sampled) from the underlying data distribution $p(x)$.\n",
    "\n",
    "Data spaces are typically high-dimensional and mathematically intractable. It is impossible to capture or enumerate every point in the space because the volume is too vast. However, valid data usually occupies a very small, concentrated region within this vast space (often referred to as a \"manifold\"). By modeling the data distribution $p(x)$, we can focus purely on these high-probability regions and ignore the massive regions of the data space that contain invalid noise.\n",
    "\n",
    "### Goal of Generation\n",
    "In generative modeling, the goal is to learn a target data distribution $p(x)$, either explicitly (learning the density function) or implicitly (learning a mechanism to generate samples).\n",
    "\n",
    "We aim to construct a model that, after observing a limited training dataset, learns to sample new, unique data points that look as if they were drawn from the original distribution $p(x)$, although the original distribution is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec25dd",
   "metadata": {},
   "source": [
    "# Naive bayes\n",
    "Naive bayes is an algorithm that can be used for classification tasks based on the Bayes' theorem. It is used on a variety of datasets, including text data, image data, and numerical data\n",
    "\n",
    "The naive bayes algorithm works by calculating the conditional probability of each class given the input features as conditions. The class with the highest probability is then predicted as the output\n",
    "\n",
    "# Bayes' theorem\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{\\text{True positives}}{\\text{True positives + False positives}}$$\n",
    "\n",
    "Interpretations: the probability of event B to occur given the occurrence of event A\n",
    "\n",
    "Assumptions: the bayes' theorem assumes that the input features are conditionally independent from each other\n",
    "\n",
    "# ML version\n",
    "$$P(y_k|X) = \\frac{P(X|y_k)P(y)}{P(X)} = \\frac{P(x_1|y-k)\\cdot P(x_2|y_k)\\cdots \\dot P(x_n|y_k)\\cdot P(y_k)}{P(X)} = \\frac{P(y_k)\\cdot \\Pi^{n}_{i=1}P(x_i|y_k)}{P(X)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$P(y_k) = \\frac{\\sum^{m}_{i=1}(y_i=y_k)}{m}$$\n",
    "\n",
    "$$P(x_i = a_j|y_k) = \\frac{\\sum^{m}_{i=1}(x_i=a_j \\text{&} y_i=y_k)}{\\sum^{m}_{i=1}(y_i=y_k)}$$\n",
    "\n",
    "\n",
    "$P(y_k|X)$: the probability of the $k$th class, $y_k$, given the input features, $X$, which contains $n$ features ($x_1, x_2, \\cdots, x_n$)\n",
    "\n",
    "$P(y_k)$: the probability of the $k$th class (Prior probability), which equals the frequency of the $k$th class appears in all training examples, $m$\n",
    "\n",
    "$P(x_i|y_k)$: the probaility of feature, $x_i$ given $y_k$ as the condition (conditioinal proability), which equals the number of cases when $x_i$ equals to $a_j$ and $y_i$ equals to $y_k$ divided by the number of cases when $y_i$ equals to $y_k$\n",
    "\n",
    "To make predictions, we select the class with the highest probability, $P(y_k|X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b19145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
