{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931305d6",
   "metadata": {},
   "source": [
    "# The Data Generation Problem\n",
    "Data comes in many modalities, such as text, images, audio, and video. We denote a generic data sample by the mathematical object $x$.\n",
    "\n",
    "### Data Space\n",
    "A data sample $x$ belongs to a known data space, denoted as $\\mathcal{X}$ (i.e., $x \\in \\mathcal{X}$). Essentially, the data space is defined as a high dimensional space that contains all possible value combinations of the raw data representation. However, not all points within this high-dimensional space represent valid or meaningful data.\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset of $5 \\times 5$ black & white images of digits. If each pixel can take a binary value $\\{0, 1\\}$, the data space is $\\mathcal{X} = \\{0, 1\\}^{5 \\times 5}$. This space contains $2^{25}$ possible combinations. However, the vast majority of these points look like random noise; only a tiny subset actually forms coherent, recognizable digits, which are what we considered as valid data points.\n",
    "\n",
    "### Data Distribution\n",
    "The data distribution, denoted as $p(x)$, describes the probability of observing a specific sample $x$ from the space $\\mathcal{X}$. This distribution is the key to differentiating meaningful data from noise in the data space\n",
    "* Valid/Meaningful samples are assigned high probability.\n",
    "* Invalid samples are assigned a probability near or equal to zero.\n",
    "\n",
    "In essence, $p(x)$ mathematically represents the hidden patterns and structure governing the data.\n",
    "\n",
    "### Dataset\n",
    "A dataset is a finite collection of samples $\\{x_1, x_2, ..., x_n\\}$ that have been drawn (sampled) from the underlying data distribution $p(x)$.\n",
    "\n",
    "Data spaces are typically high-dimensional and mathematically intractable. It is impossible to capture or enumerate every point in the space because the volume is too vast. However, valid data usually occupies a very small, concentrated region within this vast space (often referred to as a \"manifold\"). By modeling the data distribution $p(x)$, we can focus purely on these high-probability regions and ignore the massive regions of the data space that contain invalid noise.\n",
    "\n",
    "### Goal of Generation\n",
    "In generative modeling, the goal is to learn a target data distribution $p(x)$, either explicitly (learning the density function) or implicitly (learning a mechanism to generate samples).\n",
    "\n",
    "We aim to construct a model that, after observing a limited training dataset, learns to sample new, unique data points that look as if they were drawn from the original distribution $p(x)$, although the original distribution is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4d2e8",
   "metadata": {},
   "source": [
    "# Discriminative and Generative Models\n",
    "### Discriminative Models\n",
    "For supervised learning tasks with data $(x, y)$, a discriminative model learns to directly estimate the conditional probability $P(y|x)$. This approach learns a decision boundary between different data points, which is effectively a direct mapping from an input to an output. Because they focus solely on the boundary, discriminative models do not explicitly model the underlying distribution of the input data, meaning they simply learn how to separate classes to minimize the loss but do not understand what the data actually looks like.\n",
    "\n",
    "### Generative Models\n",
    "A generative model aims to learn the complete data distribution. For a dataset without labels, it learns the distribution $P(x)$, and for a labelled dataset, it learns the joint distribution $P(x, y)$. This is often understood through the factorization $P(x|y)P(y)$, meaning the model learns \"what the data looks like\" (the likelihood $P(x|y)$) given a specific label.\n",
    "\n",
    "This understanding allows generative models to perform both types of tasks:\n",
    "* Generative Task: For a desired label $y$, the model can generate new synthetic features $x$ by sampling from $P(x|y)$.\n",
    "* Discriminative Task: To classify a new input $x$, the model uses Bayes' Rule to calculate the posterior probability $P(y|x) \\propto P(x|y)P(y)$, allowing it to determine the most likely label.\n",
    "\n",
    "<img src=\"https://cdn.prod.website-files.com/65d8ee5f025f02594c614c17/66d088d78f87048868f94fee_66d085ede64cc943387d4695_1.webp\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd979150",
   "metadata": {},
   "source": [
    "# Bayes' Theorem in Machine Learning\n",
    "Given labelled data $(x, y)$:\n",
    "\n",
    "$$P(y|x) = \\frac{P(x|y)P(y)}{P(x)} = \\frac{P(x|y)P(y)}{\\sum_{k} P(x|y_k)P(y_k)}$$\n",
    "\n",
    "* **$P(x|y)$ (Likelihood):** The probability of observing feature $x$ given that it belongs to class $y$. This is what the generative model explicitly learns (the distribution of the data for each class). This model can be learned either statistically or through a neural network.\n",
    "* **$P(y)$ (Prior):** The probability of a class $y$ occurring before seeing any specific data. This represents our prior belief or assumption. It is often easy to compute by calculating the frequency of each class in the dataset.\n",
    "* **$P(x)$ (Evidence / Marginal):** The total probability of observing feature $x$ regardless of the class. \n",
    "    * Calculated as $\\sum_{k} P(x|y_k)P(y_k)$ (summing the weighted likelihoods over all possible classes). \n",
    "    * In classification, this acts as a normalization constant to ensure the posterior probabilities sum to 1.\n",
    "    * *Note:* While easy to compute in simple classification tasks with few classes, calculating $P(x)$ becomes computationally intractable in complex generative models with high-dimensional or continuous latent variables. Therefore, in classification tasks, we can simply ignore it and find the prediction by\n",
    "    $$ \\text{Prediction} = \\arg\\max_y \\big( P(x|y)P(y) \\big)$$\n",
    "    In generative tasks, we will have to approximate it.\n",
    "* **$P(y|x)$ (Posterior):** The probability that the data belongs to class $y$ given the observed feature $x$. This is the final prediction we want to make.\n",
    "\n",
    "In discriminative approach, the model learns $P(y|x)$ directly (the mapping from input to class) without modeling the underlying data distribution. In generative approach, the model learns the likelihood $P(x|y)$ (the data distribution per class) and the prior $P(y)$, then combines them using Bayes' rule to compute the posterior $P(y|x)$ for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec25dd",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "In the standard application of Bayes' rule, the likelihood term $P(x_1, x_2, \\dots, x_n | y)$ represents the joint probability of all features occurring together given the class. Computing this is difficult because features often have complex correlations (dependencies), requiring a massive amount of data to model accurately.\n",
    "\n",
    "Therefore, Naive Bayes simplifies the model by introducing the assumption of conditional independence. This assumes that, *given the class label*, knowing the value of one feature provides no information about the value of any other feature.\n",
    "\n",
    "This assumption allows us to decompose the complex joint likelihood into a simple product of individual probabilities:\n",
    "\n",
    "$$P(x|y) = P(x_1, x_2, \\dots, x_n | y) \\approx \\prod_{i=1}^{n}P(x_i | y)$$\n",
    "\n",
    "Consequently, the posterior probability for a class $y$ is proportional to:\n",
    "\n",
    "$$P(y|x) \\propto P(y) \\cdot \\prod_{i=1}^{n}P(x_i | y)$$\n",
    "\n",
    "Note: This is a strong, naive assumption because real-world features are rarely independent. However, despite this theoretical flaw, Naive Bayes is computationally efficient and often performs surprisingly well as a baseline classifier. While its probability estimates might be inaccurate due to the assumption, its classification decisions are often correct.\n",
    "\n",
    "### Example\n",
    "**Scenario:**\n",
    "We have a binary classification problem where the class $y$ can be either **Spam (1)** or **Not Spam (0)**. Our data has two features, $x_1$ and $x_2$.\n",
    "\n",
    "We want to classify a new sample where $x_1=a$ and $x_2=b$. To do this, we calculate the posterior probability for both classes and pick the highest one.\n",
    "\n",
    "**Step 1: Setup the equation for Class 1 (Spam)**\n",
    "Using the Naive Bayes assumption, we expand the formula:\n",
    "$$P(y=1 | x_1=a, x_2=b) \\propto P(y=1) \\cdot P(x_1=a | y=1) \\cdot P(x_2=b | y=1)$$\n",
    "\n",
    "**Step 2: \"Learning\" by Counting**\n",
    "We estimate these values directly from our training data frequencies:\n",
    "* $P(y=1)$: How frequent is Spam overall? (Prior)\n",
    "* $P(x_1=a | y=1)$: In Spam messages, how often does feature $x_1$ equal $a$? (Likelihood 1)\n",
    "* $P(x_2=b | y=1)$: In Spam messages, how often does feature $x_2$ equal $b$? (Likelihood 2)\n",
    "\n",
    "**Step 3: Comparison**\n",
    "We repeat the calculation for **Class 0 (Not Spam)**:\n",
    "$$P(y=0 | x_1=a, x_2=b) \\propto P(y=0) \\cdot P(x_1=a | y=0) \\cdot P(x_2=b | y=0)$$\n",
    "\n",
    "**Step 4: Decision**\n",
    "We compare the two resulting scores.\n",
    "* If Score(Spam) > Score(Not Spam), we classify the sample as Spam.\n",
    "\n",
    "\n",
    "The key distinction lies in how we treat the features $x_1$ and $x_2$. Without the Naive assumption, we must treat the features as a coupled pair $(x_1, x_2)$. To calculate the likelihood $P(x_1=a, x_2=b | y)$, we would need to count how often the exact combination **\"a AND b\"** appears together in the training data. However, if the specific pair $(a, b)$ never appeared in the training set (sparsity), the probability becomes 0, and the model fails to make a prediction.\n",
    "\n",
    "Naive Bayes assumes $x_1$ and $x_2$ are independent given the class. We treat them separately\n",
    "1.  Calculate $P(x_1=a | y)$ by looking only at the $x_1$ column.\n",
    "2.  Calculate $P(x_2=b | y)$ by looking only at the $x_2$ column.\n",
    "3.  Multiply the results.\n",
    "\n",
    "This allows the model to predict the probability of a combination $(a, b)$ even if it has never physically seen those two features appear together in history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9faa2",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "In the context of generative modeling, our goal is to approximate an unknown underlying data distribution $P_{data}(\\mathbf{x})$ using a parametric model $P_{\\theta}(\\mathbf{x})$, where $\\theta$ represents the model parameters.\n",
    "\n",
    "### Definition of Likelihood\n",
    "Before defining the estimator, we must define the **Likelihood Function**. Given a set of observed data, the likelihood function $\\mathcal{L}(\\theta)$ measures how well a model (specific parameter set $\\theta$) explains the observed data.\n",
    "\n",
    "> **Definition:** Let $\\mathcal{D} = \\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(n)}\\}$ be a dataset. The likelihood function $L(\\theta; \\mathcal{D})$ is defined as the joint probability (or joint density) of the observed data viewed as a function of the parameters $\\theta$, which is the probablity of observing all data samples together based on the given model:\n",
    "> $$L(\\theta; \\mathcal{D}) = P(\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(n)} | \\theta)$$\n",
    "\n",
    "To make the computation of the joint probability tractable, we typically assume the data points are **independent and identically distributed (i.i.d.)**. Under this assumption, we treat each sample individually, so the joint probability factorizes into the product of marginal probabilities:\n",
    "$$L(\\theta; \\mathcal{D}) = \\prod_{i=1}^{n} P_{\\theta}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "## Log-Likelihood\n",
    "In practice, we work with the **Log-Likelihood** $\\ell(\\theta)$ because the product of many small probabilities leads to numerical underflow. Since the logarithm is a monotonically increasing function, maximizing the likelihood is equivalent to maximizing the log-likelihood:\n",
    "$$\\ell(\\theta) = \\log \\left( \\prod_{i=1}^{n} P_{\\theta}(\\mathbf{x}^{(i)}) \\right) = \\sum_{i=1}^{n} \\log P_{\\theta}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "## MLE Intuition\n",
    "The data-space for high-dimensional inputs (e.g., $1024 \\times 1024$ RGB images) is unimaginably vast, with dimensions in the millions. However, meaningful data (like images of cats) does not exist uniformly across this space; instead, it concentrates on a low-dimensional subset called a **manifold**. \n",
    "\n",
    "The intuition behind MLE is built on two observations:\n",
    "1.  **Sparsity of Observations:** Out of all possible configurations in the high-dimensional space, our collected samples $\\mathcal{D}$ represent the \"realized\" events. Because these specific points were observed, they must have a high density under the true probability distribution $P_{data}$.\n",
    "2.  **The Optimization Goal:** We want our model $P_{\\theta}$ to mimic this true distribution. We achieve this by adjusting the parameters $\\theta$ so that the model \"shifts\" its probability mass away from the \"empty\" noise regions and concentrates it on the specific coordinates of our observed samples.\n",
    "\n",
    "For example, in a dataset of cat images, the model should learn that a specific arrangement of pixels forming a \"cat\" is highly probable, while a random arrangement of \"static\" noise, despite being in the same high-dimensional pixel space, has a probability near zero.\n",
    "\n",
    "Therefore, the training objective is to find the best model that assigns the highest possible probability to the observed data (the dataset). In other words, it maximize the likelihood or log-likelihood of the observed data.\n",
    "\n",
    "$$\\theta_{MLE} = \\arg \\max_{\\theta} \\sum_{i=1}^{n} \\log P_{\\theta}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "\n",
    "# Kullback-Leibler (KL) Divergence\n",
    "The KL Divergence is a measure of how one probability distribution $Q$ diverges from a second, reference probability distribution $P$. In machine learning, we often use it to measure the distance between the true data distribution $P_{data}$ and our model $P_{\\theta}$.\n",
    "\n",
    "> **Definition:** For two distributions $P$ and $Q$ defined on the same probability space, the KL Divergence is defined as:\n",
    "> $$D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "> Or in expectation form:\n",
    "> $$D_{KL}(P \\parallel Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right] = \\mathbb{E}_{x \\sim P} [\\log P(x) - \\log Q(x)]$$\n",
    "\n",
    "## Properties\n",
    "1.  **Distance**: The more different the two distributions are, the larger the divergence will be.\n",
    "1.  **Identity:** $D_{KL}(P \\parallel P) = 0$. A distribution has zero divergence from itself because they are the exact same.\n",
    "2.  **Non-negativity:** $D_{KL}(P \\parallel Q) \\geq 0$ for any $P, Q$ (via Jensen's Inequality).\n",
    "3.  **Asymmetry:** $D_{KL}(P \\parallel Q) \\neq D_{KL}(Q \\parallel P)$.\n",
    "\n",
    "## MLE as Divergence Minimization\n",
    "In generative models, the objective is the minimize the divergence between the true data distribution $P_{data}$ and our model $P_{\\theta}$ so that the model provides a good approximation for the true distribution. We show that the objective of minimizing the KL divergence is eqavelent to maximizing the likelihood. \n",
    "\n",
    "## The Mathematical Equivalence\n",
    "We can prove that minimizing the KL divergence between the data and the model is mathematically equivalent to maximizing the log-likelihood of the dataset.\n",
    "\n",
    "Given the definition of KL Divergence:\n",
    "$$D_{KL}(P_{data} \\parallel P_{\\theta}) = \\mathbb{E}_{\\mathbf{x} \\sim P_{data}} [\\log P_{data}(\\mathbf{x}) - \\log P_{\\theta}(\\mathbf{x})]$$\n",
    "\n",
    "Using the linearity of expectation, we can split this into two distinct terms:\n",
    "$$D_{KL}(P_{data} \\parallel P_{\\theta}) = \\underbrace{\\mathbb{E}_{\\mathbf{x} \\sim P_{data}} [\\log P_{data}(\\mathbf{x})]}_{\\text{Negative Entropy } -H(P_{data})} - \\underbrace{\\mathbb{E}_{\\mathbf{x} \\sim P_{data}} [\\log P_{\\theta}(\\mathbf{x})]}_{\\text{Cross-Entropy Term}}$$\n",
    "\n",
    "When we minimizing the divergence with respect to the model parameters $\\theta$, we notice the following:\n",
    "* **The Entropy Term:** $\\mathbb{E}_{\\mathbf{x} \\sim P_{data}} [\\log P_{data}(\\mathbf{x})]$ depends only on the true data. It is a constant with respect to $\\theta$ and does not change during training.\n",
    "* **The Empirical Estimate:** Since we don't know $P_{data}$ exactly, we estimate the second term using our samples $\\mathcal{D}$ because we know the data points are sampled from the true distribution.\n",
    "    $$\\mathbb{E}_{\\mathbf{x} \\sim P_{data}} [\\log P_{\\theta}(\\mathbf{x})] \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\log P_{\\theta}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "Therefore, minimizing the divergence is equivalent to maximizing the average log-likelihood:\n",
    "$$\\arg \\min_{\\theta} D_{KL}(P_{data} \\parallel P_{\\theta}) = \\arg \\max_{\\theta} \\sum_{i=1}^{n} \\log P_{\\theta}(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "Here, we assume each of our $n$ samples was sampled uniformly from the distribution, meaning each has a probability mass of $1/n$. However, the true probability is not necessarily $1/n$ for each individual sample, making the entire objective of MLE or minimizing KL divergence only an estimate.\n",
    "\n",
    "## The Lower Bound (Data Entropy)\n",
    "Note that even in a \"perfect\" training scenario where the model perfectly matches the empirical data, the log-likelihood does not necessarily reach zero. It is bounded by the **Entropy of the data**, $H(P_{data})$. This represents the inherent \"noise\" or uncertainty in the data that no model can eliminate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e55612",
   "metadata": {},
   "source": [
    "# Explicit Distribution Learning\n",
    "Explicit distribution learning involves the construction of a generative model that provides an explicit functional form for the probability density function (or mass function), denoted as $p_{\\theta}(\\mathbf{x})$. Unlike implicit models (e.g., GANs), which only provide a mechanism to sample data, explicit models allow for the direct evaluation of the likelihood of a given observation.\n",
    "\n",
    "## The Fundamental Challenge: Intractability and Normalization\n",
    "Modeling a high-dimensional data distribution $p_{data}(\\mathbf{x})$ (where $\\mathbf{x} \\in \\mathbb{R}^D$) is computationally intensive due to the **curse of dimensionality**. A valid probability distribution must satisfy the normalization constraint:\n",
    "$$\\int_{\\mathbf{x}} p_{\\theta}(\\mathbf{x}) d\\mathbf{x} = 1$$\n",
    "In high-dimensional spaces, calculating the denominator (the partition function) required to normalize a neural network's output is usually exponentially hard. Consequently, explicit modeling focuses on architectures that either bypass, simplify, or approximate this normalization while maintaining a clear expression for the density.\n",
    "\n",
    "The standard approach to estimate the distribution based through methods like MLE (minimizing KL divergence). In practice, the MLE objective is ususally implemented through 3 different modeling apporaches\n",
    "\n",
    "1. Autoregressive Modeling\n",
    "2. Energy-based Modeling\n",
    "3. Flow-based Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccceb15",
   "metadata": {},
   "source": [
    "## Autoregressive (AR) Modeling\n",
    "Autoregressive (AR) models belong to the class of **explicit density generative models**. They decompose the high-dimensional joint probability distribution of a data point into a sequence of conditional distributions. This structure allows for exact likelihood computation and stable training via MLE.\n",
    "\n",
    "\n",
    "The core principle of AR modeling is the application of the **Probability Chain Rule**. A $d$-dimensional data sample $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$ is represented as a product of $d$ univariate conditional distributions.\n",
    "\n",
    "For any specific component $x_i$ within a sample, the model predicts the probability of that component given all preceding components:\n",
    "$$p(x_i | \\mathbf{x}_{<i}) = p(x_i | x_1, x_2, \\dots, x_{i-1})$$\n",
    "\n",
    "The entire sample's probability is the product of these conditionals:\n",
    "$$p(\\mathbf{x}) = \\prod_{i=1}^{d} p(x_i | \\mathbf{x}_{<i})$$\n",
    "By converting a complex $d$-dimensional joint distribution into $d$ one-dimensional distributions, the model simplifies the learning task while maintaining an **explicit** representation of the distributions for the sample.\n",
    "\n",
    "AR models assume a fixed ordering of dimensions (e.g., raster scan for images, left-to-right for text). The model assumes that $x_i$ depends only on the observed values $\\mathbf{x}_{<i}$, satisfying the **causal constraint**.\n",
    "\n",
    "### Architectural Pipeline\n",
    "The transformation from input to probability follows this general flow:\n",
    "1.  **Input Context:** Previous elements $\\mathbf{x}_{<i}$ are fed into the model.\n",
    "2.  **Neural Network Feature Extractor:** A backbone architecture (e.g., RNN, LSTM, Transformer, or Causal CNN) processes the sequence to produce a hidden representation $h_i$.\n",
    "3.  **Output Head:**\n",
    "    * **Discrete Data (e.g., Text):** A Softmax layer over a vocabulary.\n",
    "    * **Continuous Data (e.g., Audio/Images):** A parametric distribution, such as a **Gaussian Mixture Model (GMM)** or a discretized logistic distribution, where the network predicts parameters $\\mu_i$ and $\\sigma_i$.\n",
    "4.  **Sampling:** A value is sampled from $p(x_i | \\mathbf{x}_{<i})$ and appended to the context for the next step $i+1$. The first value $p(x_0)$ is sampled from the prior after training.\n",
    "\n",
    "### MLE Objective and Loss Function\n",
    "AR models are trained by maximizing the log-likelihood of the training data $\\mathcal{D}$. This is equivalent to minimizing the **Cross-Entropy Loss**:\n",
    "$$\\mathcal{L}(\\theta) = -\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\sum_{i=1}^{d} \\log p_{\\theta}(x_i | \\mathbf{x}_{<i})$$\n",
    "\n",
    "### Teacher Forcing\n",
    "During training, we use **Teacher Forcing**. Instead of feeding the model's own (potentially erroneous) predictions from step $i-1$ into step $i$, we provide the **ground truth** values from the training set. This allows for parallelization during training (especially in Transformers and Causal CNNs) because all $x_i$ conditionals can be computed simultaneously, which also leads to faster convergence.\n",
    "\n",
    "### Challenges\n",
    "1. **Exposure Bias**: A significant drawback of Teacher Forcing is **Exposure Bias**. During training, the model only sees ground truth context. During inference (generation), it sees its own generated (noisy) samples. Errors accumulate over time, leading to a divergence between the training and testing distributions.\n",
    "\n",
    "2. **Sampling Bottleneck**: While training of AR models can be parallelized, **inference is inherently sequential and recursive** since any future data depends on the past data. To generate $x_{100}$, the model must first generate and process $x_1$ through $x_{99}$. The computational complexity of sampling a single sequence is $O(d)$, where $d$ is the number of dimensions/tokens. This makes AR models significantly slower at test-time compared to parallel generative models like GANs or non-autoregressive Flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b2816",
   "metadata": {},
   "source": [
    "## Energy-Based Model (EBM)\n",
    "Energy-Based Model (EBM) represents a fundamental shift from directly modeling probabilities to modeling the energy of a data point. Then, we convert the energies of data points into a valid distribution.\n",
    "\n",
    "Instead of trying to design a neural network that explicitly outputs a normalized probability distribution (which must sum to 1), we design a model that outputs a single scalar value called **energy** for a given input $x$. Borrowing from statistical physics, energy represents the state of a system. Low energy implies a stable, likely state. High energy implies an unstable, unlikely state.\n",
    "\n",
    "The Role of the Model: We define an energy function $E_\\theta(x)$, parameterized by a neural network, which maps an input $x$ to a scalar energy value:\n",
    "    $$E_\\theta(x): \\mathbb{R}^D \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "Ideally, the observed data (Real) should be assigned **low energy**, and noise/unobserved data (Fake) should be assigned **high energy**.\n",
    "\n",
    "### The Boltzmann Distribution\n",
    "To utilize the energy function for generative tasks, we must convert the energy (scalar output) of the neural network into a valid probability distribution. We achieve this using the **Boltzmann Distribution**\n",
    "\n",
    "Given an energy function $E_\\theta(x)$, the probability density function $p_\\theta(x)$ is defined as:\n",
    "\n",
    "$$p_\\theta(x) = \\frac{\\exp\\left(-E_\\theta(x)\\right)}{Z(\\theta)}$$\n",
    "\n",
    "*  **The Gibbs Measure ($\\exp(-E_\\theta(x))$):** This term converts the energy into a non-negative value. Because of the negative sign, lower energy results in a higher unnormalized probability score.\n",
    "*  **The Partition Function ($Z(\\theta)$):** This is the normalization constant required to ensure the probability distribution sums to 1.\n",
    "\n",
    "The core difficulty in EBMs lies in the partition function $Z(\\theta)$. It is obtained by **marginalizing** (integrating or summing) the unnormalized probabilities over the entire data space, which can be extremely difficult for high dimensional data space.\n",
    "\n",
    "$$Z(\\theta) = \\int_{x \\in \\mathcal{X}} \\exp\\left(-E_\\theta(x)\\right) dx$$\n",
    "\n",
    "* **Independence from Samples:** Note that $Z(\\theta)$ is a function of the model parameters $\\theta$, but it does **not** depend on any specific data point $x$ because it always sum over the entire data space.\n",
    "\n",
    "### Universality of Boltzmann Distribution\n",
    "Instead of modeling the distribution directly, the reason we go through the two step process of first defining energy of data and then converting it into a distribution is because of the **Universality of the Boltzmann Distribution**.\n",
    "\n",
    "For almost any well-defined, strictly positive probability density $Q(x)$, there exists an energy function $f(x)$ such that $Q$ can be expressed as a Boltzmann distribution.\n",
    "\n",
    "This implies that EBMs are extremely expressive and can be applied to almost any data. By learning an appropriate energy function $E_\\theta(x)$, a neural network can theoretically approximate any complex distribution of data, without being restricted by specific architectural constraints required to ensure normalization (like in Autoregressive models or Flow-based models).\n",
    "\n",
    "> Note: Softmax, Gaussian, Bernulli, etc, are all spcecial cases of the Boltzmann distribution.\n",
    "\n",
    "### Data $\\to$ Energy $\\to$ Explicit Distribution\n",
    "The ultimate workflow of learning an Energy-Based Model can be summarized as follows:\n",
    "\n",
    "1.  **The Objective:** We want to learn the true underlying distribution of a dataset, $p_{data}(x)$.\n",
    "2.  **The Proxy:** We define a parameterized energy model $f_\\theta(x)$ (usually a deep neural network).\n",
    "3.  **The Transformation:** We convert this energy model into an explicit distribution using the Boltzmann equation.\n",
    "4.  **The Learning Process:** We adjust $\\theta$ to assign **low** energy for real samples and high energy for other points in the space ($x_{fake}$).\n",
    "$$\\text{Data } x \\xrightarrow{\\text{Learn } f_\\theta} \\text{Energy } E(x) \\xrightarrow{\\text{Boltzmann}} \\text{Distribution } p(x)$$\n",
    "\n",
    "### EBM Training\n",
    "To train an Energy-Based Model, our primary goal is to maximize the likelihood of the observed data. Given a dataset $\\mathcal{D} = \\{x^{(1)}, x^{(2)}, \\dots, x^{(N)}\\}$ sampled from the true data distribution $p_{data}(x)$, we wish to find parameters $\\theta$ that maximize the log-likelihood objective (note that the objective of the formula is to maximize the likelihood not minimizing the loss):\n",
    "\n",
    "$$\\log \\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N} \\log p_\\theta(x^{(i)})$$\n",
    "\n",
    "Substituting the definition of the Boltzmann distribution $p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z(\\theta)}$, the objective function for a single data point becomes:\n",
    "\n",
    "$$\\log p_\\theta(x) = \\log \\left( \\frac{\\exp(-E_\\theta(x))}{Z(\\theta)} \\right) = -E_\\theta(x) - \\log Z(\\theta)$$\n",
    "\n",
    "Thus, the total log-likelihood we wish to maximize is:\n",
    "\n",
    "$$\\log \\mathcal{L}(\\theta) = - \\underbrace{\\log Z(\\theta)}_{\\text{Partition Term}} - \\frac{1}{N}\\sum_{i=1}^{N} \\underbrace{E_\\theta(x^{(i)})}_{\\text{Energy Term}} $$\n",
    "\n",
    "The second term of the objective function represents the **\"average\"** energy to the real data points. We can rewrite this sum as an **empirical (estimated) expectation** from the true data distribution $p_{data}$, where\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} E_\\theta(x^{(i)}) \\approx \\mathbb{E}_{x \\sim p_{data}} [E_\\theta(x)]$$\n",
    "\n",
    "However, this expectation is estimated because, we do not have access to the infinite continuous distribution $p_{data}$. We only have a finite dataset and assume each sample in our dataset has an equal probability of $1/N$.\n",
    "\n",
    "The first term, $\\log Z(\\theta)$, ensures the distribution is normalized.\n",
    "$$Z(\\theta) = \\int_{x \\in \\mathcal{X}} \\exp(-E_\\theta(x)) \\, dx$$\n",
    "\n",
    "Calculating this directly is **intractable** for high-dimensional data (e.g., images) because it requires integrating (or summing) over the entire data space. Consequently, we cannot easily evaluate the objective $\\log \\mathcal{L}(\\theta)$ itself.\n",
    "\n",
    "However, for training via Gradient Descent, we do not need the value of the objective; we only need its **gradient** $\\nabla_\\theta \\log \\mathcal{L}$. \n",
    "\n",
    "For the **Energy Term**, the gradient is \n",
    "$$\\nabla_\\theta \\mathbb{E}_{x \\sim p_{data}} [E_\\theta(x)] = \\mathbb{E}_{x \\sim p_{data}} [\\nabla_\\theta E_\\theta(x)]$$\n",
    "This computation is easy because it is the gradient with respect to the energy model. Since the energy model is just the neural network, this gradient is can be easily calculated with backpropagation.\n",
    "\n",
    "For the **Partition Term**,\n",
    "$$\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\nabla_\\theta Z(\\theta)$$\n",
    "\n",
    "Substitute the integral definition of $Z(\\theta)$:\n",
    "$$\\nabla_\\theta Z(\\theta) = \\nabla_\\theta \\int \\exp(-E_\\theta(x)) \\, dx = \\int \\nabla_\\theta \\exp(-E_\\theta(x)) \\, dx = \\int \\exp(-E_\\theta(x)) \\cdot \\left( -\\nabla_\\theta E_\\theta(x) \\right) \\, dx$$\n",
    "\n",
    "Now, substitute this back into the expression for $\\nabla_\\theta \\log Z(\\theta)$:\n",
    "\n",
    "$$\\nabla_\\theta \\log Z(\\theta) = \\frac{1}{Z(\\theta)} \\int \\exp(-E_\\theta(x)) \\left( -\\nabla_\\theta E_\\theta(x) \\right) \\, dx$$\n",
    "\n",
    "We can distribute $\\frac{1}{Z(\\theta)}$ inside the integral since it is independent from $x$. Notice that $\\frac{\\exp(-E_\\theta(x))}{Z(\\theta)}$ is exactly the Boltzmann Distribution (model probability) $p_\\theta(x)$:\n",
    "\n",
    "$$\\nabla_\\theta \\log Z(\\theta) = \\int \\underbrace{\\frac{\\exp(-E_\\theta(x))}{Z(\\theta)}}_{p_\\theta(x)} \\left( -\\nabla_\\theta E_\\theta(x) \\right) \\, dx$$\n",
    "\n",
    "This integral is simply the definition of an expectation over the **model distribution** without the hard to compute partition function:\n",
    "\n",
    "$$\\nabla_\\theta \\log Z(\\theta) = \\mathbb{E}_{x \\sim p_\\theta(\\cdot)} [-\\nabla_\\theta E_\\theta(x)]$$\n",
    "\n",
    "> This term is the expectation of samples from the **Current Model Distribution**, meaning as long as we can sample from the current model, we are able to train the EBM. Sampling is also exponentially hard in high dimensional space, but we can apply tricks for it to work.\n",
    "\n",
    "### Final MLE Gradient\n",
    "Combining the derivatives of the **Energy Term** and the **Partition Term**, the final gradient of the log-likelihood is:\n",
    "\n",
    "$$\\nabla_\\theta \\mathcal{L} = - \\mathbb{E}_{x \\sim p_{data}} [\\nabla_\\theta E_\\theta(x)] + \\mathbb{E}_{x \\sim p_\\theta} [\\nabla_\\theta E_\\theta(x)]$$\n",
    "\n",
    "Therefore, the original maximization objective (by removing the gradient) can be written as\n",
    "\n",
    "$$\\mathcal{L} = - \\mathbb{E}_{x \\sim p_{data}} [E_\\theta(x)] + \\mathbb{E}_{x \\sim p_\\theta} [E_\\theta(x)]$$\n",
    "\n",
    "This objective dictates the training dynamics of an Energy-Based Model:\n",
    "\n",
    "1.  **Energy Term (Data):** We want to **decrease** (minimize) the energy of real data points $x \\sim p_{data}$. This pulls the energy manifold down at locations where data exists.\n",
    "2.  **Partition Term (Model):** We want to **increase** (maximize) the energy of samples generated by the current model $x \\sim p_\\theta$. This pushes the energy manifold up at locations where the model currently assigns high probability (hallucinations).\n",
    "\n",
    "Essentially, by minimizing the energy of real data and maximizing the energy of model samples (\"fake\" data), the model is forced to distinguish real and fake samples, and the gradient pushes the model distribution $p_\\theta$ to shift closer to the real distribution $p_{data}$. This forces the model to learn to generate samples more similar to the real sample. The training reaches equilibrium when the model samples match the real data statistics, meaning the model can no longer distinguish between the two. This is the **Self-Adversarial Nature** of EBM (different from GAN). \n",
    "\n",
    "### Markov Chain Monte Carlo (MCMC) Algorithm\n",
    "MCMC model the complex sampling process as a Markove chain. The goal of mcmc is to start from an simple distribution, like guassian, and gradually shifts it towards the target (model) distribution. The idea is that a direct shift in one step is too complicated, but we can break it down into many small approximation steps and with enough step, we will\n",
    "\n",
    "Markov chain & formula\n",
    "\n",
    "There are different orders of MCMC method\n",
    "* Zero order: use target distribution p_w(x) directly to build the Markov chain\n",
    "    - Simple, but take long to converge\n",
    "* First order: use âˆ‡ log P_w(x) to build the Markov chain\n",
    "    - More computation is needed but converge faster\n",
    "* Higher order: too computationally expensive to consider\n",
    "\n",
    "#### Zero Order MCMC (Gibbs Sampling)\n",
    "Gibbs sampling is an implementation of zero order MCMC method.\n",
    "Instead of trying to sample the entire joint variables all at once from the distributation, it sample **one variable at a time**, conditioned on the values of all other variables.\n",
    "\n",
    "Suppose we want to sample a vector (e.g., an image with $D$ pixels) $x = (x_1, x_2, \\dots, x_D)$ from a model distribution $p_w(x)$. Calculating $p_w(x)$ directly is hard (due to the partition function $Z$). However, the **conditional distribution** of one pixel given the rest is often tractable.\n",
    "\n",
    "The Gibbs Sampling algorithm updates the state iteratively:\n",
    "1.  **Initialize:** Start with a random state $x^{(0)}$ (like a Gaussian noise).\n",
    "2.  **Loop:** For each step $t$:\n",
    "    **Loop:** For each varaible in $D$\n",
    "        * Sample formula here \n",
    "\n",
    "\n",
    "This means we need to train the model to generate one pixel at a time conditioned on all other pixels, which is incredibly slow, but theortially feasible.\n",
    "\n",
    "\n",
    "# Markov Chain Mon\n",
    "Markov Chain Monte Carlo (MCMC) is a class of algorithms used to sample from a complex probability distribution when direct sampling is intractable. In the context of generative modeling, we wish to sample from the model distribution $p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z}$. Because the partition function $Z$ is unknown, we cannot directly sample from the model distribution.\n",
    "\n",
    "MCMC models the sampling process as a **Markov Chain**.\n",
    "* **Initialization:** We start from a tractable, simple distribution $\\pi_0(x)$, such as a Gaussian noise distribution ($x^{(0)} \\sim \\mathcal{N}(0, I)$) or uniform noise.\n",
    "* **Transition:** We define a transition operator $T(x' | x)$ that shifts the current state slightly towards high-probability regions of the target distribution $p_\\theta(x)$.\n",
    "* **Convergence:** The core idea is that a direct jump from noise to data is impossible , but breaking the journey into many small approximation steps through a Markov Chain isfeasible. As the number of steps $t \\to \\infty$, the distribution of the samples $\\pi_t(x)$ converges to the target distribution $p_\\theta(x)$.\n",
    "\n",
    "$$x^{(0)} \\xrightarrow{T} x^{(1)} \\xrightarrow{T} x^{(2)} \\dots \\xrightarrow{T} x^{(t)} \\sim p_\\theta(x)$$\n",
    "\n",
    "MCMC algorithms can be categorized by the amount of information they utilize about the geometry of the target distribution (specifically, whether they use derivatives).\n",
    "\n",
    "* Zero-Order Methods: use the target distribution function $p_\\theta(x)$ directly to accept or reject proposed moves. It is simple to implement but very slow in convergence in high-dimensional spaces.\n",
    "\n",
    "* First-Order Methods: use the gradient of the log-probability with respect to the input, $\\nabla_x \\log p_\\theta(x)$ (or $-\\nabla_x E_\\theta(x)$). This allows for faster convergence, but requires more computations.\n",
    "\n",
    "* Higher-Order Methods: use second-order derivatives (Hessian matrix) to understand the curvature of the energy landscape. Generally, higher order methods are too computationally expensive for implementation.\n",
    "\n",
    "\n",
    "#### Zero-Order MCMC: Gibbs Sampling\n",
    "**Gibbs Sampling** is a canonical implementation of a zero-order MCMC method. It is particularly useful when the joint distribution $p(x_1, \\dots, x_D)$ is complex, but the **conditional distribution** of a single variable given all others is simple and tractable. Instead of updating the entire vector $x$ at once (which is hard), Gibbs sampling updates **one variable at a time**, conditioned on the current values of all other variables.\n",
    "\n",
    "Let $x$ be a $D$-dimensional vector (e.g., an image with $D$ pixels). The algorithm proceeds as follows:\n",
    "\n",
    "1.  **Initialize:**\n",
    "    Start with a random state vector $x^{(0)}$ (e.g., random noise).\n",
    "\n",
    "2.  **The Gibbs Loop:**\n",
    "    Repeat for $t = 1$ to $T$ (where $T$ is the number of steps needed to mix):\n",
    "    \n",
    "    * Iterate through each dimension $j = 1$ to $D$:\n",
    "        * Sample a new value for component $x_j$ from its conditional distribution given all other current values $x_{<j}^{(t)}, x_{>j}^{t-1}$ (Notice for $x_{<j}$ we use the new values at $t$, where for $x_{>j}$ we have to use the value at $t-1$ since the new value is not yet avaliable):\n",
    "        \n",
    "        $$x_j^{(t)} \\sim p_\\theta(x_j \\mid x_{<j}^{(t)}, x_{>j}^{t-1})$$\n",
    "\n",
    "\n",
    "By cycling through the variables, the state $x$ eventually converges to the joint distribution $p(x)$. The value $T$ here is arbitrary defined (e.g. $T=1000$), which is the amount of iteration required for the original distribution to converge to the target distribution.\n",
    "\n",
    "To compute the transition probablity $p_\\theta(x_j \\mid x_{<j}, x_{>j})$, we use Bayes rule, where\n",
    "\n",
    "$$\n",
    "p_\\theta(x_j \\mid x_{<j}, x_{>j}) = \\frac{p_\\theta(x_j, x_{<j}, x_{>j})}{p_\\theta(x_{<j}, x_{>j})}\n",
    "$$\n",
    "\n",
    "Recall the definition of the Boltzmann distribution for the joint configuration:\n",
    "$$p_\\theta(x) = \\frac{1}{Z} \\exp(-E(x))$$\n",
    "\n",
    "So, for the numerator (the joint probability of the specific state where pixel $j$ has value $x_j$):\n",
    "$$\n",
    "\\text{Numerator} = \\frac{1}{Z} \\exp\\left(-E(x_j, x_{<j}, x_{>j})\\right) = \\frac{1}{Z} \\exp\\left(-E(x)\\right)\n",
    "$$\n",
    "\n",
    "The denominator is the marginal probability of the *other* pixels. We obtain this by summing the joint probability over all possible values $k$ that pixel $j$ could take (marginalization):\n",
    "$$\n",
    "\\text{Denominator} = \\sum_{k \\in \\mathcal{V}} p_\\theta(x_j=k, x_{<j}, x_{>j})\n",
    "$$\n",
    "\n",
    "Substitute the Boltzmann definition again:\n",
    "$$\n",
    "\\text{Denominator} = \\sum_{k \\in \\mathcal{V}} \\left( \\frac{1}{Z} \\exp\\left(-E(x_j=k, x_{<j}, x_{>j})\\right) \\right)\n",
    "$$\n",
    "\n",
    "Now, putting everyting together\n",
    "\n",
    "$$\n",
    "p_\\theta(x_j \\mid x_{<j}, x_{>j}) = \\frac{\\frac{1}{Z} \\exp\\left(-E(x)\\right)}{\\sum_{k \\in \\mathcal{V}} \\frac{1}{Z} \\exp\\left(-E(x_j=k, x_{<j}, x_{>j})\\right)}\n",
    "$$\n",
    "\n",
    "Since $\\frac{1}{Z}$ is a constant factor common to every term in the sum, we can factor it out of the denominator:\n",
    "\n",
    "$$\n",
    "p_\\theta(x_j \\mid x_{<j}, x_{>j}) = \\frac{\\frac{1}{Z} \\exp\\left(-E(x)\\right)}{\\frac{1}{Z} \\left[ \\sum_{k \\in \\mathcal{V}} \\exp\\left(-E(x_j=k, x_{<j}, x_{>j})\\right) \\right]}\n",
    "$$\n",
    "\n",
    "The $\\frac{1}{Z}$ terms in the numerator and denominator cancel each other out perfectly. We are left with a tractable Softmax function that depends only on the energy values, which we can compute directly from the energy model:\n",
    "\n",
    "$$\n",
    "p_\\theta(x_j \\mid x_{<j}, x_{>j}) = \\frac{\\exp\\left(-E(x_j, x_{<j}, x_{>j})\\right)}{\\sum_{k \\in \\mathcal{V}} \\exp\\left(-E(x_j=k, x_{<j}, x_{>j})\\right)}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{V}$ is the set of all possible values for pixel $x_j$ (e.g., $\\{0, \\dots, 255\\}$).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. The Sampling Step (Important)\n",
    "Once we calculate this probability distribution for the specific pixel $x_j$, we do **not** simply pick the value $k$ that yields the lowest energy (highest probability). Doing so would be greedy optimization, not sampling.\n",
    "\n",
    "Instead, we perform **Weighted Random Selection**:\n",
    "1.  Compute the energy $E$ for every possible value $k$ that $x_j$ can take.\n",
    "2.  Convert these energies into a probability distribution using the **Softmax** function (the equation derived above).\n",
    "3.  Sample a value from this distribution.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Computational Implication\n",
    "This derivation highlights the computational cost of Gibbs Sampling for high-dimensional, non-binary data:\n",
    "* To update a **single pixel** $x_j$, we must evaluate the Energy Function $|\\mathcal{V}|$ times (once for every possible value $k$ in the denominator sum).\n",
    "* For a standard 8-bit image, $|\\mathcal{V}| = 256$.\n",
    "* Therefore, one full sweep over an image requires $256 \\times D$ evaluations of the neural network.\n",
    "\n",
    "This cost is why Gibbs Sampling is rarely used for continuous or high-cardinality data in modern Deep Learning, and is typically replaced by **Langevin Dynamics** (which uses gradients to avoid checking every possible value).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Theoretical Constraint (Performance)\n",
    "While theoretically sound, standard Gibbs Sampling highlights a major computational bottleneck in high dimensions:\n",
    "* To generate **one** full sample $x^{(t)}$ from $x^{(t-1)}$, we must perform $D$ separate sampling operations.\n",
    "* If $x$ is a $32 \\times 32$ image ($D=1024$), one Markov step requires 1024 distinct updates.\n",
    "* The conditional dependency prevents parallelization: we cannot sample $x_2^{(t)}$ until we know $x_1^{(t)}$.\n",
    "\n",
    "This makes \"pixel-by-pixel\" Gibbs sampling incredibly slow for large images, often necessitating Block Gibbs Sampling (in RBMs) or First-Order Gradient methods (Langevin) for modern applications.\n",
    "#### EBM Training by Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d6ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d254f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c11de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
