{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67463e23",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "Reinforcement learning is a framework in which an agent learns to make decisions by interacting with an environment in order to maximize cumulative reward. The agent observes the current state, selects an action, receives a reward, and transitions to a new state. Over time, it learns a policy that maximizes long-term returns through trial and error.\n",
    "\n",
    "## Terminology\n",
    "**Agent**: the entity that makes decisions, performs actions in the environment, receives rewards, and learns from the consequences of its actions\n",
    "\n",
    "**Environment**: everything external to the agent that it can interact with\n",
    "\n",
    "**State ($s$)**: all the information needed to describe the current situation of the environment. The agent will take action based on the current state, and an agent action (most of the time) will cause the transition to a new state. A state should be Markovian\n",
    "\n",
    "**Action ($a$)**: the decision taken by the agent after assessing the current state that can affect the environment. An action can be either discrete or continuous\n",
    "\n",
    "**Reward ($r$)**: a feedback given to an agent after it performs an action. The reward can be positive or negative based on the action taken and the resulting state, which tells the agent how good or bad its action was. A reward can be immediate or delayed. The goal of the agent is to maximize the cumulative reward. The reward signal is the way of communicating to the agent what we want achieved, not how we want it achieved.\n",
    "\n",
    "**Discount factor ($\\gamma$)**: a number between 0 and 1 that balances the future reward value based on the number of actions required. The more actions required, the smaller the rewards will be. Larger $\\gamma$ means the algorithm is patient and will look for long term reward, where smaller $\\gamma$ means the algorithm is impatient and will look for short term reward. The discount factor ensures the reward convergence in infinite horizon problems \n",
    "\n",
    "**Return ($G_t$)**: the total accumulated reward with discount after a timestep $t$, where\n",
    "$$G_t= r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... + \\gamma^{n-1} r_{t+n},$$\n",
    "where $r_{t+1}$ represents the reward after $t$th action is taken followed by $n$ total actions afterward\n",
    "\n",
    "**Policy ($\\pi(s)$)**: a function that takes in the current state of the agent, $s$, and return an action, $a$ to perform. The agent makes decision based on a policy. The goal of RL is to find an optimal policy $\\pi^*$ that maximizes expected return from every state\n",
    "\n",
    "**Trajectory**: the sequence of states, actions, and rewards the agent experiences\n",
    "\n",
    "**Episode**: a trajectory that ends in a terminal state\n",
    "\n",
    "**Exploration**: the agent tries new or less-known actions to discover the environment\n",
    "\n",
    "**Exploitation**: the agent select the best known action to maximize the reward to its current knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216127c",
   "metadata": {},
   "source": [
    "## General RL workflow\n",
    "For a general RL workflow, the agent observes the current state of the environment and selects an action based on a policy at each time step. The environment then responds by transitioning to a new state and providing a reward that reflects the quality of the action taken. The agent uses this reward, along with the new state, to update its understanding of the environment and improve its policy\n",
    "\n",
    "This cycle of observing, acting, receiving rewards, and learning continues over many episodes (trial and error), allowing the agent to gradually learn a policy that maximizes long-term cumulative reward. Key components of this process include exploration (trying new actions to gather information) and exploitation (choosing the best-known action to maximize reward). Over time, the agent aims to find a balance between the two and converge toward an optimal behaviour\n",
    "\n",
    "<img src=\"https://www.scribbr.com/wp-content/uploads/2023/08/the-general-framework-of-reinforcement-learning.webp\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ce44e",
   "metadata": {},
   "source": [
    "# Algorithm Summary\n",
    "<img src=\"https://miro.medium.com/1*itmyDvkBatuB-1Zw6zWL6A.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34d7ee",
   "metadata": {},
   "source": [
    "# Multi-armed bandit\n",
    "Multi-armed bandit is the simplest reinforcement learning problem. An agent will choose between $k$ different actions at each timestep, and recieves a reward based on the action chosen, but the reward distributions are unknown and different for each action. The goal of the agent is to maximize the cumulative rewards in the given amount of steps\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:894/1*ZS_craAiKCJzFj9dQ9RaYQ.png\" width=500>\n",
    "\n",
    "## Action-Value\n",
    "\n",
    "The value of an action, $q(a)$, is the expected reward received when action $a$ is taken:\n",
    "\n",
    "$$q(a) = \\mathbb{E}[R \\mid A = a]$$\n",
    "\n",
    "* $q(a)$: the expected reward of taking action $a$\n",
    "* In multi-armed bandit problems, there is no concept of state or policy, so the value depends only on the action itself.\n",
    "\n",
    "### Sample-Average Method\n",
    "In reinforcement learning, the true action values $q(a)$ are unknown and must be estimated through repeated interactions with the environment.  \n",
    "One simple estimation approach is the sample-average method, where the estimated value $\\hat{q}_t(a)$ at time $t$ is:\n",
    "\n",
    "$$\\hat{q}_t(a) = \\frac{\\sum_{i=1}^{t-1} \\mathbb{1}[A_i = a] \\cdot r_i}{\\sum_{i=1}^{t-1} \\mathbb{1}[A_i = a]} = \\frac{\\text{Cumulative reward recieved from action $a$ before timestep $t$}}{\\text{Total number of time the action $a$ is taken before timestep $t$}}$$\n",
    "\n",
    "* $A_i$: the action taken at time step $i$\n",
    "* $r_i$: the reward received at time step $i$\n",
    "* $\\mathbb{1}[A_i = a]$: an **indicator function** that equals 1 if action $a$ was taken at time $i$, and 0 otherwise\n",
    "\n",
    "As the number of samples increases, $\\hat{q}_t(a)$ converges to the true expected value $q(a)$, by the law of large numbers.\n",
    "\n",
    "#### Incremental Updates\n",
    "\n",
    "When learning to estimate the expected reward of each action, we need to update our estimate each time a new reward is observed. One issue with the sample-average method is that it requires storing all past rewards for each action to compute $\\hat{q}_t(a)$. As the number of trials increases, the required memory grows linearly, which becomes inefficient. To solve this, we can rewrite the sample-average update in a recursive (incremental) form, where\n",
    "\n",
    "$$\\hat{q}_{t+1} = \\hat{q}_t + \\alpha_t \\left(r_t - \\hat{q}_t\\right)$$\n",
    "\n",
    "* $\\hat{q}_{t+1}$: the updated estimate of the action-value after time step $t$\n",
    "* $\\hat{q}_t$: the previous estimate before seeing the latest reward\n",
    "* $r_t$: the reward received at time step $t$\n",
    "* $\\alpha_t$: the step size or learning rate, a value between 0 and 1 that determines how much the estimate is updated\n",
    "\n",
    "This form only requires storing the current estimate and does not require keeping track of all past rewards, making it computationally efficient.\n",
    "\n",
    "## Non-Stationary Problem\n",
    "A problem is said to be stationary if the reward distribution for each action remains the same over time. However, in many real world RL problems, the environment is non-stationary, meaning the reward distributions change over time. In such cases, an agent must be able to adapt its action-value estimates to reflect the most recent outcomes more than older ones. The incremental update rule can be written in a recursive form\n",
    "\n",
    "$$\\hat{q}_{t+1} = (1-\\alpha)^t\\hat{q}_1 + \\sum^{t}_{i=1}\\alpha (1-\\alpha)^{t-i} r_i$$\n",
    "\n",
    "* $\\alpha$: a constant step size between 0 and 1\n",
    "* $r_i$: the reward recieved after taking the action at timestep $i$\n",
    "\n",
    "This formula represents the exponentially decaying weighted average of past rewards, where recent rewards are given more significance, and older rewards gradually “fade out” due to the $(1 - \\alpha)^{t-i}$ decay factor.\n",
    "\n",
    "## Exploration and Exploitation Tradeoff\n",
    "One key challenge in reinforcement learning is deciding when to explore and when to exploit, as an agent cannot do both simultaneously. Exploration helps the agent improve its knowledge about the environment, which can lead to greater rewards in the long term. Exploitation, on the other hand, involves leveraging the agent’s current knowledge to maximize immediate rewards. An optimal policy should strike a balance between exploration and exploitation based on the agent’s current knowledge and state, in order to maximize cumulative reward over time.\n",
    "\n",
    "\n",
    "### Greedy\n",
    "The greedy policy is simple as the agent always exploits by choosing the action with the highest estimated reward, without any exploration. While this strategy can work in very simple or well-understood environments, it often performs poorly in more complex or uncertain problems, because the agent never tries new actions and thus fails to discover potentially better options or adapt when conditions change.\n",
    "\n",
    "### $\\epsilon$ Greedy\n",
    "The $\\epsilon$-greedy policy is a variation of the greedy policy that introduces a small amount of exploration.  \n",
    "At each time step, the agent exploits with probability $1 - \\epsilon$, and explores by choosing a random action) with probability $\\epsilon$. This policy helps the agent avoid getting stuck with suboptimal actions by occasionally trying alternatives.This can be written as\n",
    "\n",
    "$$\n",
    "A_t =\n",
    "\\begin{cases}\n",
    "\\arg\\max_a \\hat{q}_t(a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "\\text{a random action} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In general, the $\\epsilon$ greedy policy will perform better than greedy in the long run as it gains more knowledge about the environment through exploration\n",
    "\n",
    "### Optimistic Initial Value\n",
    "Optimistic initial value is another strategy that balances exploration and exploitation by encouraging the agent to explore early. The idea is to initialize the estimated value of all actions to a number higher than the actual maximum reward. This causes the agent to optimistically assume all actions are promising, so it will try each action early on to verify its assumption. In the early timesteps, the agent explores all actions because it believes \"every action might be great.\" Over time, as the agent gathers more data and updates its estimates, the action values converge to their true values, and the agent naturally shifts to exploitation.\n",
    "\n",
    "However, here are some issues of optimistic initial value:\n",
    "1. Only encourages early exploration: Once the agent’s estimates stabilize, it behaves greedily and stops exploring, which can lead to suboptimal policies if early exploration missed better actions\n",
    "2. Poor performance in non-stationary environments: In environments where the reward distributions change over time, the agent may stop exploring too early and fail to adapt\n",
    "3. Choosing the initial optimistic value is tricky: It must be high enough to encourage exploration, but not too high to delay convergence. Often, the true maximum reward is unknown, making this hard to tune\n",
    "\n",
    "### Upper-Confidence Bound Action Selection (UCB)\n",
    "\n",
    "UCB is a method for balancing exploration and exploitation by considering both the current value estimates and the uncertainty in those estimates. At each time step, the agent selects the action using the following rule:\n",
    "\n",
    "$$\n",
    "A_t = \\arg\\max_a \\left[ \\hat{q}_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]\n",
    "$$\n",
    "\n",
    "* $A_t$: the action selected at time step $t$\n",
    "* $\\hat{q}_t(a)$: the current estimated value of action $a$\n",
    "* $c$: a user-defined parameter that controls the degree of exploration. Larger $c$ encourages exploration; smaller $c$ favors exploitation\n",
    "* $t$: the current time step\n",
    "* $N_t(a)$: the number of times action $a$ has been selected so far\n",
    "\n",
    "In this formula, $\\hat{q}_t(a)$ is the exploitation term, which indicates the agent's current estimate of how good action $a$ is. The term $c \\sqrt{\\frac{\\ln t}{N_t(a)}}$ is the exploration bonus, which is large when action $a$ has been selected only a few times (low $N_t(a)$), encouraging the agent to explore it.\n",
    "\n",
    "The logarithmic (unbounded) growth in $\\ln t$ ensures that all actions will eventually be explored, but actions with lower estimated value or that have already been selected many times will be chosen less frequently over time. This way, UCB systematically prioritizes actions with high upper confidence bounds\n",
    "\n",
    "Note: UCB is an deterministic algorithm\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200126023259/Screenshot-2020-01-26-at-2.32.38-AM.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcaef82",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "In the bandit problem, the agent selects actions in the same static environment, where each action yields a reward independent of past actions or time. However, in many real-world problems, the environment is dynamic, and the agent must select different actions depending on the current situation. Markov Decision Processes (MDPs) provide a classical formalization for sequential decision making tasks, where each action affects not only the immediate reward but also the next state and all future actions and rewards as well.\n",
    "\n",
    "All MDPs are Markovian, meaning the future state and reward depend only on the current state and action, not on the full history. Therefore, knowing the current state is sufficient for optimal decision making, and remembering earlier states does not improve predictions about the future.\n",
    "\n",
    "## The Agent-Environment Interface\n",
    "In MDPs, the agent and environment interact continuously in either discrete or continuous time. At each timestep $t$, the agent selects an action $A_t$ based on the current state of the environment $S_t$. As the result of its action, the agent will recieve a reward $r_t$ from the envirnment at the next timestep $t+1$, and the environment will transition into a new state $S_{t+1}$. Then, the agent selection its next action $A_{t+1}$ based on the new states. The sequence of states, actions, rewards is called a trajectory\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/340694475/figure/fig2/AS:938161513455616@1600686543282/The-agent-environment-interaction-in-reinforcement-learning.jpg\" width=500>\n",
    "\n",
    "## Dynamics of MDPs\n",
    "In finite MDPs, where the sets of states, actions, and rewards are all finite, the dynamics of the environment are defined by the transition probability function:\n",
    "\n",
    "$$p(s', r \\mid s, a)$$\n",
    "\n",
    "This represents the joint probability of transitioning to state $s'$ and receiving reward $r$, given that the agent is in state $s$ and takes action $a$. In other words, it defines:\n",
    "1. the likelihood of moving to a specific next state $s'$\n",
    "2. and receiving a particular reward $r$\n",
    "based on the current state–action pair $(s, a)$.\n",
    "\n",
    "This formulation captures the Markov property, where the next state and reward depend only on the current state and action, not on any earlier history.\n",
    "\n",
    "## Episodic and Continuing Tasks\n",
    "There are two types of tasks in RL, episodic tasks and continuing tasks.\n",
    "\n",
    "The tasks are episodic when the agent–environment interaction breaks naturally into subsequences, where each episodes begins from a standard starting state or a sample from a standard distribution of starting states and will eventual reach a terminal state. Each episode begins independently of how the previous one ended.\n",
    "\n",
    "The tasks are continuing when the agent–environment interaction cannot be broken down into subsequences and will go on without an ending\n",
    "## Episodic and Continuing Tasks\n",
    "\n",
    "In reinforcement learning, tasks are generally classified into two types: episodic and continuing.\n",
    "\n",
    "A task is episodic when the agent–environment interaction is naturally divided into distinct episodes. Each episode begins in a standard starting state (or sampled from a starting state distribution), proceeds through a sequence of interactions, and eventually reaches a terminal state. After an episode ends, the environment resets, and the next episode begins independently of how the previous one ended.\n",
    "\n",
    "A task is continuing when there is no natural endpoint, where the agent–environment interaction continues indefinitely without reaching a terminal state. This setting is common in real-world systems that operate continuously, such as online recommendation engines or stock trading agents.\n",
    "\n",
    "<img src=\"https://av.tib.eu/thumbnail/63100\" width=500>\n",
    "\n",
    "## Goal of Reinforcement Learning\n",
    "\n",
    "The goal of reinforcement learning at any time step $t$ is to maximize the expected return, denoted $G_t$, which represents the cumulative reward the agent can expect to receive starting from that time step.\n",
    "\n",
    "In episodic tasks, each episode has a fixed or variable length and terminates at some final time step $T$.  \n",
    "Since the episode ends, the return is always finite:\n",
    "\n",
    "$$G_t = r_{t+1} + r_{t+2} + \\dots + r_T$$\n",
    "\n",
    "\n",
    "In continuing tasks, the agent–environment interaction does not end, so we introduce a discount factor $\\gamma \\in [0, 1)$ to ensure the return remains finite, where\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "* A larger discount factor ($\\gamma \\to 1$) makes the agent more far-sighted, valuing long-term rewards.\n",
    "* A smaller discount factor makes the agent more short-sighted, prioritizing immediate rewards.\n",
    "\n",
    "Note: $\\frac{R_{\\text{max}}}{1 - \\gamma}$ is an upper bound on return only if each reward is bounded by $R_{\\text{max}}$, i.e., $r_t \\leq R_{\\text{max}}$.\n",
    "\n",
    "\n",
    "### Recursive Form of Return\n",
    "The return can also be defined recursively, where\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "This recursive relationship is fundamental in deriving value functions and forms the basis of many RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7861349",
   "metadata": {},
   "source": [
    "# Policies and Value Functions\n",
    "## Policies\n",
    "A policy, denoted by $\\pi$, defines the agent’s behaviour by specifying a mapping from states to a probability distribution over actions. Formally, given the current state $s$, the policy defines\n",
    "\n",
    "$$\\pi(a \\mid s)$$\n",
    "\n",
    "This expression represents the probability of selecting action $a$ when the agent is in state $s$.\n",
    "\n",
    "Policies can be:\n",
    "* Deterministic: where $\\pi(s)$ directly maps to a specific action.\n",
    "* Stochastic: where $\\pi(a \\mid s)$ gives a probability distribution over actions.\n",
    "\n",
    "Note: A policy is typically a function of only the current state $s$, due to the Markov property. If the policy depends on more than the current state (e.g., past states), then the problem setting is no longer a MDP unless those inputs are encoded into the current state.\n",
    "\n",
    "## State Value Function\n",
    "The state value function, denoted by $v_\\pi(s)$, represents the expected return when the agent starts in state $s$ at time step $t$ and follows a policy $\\pi$ thereafter. It is defined as\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} \\mid s_t = s \\right]$$\n",
    "\n",
    "\n",
    "## Action Value Function\n",
    "The action value function, denoted by $Q_\\pi(s, a)$, represents the expected return when the agent starts in state $s$, takes action $a$, and then follows policy $\\pi$ thereafter. It is defined as\n",
    "\n",
    "$$Q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s, a_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} \\mid s_t = s, a_t = a \\right]$$\n",
    "\n",
    "## Bellman Equation\n",
    "The Bellman equation provides a recursive formulation of the state value function and action value function. Instead of computing returns as infinite sums of future rewards, the Bellman equations allow us to compute value functions based on expected immediate reward plus the discounted value of the next states.\n",
    "\n",
    "### Bellman Equation for the State Value Function\n",
    "The Bellman equation for the state value function expresses the value of a state $s$ under a policy $\\pi$ in terms of\n",
    "1. All the actions available in $s$\n",
    "2. The probability of choosing each action under policy $\\pi$,\n",
    "3. The environment's transition dynamics/probability $p(s', r \\mid s, a)$,\n",
    "4. The values of successor states $v_\\pi(s')$.\n",
    "\n",
    "It is defined as\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s \\right] = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "* $s$, $s'$: the current and next state respectively\n",
    "* $v_\\pi(s)$: expected return starting from state $s$ and following policy $\\pi$\n",
    "* $\\pi(a \\mid s)$: probability of taking action $a$ in state $s$\n",
    "* $p(s', r \\mid s, a)$: probability of transitioning to state $s'$ and receiving reward $r$ given current state $s$ and action $a$\n",
    "* $\\gamma$: discount factor, $0 \\leq \\gamma < 1$\n",
    "\n",
    "At a high level, the Bellman equation follows\n",
    "1. From the current state $s$, consider all possible actions $a$ (weighted by the policy).\n",
    "2. For each action, consider all possible resulting next states $s'$ and rewards $r$ (weighted by the environment's dynamics).\n",
    "3. For each possible $(s', r)$ pair, compute the expected return, which is the immediate reward $r$ plus the discounted future value $v_\\pi(s')$.\n",
    "4. Compute the weighted sum all of these to compute $v_\\pi(s)$.\n",
    "\n",
    "Note: $\\pi(a \\mid s)$ is the probability of taking action $a$ in state $s$ under policy $\\pi$ and $p(s', r \\mid s, a)$ is the probability of transitioning to next state $s'$ and receiving reward $r$, given current state $s$ and action $a$. In the equation, we are summing over all possible immediate outcomes, which are the actions the agent might take, the rewards it might receive, and the next states it might reach. This allows the equation to \"look ahead\" one step into the future and compute the expected value of that step, using $\\pi(a \\mid s)$ to weigh each action and $p(s', r \\mid s, a)$ to weigh each possible outcome of that action. This can be visualized with the Backup Diagram.\n",
    "\n",
    "<img src=\"https://goodboychan.github.io/images/backup_diagram_for_v.png\" width=300>\n",
    "\n",
    "The result is a weighted average of the immediate reward plus the discounted value of future states. This recursive structure can be thought of as expanding a search tree, where each branch corresponds to a possible action and outcome.\n",
    "\n",
    "#### Notational Notes\n",
    "* We omit the time index $t$ in $v_\\pi(s)$ because under the Markov property, the value of a state depends only on the current state, not on the specific time step.\n",
    "* The summation over $r$ is used when the reward distribution is stochastic; in deterministic cases, this may be dropped.\n",
    "\n",
    "### Bellman Equation for the Action-Value Function\n",
    "Similarly, the action-value function can also be expressed in a recursive form using the Bellman equation.  \n",
    "It defines the expected return starting from state $s$, taking a specific action $a$, and then following policy $\\pi$ thereafter:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s, a_t = a \\right] = \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' \\mid s') q_\\pi(s', a') \\right] = \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right]$$\n",
    "\n",
    "* There is no policy term in the outer expectation because the first action $a$ is already fixed (we are evaluating $q_\\pi(s, a)$ for a specific action).\n",
    "* The recursion comes into play after the transition to the next state $s'$, where the agent resumes following policy $\\pi$. For each possible next state $s'$ and reward $r$, we\n",
    "    1. Compute the immediate reward $r$\n",
    "    2. Add the discounted expected value of the next state, where the value is a weighted sum over all possible next actions $a'$ under policy $\\pi$\n",
    "    \n",
    "This gives a complete recursive expression for $q_\\pi(s, a)$, based on the one-step lookahead and expected future return.\n",
    "\n",
    "\n",
    "In summary, the Bellman equation leverages the recursive structure of MDPs to transform an infinite sum of future rewards into a system of linear equations. While this makes value computation more tractable in theory, solving this system exactly becomes computationally infeasible in large-scale problems due to the exponential growth in the number of states and actions.\n",
    "\n",
    "### Relationship Between State-Value and Action Value\n",
    "1. State value expressed in terms of action value: $v(s)$ is the weighted sum of $q(s,a)$ across all possible actions weighted by the policy\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s) q_{\\pi}(s, a)$$\n",
    "\n",
    "2. Action value expressed in terms of next state value: $q(s,a)$ is the expected immediate reward plus the discounted value of next state $s'$\n",
    "$$q_{\\pi}(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right]$$\n",
    "\n",
    "## Optimal Policy\n",
    "The goal of reinforcement learning is to find an optimal policy that maximizes the expected cumulative reward. A policy $\\pi$ is said to be better than or equal to another policy $\\pi'$ if, for every state $s$, the expected return of following $\\pi$ is at least as high as that of following $\\pi'$. Formally, $\\pi \\geq \\pi'$ if and only if\n",
    "\n",
    "$$ v_\\pi(s) \\geq v_{\\pi'}(s) \\quad \\text{for all } s \\in \\mathcal{S}$$\n",
    "\n",
    "An optimal policy, denoted by $\\pi_*$, is a policy that is better than or equal to all other policies. That is, for any other policy $\\pi'$\n",
    "\n",
    "$$v_{\\pi_*}(s) \\geq v_{\\pi'}(s) \\quad \\text{for all } s \\in \\mathcal{S}$$\n",
    "\n",
    "There always exists at least one optimal policy, and in some cases, multiple optimal policies may exist, all yielding the same optimal state-value function, $v_*(s)$, and optimal action-value function $q_*(s, a)$.\n",
    "\n",
    "### Optimal Value Functions\n",
    "The optimal state-value function, denoted by $v_*(s)$, represents the maximum expected return that can be achieved from state $s$ by among all policies, where\n",
    "\n",
    "$$v_*(s) = \\max_{\\pi} v_\\pi(s) \\quad \\text{for all } s \\in \\mathcal{S}$$\n",
    "\n",
    "Similarly, the optimal action-value function, denoted by $q_*(s, a)$, represents the maximum expected return achievable from state $s$ by taking action $a$ and then following the best possible policy thereafter:\n",
    "\n",
    "$$q_*(s, a) = \\max_{\\pi} q_\\pi(s, a) \\quad \\text{for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}$$\n",
    "\n",
    "### Bellman Optimality Equations\n",
    "\n",
    "We can express the Bellman equations for the optimal value functions without referencing any specific policy. These are known as the Bellman Optimality Equations.\n",
    "\n",
    "The optimal state-value function $v_*(s)$ satisfies:\n",
    "\n",
    "$$v_*(s) = \\max_{a} \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_*(s') \\right]$$\n",
    "\n",
    "This equation tells us that under an optimal policy, the value of a state equals the maximum expected return achievable by taking the best possible action from that state. In other words, the best action in any state is the one that leads to the highest expect state value.\n",
    "\n",
    "The optimal action-value function $q_*(s, a)$ satisfies:\n",
    "\n",
    "$$q_*(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma \\max_{a'} q_*(s', a') \\right]$$\n",
    "\n",
    "This equation expresses the value of taking action $a$ in state $s$ as the expected immediate reward plus the best possible value achievable from the resulting next state $s'$, by taking the best action $a'$ at that point. Since the current state and action $(s, a)$ are already fixed, there is no choice to be made immediately, the choice comes at the next decision point.\n",
    "\n",
    "It is essential to express these equations without referencing any policy because the goal of reinforcement learning is to discover the optimal policy. We cannot write the equations in terms of a policy that we don't yet know. The Bellman Optimality Equations define the criteria that an optimal policy must satisfy, and solving them (exactly or approximately) allows us to find the best possible decisions at each state.\n",
    "\n",
    "### Deriving the Optimal Policy from Optimal Value Functions\n",
    "\n",
    "Once we have the optimal value functions, it is straightforward to derive the optimal policy from them. The optimal policy $\\pi_*$ selects the action in each state that maximizes the expected return, based on the optimal state-value function $v_*(s)$, where\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_*(s') \\right]$$\n",
    "\n",
    "This means that the optimal policy always chooses the action that leads to the highest expected state value in the next state.\n",
    "\n",
    "Using the optimal action-value function $q_*(s, a)$, the optimal policy can be written more simply as\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_a q_*(s, a)$$\n",
    "\n",
    "This means the optimal policy selects the action that has the highest action value for the current state.\n",
    "\n",
    "In both cases, the agent behaves greedily with respect to the optimal value function, where it chooses the action that promises the greatest long-term reward.\n",
    "\n",
    "However, in most real-world scenarios, obtaining the optimal policy by directly solving the Bellman equations is unrealistic because it requires\n",
    "\n",
    "1. Substantial knowledge of the environment, including the transition dynamics $p(s', r \\mid s, a)$, which we often do not have access to in practice.\n",
    "2. Solving the Bellman equations involves extremely high computational and memory demands, especially in environments with large or continuous state and action spaces.\n",
    "\n",
    "As a result, in most practical applications, we use approximation methods to learn a \"good enough\" policy or value function, rather than attempting to compute the exact optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872162c",
   "metadata": {},
   "source": [
    "# Policy Evaluation & Improvement \n",
    "\n",
    "In reinforcement learning, two fundamental tasks are:\n",
    "* Policy Evaluation: Determining how good a given policy $\\pi$ is by computing its state-value function $v_\\pi$.\n",
    "* Policy Improvement: Improving a policy by iteratively producing strictly better policies until reaching an good enough or optimal policy.\n",
    "\n",
    "Dynamic Programming (DP) methods can be used to solve both tasks if we have access to the environment's dynamics $p(s', r \\mid s, a)$.\n",
    "\n",
    "## Iterative Policy Evaluation\n",
    "To evaluate the state values under a policy $\\pi$, we use the Bellman equation for the value function, where\n",
    "\n",
    "$$v_\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "Since computing $v_\\pi$ exactly is often impractical, we use an iterative approximation, where\n",
    "\n",
    "$$v_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right]$$\n",
    "\n",
    "Each iteration updates the estimated value of every state once, using the latest available estimates. With enough iterations, the values are guaranteed to converge to the true value function $v_\\pi$. The key idea of policy evaluation is to use observed (or expected) rewards and the estimated state values one step ahead to iteratively update state values until they converge.\n",
    "\n",
    "### Algorithm: Iterative Policy Evaluation (Given a Policy $\\pi$)\n",
    "\n",
    "1. Initialize two arrays, $v_k$ and $v_{k+1}$, to store the old and updated state values for all states.\n",
    "2. Randomly initialize the state values (or initialize all to zero).\n",
    "3. For each state $s$, compute a new value using\n",
    "   $$v_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right]$$\n",
    "4. Store all new values in $v_{k+1}$.\n",
    "5. After updating all states, replace $v_k$ with $v_{k+1}$ (set $v_k = v_{k+1}$).\n",
    "6. Repeat steps 3–5 until the values converge, meaning the change between $v_k$ and $v_{k+1}$ is smaller than a chosen threshold.\n",
    "\n",
    "A unique value function $v_\\pi$ is guaranteed to exist for any given policy $\\pi$ as long as the task is episodic or the discount factor satisfies $\\gamma < 1$. Under these conditions, the iterative updates are guaranteed to converge to the true values.\n",
    "\n",
    "## Policy Improvement\n",
    "The primary goal of computing the value function of a policy is to help us find a better policy. Given the state-value function $v_\\pi$ of any arbitrary policy $\\pi$, we can construct a new policy $\\pi'$ by acting greedily with respect to $v_\\pi$. That is, in each state, $\\pi'$ selects the action that yields the highest expected return based on the current value estimates. By doing this, the new policy $\\pi'$ is guaranteed to be at least as good as the original policy $\\pi$. If the new policy’s value function $v_{\\pi'}$ is strictly better than $v_\\pi$, we have improved the policy. If $v_{\\pi'} = v_\\pi$, this means the current policy is already optimal, and no further improvement is possible.\n",
    "\n",
    "### Policy Improvement Theorem\n",
    "Formally, given two policies $\\pi$ and $\\pi'$, the new policy $\\pi'$ is guaranteed to be at least as good as $\\pi$ if, for every state $s$:\n",
    "\n",
    "$$q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s)$$\n",
    "\n",
    "This condition means that in every state $s$, the action chosen by $\\pi'$, which can be a same or different action chosen by $\\pi$, always yields an expected return greater than or equal to the value of following the original policy $\\pi$ from that state onward.\n",
    "\n",
    "As a result,\n",
    "\n",
    "$$v_{\\pi'}(s) \\geq v_\\pi(s) \\quad \\text{for all } s$$\n",
    "\n",
    "### Greedy Policy Improvement\n",
    "Applying the policy improvement theorem, given an original policy $\\pi$ and its state-value function $v_\\pi(s)$, we can obtain a better or equally good policy $\\pi'$ by selecting actions greedily with respect to $v_\\pi(s)$, where\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a q_\\pi(s, a)$$\n",
    "\n",
    "* If $\\pi' \\neq \\pi$, then $\\pi'$ is a strictly better policy (strict improvement).\n",
    "* If $\\pi' = \\pi$, then the policy is already greedy with respect to its own value function, which indicates that the current policy is optimal and cannot be improved further.\n",
    "\n",
    "## Policy Iteration\n",
    "Policy Iteration is an algorithmic process for finding the optimal policy by repeatedly alternating between policy evaluation and policy improvement. The key idea is that by evaluating the current policy and then improving it based on the evaluation, we can iteratively converge to an optimal or sufficiently good policy. In this process, each policy evaluation gives us more accurate value estimates, and each policy improvement ensures the new policy is strictly better or equally good. The process is guaranteed to converge to the optimal policy in finite MDPs.\n",
    "\n",
    "### Policy Iteration Algorithm\n",
    "\n",
    "1. Initialize:  \n",
    "   * Start with an arbitrary policy $\\pi$\n",
    "   * Initialize the state-value function $v$\n",
    "\n",
    "2. Policy Evaluation: \n",
    "   * Compute $v_\\pi$ for the current policy using iterative updates until convergence.\n",
    "\n",
    "3. Policy Improvement:\n",
    "   * For each state $s$, update the policy greedily with respect to $v_\\pi$:  \n",
    "   $$\\pi_{\\text{new}}(s) = \\arg\\max_a \\sum_{s'} \\sum_r p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "4. Check for Convergence:\n",
    "   * If the policy hasn’t changed, the optimal policy has been found.\n",
    "   * Otherwise, set $\\pi = \\pi_{\\text{new}}$ and return to Step 2.\n",
    "\n",
    "<img src=\"https://lcalem.github.io/imgs/sutton/gpi_interaction.png\" width=500>\n",
    "\n",
    "# Generalized Policy Iteration\n",
    "Generalized Policy Iteration  refers to the framework in reinforcement learning where policy evaluation and policy improvement are interact repeatedly and approximately to gradually refine a policy toward optimality. The key idea is that evaluation and improvement do not need to fully complete before the next begins. As long as policy evaluation moves the value function closer to accurate estimates under the current policy, and policy improvement makes the policy greedier with respect to those estimates, the process will converge over time. If both the value function and the policy eventually stabilize, meaning no further updates occur, then the policy must be optimal, and the value function must be the optimal value function.\n",
    "\n",
    "\n",
    "## Value Iteration\n",
    "Value iteration is a form of generalized policy iteration that streamlines the process of finding the optimal policy. It addresses the inefficiency in traditional policy iteration, where the policy evaluation step requires multiple sweeps over all states until the value function converges. Instead of fully evaluating the current policy before improving it, value iteration performs a single update to each state's value in each iteration and greedy policy improvement. This reduces computational cost while still ensuring convergence to the optimal policy.\n",
    "\n",
    "The core update rule is:\n",
    "$$v_{k+1}(s) = \\max_a \\sum_{s'} \\sum_r p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right]$$\n",
    "\n",
    "Each update improves the value estimate of state $s$ by considering the best possible action to take, which is already a greedy selection. Once the value function has converged (changes are below a small threshold), the optimal policy can be derived by acting greedily with respect to the final value function\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s'} \\sum_r p(s', r \\mid s, a) \\left[ r + \\gamma v_*(s') \\right]$$\n",
    "\n",
    "Value iteration has the advantage of\n",
    "1. More efficient than policy iteration, especially in large state spaces.\n",
    "2. Combines evaluation and improvement into a single update step.\n",
    "3. Guaranteed to converge to the optimal value function and policy under standard conditions (finite MDP, $\\gamma < 1$ or episodic tasks).\n",
    "\n",
    "# DP Efficiency & Limitations\n",
    "Dynamic programming methods, such as policy iteration and value iteration, are foundational tools in reinforcement learning for solving MDPs. These methods are guaranteed to converge to the optimal policy under the right conditions and are considered efficient in a theoretical sense.\n",
    "\n",
    "DP methods are polynomial time algorithms, meaning their computational complexity grows at a manageable rate with respect to the number of states and actions. This makes them efficient in contrast to brute force approaches that might try all possible policies.\n",
    "\n",
    "There are two types of DP methods, synchronous and asynchronous DP.\n",
    "\n",
    "* Synchronous DP: the value of all states is updated simultaneously and systematically in each iteration based on the values from the previous iteration. This is conceptually simple and easy to parallelize, but can be inefficient if many states are rarely visited or irrelevant in a given policy.\n",
    "\n",
    "* Asynchronous DP: only a subset of states is updated at a time. This allows for more targeted updates focusing on states that are frequently visited or have recently changed. Asynchronous methods can often reach good solutions more quickly in practice. However, to guarantee convergence to the correct value function, all states must still be updated sufficiently often over time. This means asynchronous DP does not necessarily reduce the total number of updates needed for convergence, but it allows for smarter scheduling and prioritization of updates. In practice, this leads to more efficient use of computational resources, especially when combined with techniques like prioritized sweeping.\n",
    "\n",
    "## Limitations of DP\n",
    "However, \"efficiency\" of DP in a theoretical sense does not always translate to practicality in real world problems because of the following reasons\n",
    "\n",
    "1. Curse of Dimensionality: DP methods become intractable in high dimensional state spaces, where the number of possible states grows exponentially with the number of variables describing the state.\n",
    "\n",
    "2. Requirement of the Transition Probability: DP requires complete knowledge of the environment, specifically the transition probability function, $p(s', r \\mid s, a)$. In real world problems, this dynamic model is usually unknown, making pure DP methods inapplicable without approximation or learning.\n",
    "\n",
    "Dynamic Programming is a powerful baseline, but its real world applicability is often limited. That’s why modern reinforcement learning often turns to approximate dynamic programming, Monte Carlo methods, and temporal difference learning, which relax these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63369ea4",
   "metadata": {},
   "source": [
    "# Sampling Based Learning Methods\n",
    "In many real-world reinforcement learning scenarios, we do not have access to the full model of the environment, such as the transition probabilities or reward distributions. As a result, it becomes infeasible to apply planning methods like dynamic programming, which require full knowledge of the environment's dynamics.\n",
    "\n",
    "Sampling-based learning methods (model-free methods) address this by learning directly from sampled experiences, the actual sequences of states, actions, and rewards encountered by the agent while interacting with the environment. Instead of computing expectations over all possible next states as in DP, sampling based methods approximate value functions or policies using observed outcomes through experiences.\n",
    "\n",
    "Common examples of sampling-based methods include\n",
    "* Monte Carlo methods: Learn from complete episodes by averaging returns.\n",
    "* Temporal-Difference (TD) learning: Learn from bootstrapped estimates using partial episodes\n",
    "\n",
    "These methods are more scalable and practical in complex environments because they do not require storing or computing the full transition dynamics. However, they also come with trade-offs, such as increased variance in learning and the need for sufficient exploration to obtain a good enough estimate. In summary, sampling-based learning allows an agent to learn optimal behaviour from trial-and-error experience, without needing a complete model of the environment\n",
    "\n",
    "## Monte Carlo Method\n",
    "Monte Carlo methods estimate value functions by averaging returns obtained from multiple sampled episodes. These methods rely solely on sequences of states, actions, and rewards generated through actual interactions with the environment. Unlike DP, Monte Carlo methods do not require knowledge of the environment’s dynamics.\n",
    "\n",
    "Similar to DP, Monte Carlo methods follow the principle of Generalized Policy Iteration, where they alternate between policy evaluation (estimating how good the current policy is) and policy improvement (using the value estimates to make the policy better), eventually converging to an optimal or good enough policy.\n",
    "\n",
    "### Monte Carlo for Policy Evaluation\n",
    "To evaluate a policy using the Monte Carlo approach\n",
    "1. Given a policy, run simulations based on it to generate many episodes.\n",
    "2. Record the observed returns of each visited stated in the episode.\n",
    "3. Update the value estimate towards based on the observed return from that state.\n",
    "\n",
    "Given that episodes terminate, the return $G_t$ at each time step $t$, where the agent is in state $s_t$ can be computed backward from the end of the episode, where\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots + \\gamma^{T-t-1} r_T$$\n",
    "\n",
    "Then, we update the value function of the state $s_t$ to be\n",
    "$$v(s_t) \\leftarrow v(s_t) + \\alpha \\left[ G_t - v(s_t) \\right]$$\n",
    "where $[ G_t - v(s_t)]$ refers to the error between the observed return and the current estimates, and $\\alpha$ is the update step size or learning rate.\n",
    "\n",
    "With enough samples, the average return for each state converges to the true expected return under the given policy.\n",
    "\n",
    "### Monte Carlo vs DP\n",
    "1. No Need for a Model: Monte Carlo methods do not require access to the environment's transition or reward model. They learn directly from sampled episodes, unlike DP which requires complete knowledge of the dynamics.\n",
    "\n",
    "2. Independent State Updates: In Monte Carlo, the value of each state is computed independently by averaging returns from that specific state. In contrast, DP relies on bootstrapping, where the value of a state is a weighted sum of the values of successor states. This means\n",
    "   * Monte Carlo computation scales with the number of episodes (and their length), not the size of the state space (the size of MDP).\n",
    "   * DP computation scales with the number of states and the transition model complexity.\n",
    "\n",
    "<img src=\"https://i0.wp.com/roboticseabass.com/wp-content/uploads/2020/08/rl_intro_banner-1.png?fit=1200,448&ssl=1\" width=700>\n",
    "\n",
    "### Monte Carlo for Action-Value Estimation\n",
    "In DP, state values are sufficient to determine a policy because the model (transition probabilities and rewards) allows us to look one step ahead and select the action that leads to the best expected outcome. However, in Monte Carlo methods, where no model of the environment is available, state values alone are insufficient for decision making because even though we know the all the state values, we do not know the transition probabilities, and thus, do not know which state the agent will end up in after taking an action. Therefore, we must explicitly estimate action-value functions, $q_\\pi(s, a)$, to determine which actions are better under a given policy.\n",
    "\n",
    "To estimate action values\n",
    "1. Generate episodes using the current policy.\n",
    "2. For each state–action pair, we record the return following that pair.\n",
    "3. The estimated action value $q_\\pi(s, a)$ is the average of these observed returns.\n",
    "\n",
    "This is similar to estimating state values, except we condition on both the state and the action taken, where\n",
    "$$q(s, a) \\leftarrow q(s, a) + \\alpha \\left[ G_t - q(s, a) \\right]$$\n",
    "\n",
    "### Exploring Starts\n",
    "However, this methods has an critical issue when the agent follows a deterministic policy, and certain state–action pairs may never be visited. As a result, some action values will never be updated, and the agent cannot learn about better actions it never tries. This violates the principle of sufficient exploration, making it impossible to guarantee convergence to the optimal policy.\n",
    "\n",
    "One common solution in Monte Carlo control is exploring starts, where each episode begins from a randomly chosen state–action pair and follow the policy for the remainder of the episode. This ensures that all state–action pairs have a non-zero probability of being explored. With enough episodes, this guarantees that all $q_\\pi(s, a)$ values are visited and accurately estimated.\n",
    "\n",
    "### Monte Carlo Method for Policy Iteration\n",
    "Monte Carlo methods can be used to perform policy iteration. This is achieved through sampling episodes and estimating the action values from experience. The algorithm proceeds as follows\n",
    "\n",
    "1. Initialization\n",
    "* Initialize an arbitrary policy $\\pi$.\n",
    "* Initialize the action-value function $q_\\pi(s, a)$ arbitrarily (e.g., to zero for all $(s, a)$ pairs).\n",
    "\n",
    "2. Generate an Episode\n",
    "* Use exploring starts to ensure sufficient coverage of all state-action pairs.\n",
    "* Generate an episode by starting from a random state-action pair, then follow the current policy $\\pi$.\n",
    "* Record the sequence of $(s_t, a_t, r_{t+1})$ for the entire episode until termination.\n",
    "\n",
    "\n",
    "3. Monte Carlo Policy Evaluation\n",
    "* For each state-action pair $(s, a)$ that appears in the episode:\n",
    "     - Compute the return $G_t$ backward with discount.\n",
    "     - Update the action-value estimate $q_\\pi(s, a)$ as the sample average of all returns observed for that pair:\n",
    "    $$q_\\pi(s, a) \\leftarrow q(s, a) + \\alpha \\left[ G_t - q(s, a) \\right]$$\n",
    "\n",
    "Note: Only the $(s, a)$ pairs actually visited in the episode are updated; others remain unchanged. In theory, an infinite number of episodes ensures accurate value estimation for all pairs. In practice, however, we use limited episodes, meaning early updates may be noisy, but over many iterations, the estimates improve.\n",
    "\n",
    "4. Policy Improvement\n",
    "* For each state $s$ visited in the episode, improve the policy by choosing the action with the highest estimated value to make the policy greedy with respect to the current $q_\\pi$, where\n",
    "  $$\\pi_{\\text{new}}(s) = \\arg\\max_a q_\\pi(s, a)$$\n",
    "\n",
    "5. Repeat\n",
    "* Return to Step 2 and continue until the policy converges, or for a fixed number of iterations.\n",
    "\n",
    "Note: Using one or few episode per iteration is analogous to stochastic gradient descent, where each update may be noisy, but the overall trend improves the policy over time. The use of exploring starts ensures that all state-action pairs have a non-zero chance of being updated, which is critical for convergence guarantees.\n",
    "\n",
    "### Monte Carlo for Action-Value Without Exploring Starts\n",
    "In Monte Carlo control with exploring starts, every state-action pair must have a non-zero probability of being the initial pair in an episode. This guarantees that all $(s, a)$ pairs are eventually visited, which is critical for ensuring convergence to the optimal policy. However, exploring starts are impractical in real world problems, especially those with continuous or infinite state-action spaces, like self-driving cars, where it’s impossible to initialize the agent in every possible situation.\n",
    "\n",
    "### $\\boldsymbol{\\varepsilon}$-Soft Policies\n",
    "To ensure sufficient exploration without exploring starts, we use $\\varepsilon$-soft policies, which are stochastic policies that assign non-zero probability to all actions in every state, where\n",
    "\n",
    "$$\\pi(a \\mid s) \\geq \\frac{\\varepsilon}{|\\mathcal{A}(s)|}$$\n",
    "\n",
    "This guarantees that every action in every state has a chance of being selected with at least probability of $\\frac{\\varepsilon}{\\text{Number of possible actions}}$. Thus, all state-action pairs are eventually explored with enough episodes. Both $\\varepsilon$-greedy policy and uniform random policy are examples of $\\varepsilon$-soft policies\n",
    "\n",
    "Using an $\\varepsilon$-soft policy ensures exploration and removes the need for exploring starts. However, there’s a trade-off, where the agent can only converge to the optimal $\\varepsilon$-soft policy, not the optimal deterministic policy. Even after learning a near-optimal policy, the agent still occasionally explores due to the non-zero $\\varepsilon$.\n",
    "\n",
    "In theory, if each state-action pair is visited many times, its action-value estimate will converge towards the true value, and if we are confident enough about the estimation, we can stop exploring and commit to a greedy policy.\n",
    "\n",
    "### Monte Carlo Control with $\\varepsilon$-Soft Policy (Policy Iteration)\n",
    "The process is similar to standard Monte Carlo policy iteration, with the key difference being the use of stochastic policies:\n",
    "\n",
    "1. Initialize:\n",
    "   - Initialize a random $\\varepsilon$-soft policy $\\pi$.\n",
    "   - Initialize action-value estimates $q(s, a)$ arbitrarily.\n",
    "\n",
    "2. Generate Episode:\n",
    "   - Generate an episode following the current $\\varepsilon$-soft policy.\n",
    "   - Record the sequence of $(s_t, a_t, r_{t+1})$.\n",
    "\n",
    "3. Policy Evaluation:\n",
    "   - For each state-action pair in the episode:\n",
    "     - Compute return $G_t$ and update $q(s, a)$ as the sample average.\n",
    "\n",
    "4. Policy Improvement:\n",
    "   - For each state $s$ visited:\n",
    "     - Make the policy $\\varepsilon$-greedy with respect to the current $q(s, a)$:\n",
    "       $$\n",
    "       \\pi(a \\mid s) =\n",
    "       \\begin{cases}\n",
    "       1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|}, & \\text{if } a = \\arg\\max_a q(s, a) \\\\\n",
    "       \\frac{\\varepsilon}{|\\mathcal{A}(s)|}, & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "       $$\n",
    "\n",
    "5. Repeat steps 2–4 until convergence.\n",
    "6. Convert $\\varepsilon$-soft policy to greedy policy.\n",
    "\n",
    "Overall, $\\varepsilon$-soft policy solves the problem of insufficient exploration with a large number of possible state-action pairs, and is able to achieve a good enough, sub-optimal stochastic policy. Once all possible state-action pairs are visited enough number of times, we can convert the policy into a greedy one.\n",
    "\n",
    "In tabular Monte Carlo, both exploring start and $\\varepsilon$-greedy ensure asymptotic convergence to the optimal policy given infinite episodes and satisfying specific conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a2dfc",
   "metadata": {},
   "source": [
    "# On-Policy vs Off-Policy Learning\n",
    "In reinforcement learning, there are two fundamental paradigms for learning from interaction, on-policy and off-policy learning. The key difference lies in whether the policy being learned is the same as the policy used to generate the data.\n",
    "\n",
    "## On-Policy Learning\n",
    "In on-policy learning, the agent learns about and improves the same policy that it uses to make decisions. This means the data used for learning is generated by following the current policy, which typically includes a balance of exploitation and exploration. \n",
    "\n",
    "**Pros:**\n",
    "1. Conceptually simpler.\n",
    "2. Lower variance in value estimates since learning is based on the actual policy being executed.\n",
    "\n",
    "**Cons:**\n",
    "1. Exploration is constrained by the policy itself.\n",
    "2. May learn slower due to limited exploration.\n",
    "\n",
    "## Off-Policy Learning\n",
    "In off-policy learning, the agent learns about a different policy than the one used to generate the data. There are two distinct policies involved:\n",
    "\n",
    "1. Target policy: the policy the agent is trying to learn or evaluate, denoted by $\\pi(a \\mid s)$\n",
    "2. Behaviour policy: the policy used to generate data (select actions), denoted by $b(a \\mid s)$\n",
    "\n",
    "This allows the agent to learn an optimal or deterministic target policy while still exploring using a stochastic behaviour policy.\n",
    "\n",
    "**Pros:**\n",
    "1. More general and powerful; can learn from data generated by other agents or past experiences.\n",
    "2. Enables stronger exploration while still converging to an optimal deterministic policy.\n",
    "\n",
    "**Cons:**\n",
    "1. Higher variance in updates, especially when using importance sampling.\n",
    "2. Requires careful alignment between behaviour and target policies.\n",
    "\n",
    "For off-policy learning to be valid, the behaviour policy must cover all the actions that the target policy might take. That is:\n",
    "$$\\text{If } \\pi(a \\mid s) > 0, \\text{ then } b(a \\mid s) > 0$$\n",
    "\n",
    "This ensures that the agent gets enough data to estimate the value of the target policy correctly. If the behaviour policy never explores certain actions that the target policy might select, it is impossible to evaluate or improve the target policy accurately.\n",
    "\n",
    "## Importance Sampling\n",
    "Importance sampling is a fundamental technique used in off-policy reinforcement learning. It allows us to estimate expectations under one probability distribution (the target policy) using samples drawn from a different distribution (the behaviour policy). This is crucial in off-policy learning, where the agent aims to learn or evaluate a policy that is different from the one used to generate data.\n",
    "\n",
    "In off-policy learning, we want to compute the expected return or value function under the target policy $\\pi$, but the agent collects data using the behaviour policy $b$, which is, generally, a more exploratory policy. Since the distrubtion of the target and behaviour policies are different, we need a way to \"correct\" for the difference in distributions, which is importance sampling.\n",
    "\n",
    "### The Mathematics\n",
    "Suppose we want to estimate the expected value of some function $f(x)$ under a target distribution $p(x)$, but we only have samples from a different distribution $q(x)$. Then:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p}[f(x)] = \\sum_x p(x) f(x) = \\sum_x \\frac{p(x)}{q(x)} q(x) f(x) = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} f(x)\\right]\n",
    "$$\n",
    "\n",
    "Here, $\\frac{p(x)}{q(x)}$ is the importance sampling ratio, which reweights the samples to reflect their importance under the target distribution. The importance sampling ratio reflects the ratio between the two distributions, which allows us to estimate expectations under one distribution using samples from another by accounting for the difference in likelihood between the two. The significance here is that we can compute the expected value of one distribution by computing the expectation from another distribution.\n",
    "\n",
    "### Importance Sampling in Reinforcement Learning\n",
    "In the context of RL, the goal is to estimate the expected return under a target policy $\\pi$, but the data (state-action-reward trajectories) was generated by a different behaviour policy $b$. Even if the actions taken differ, both the target policy $\\pi$ and the behaviour policy $b$ interact with the same environment. This means the state transition dynamics are the same for both, since they are governed by the MDP and do not depend on the policy itself. As a result, the only difference between trajectories generated by $b$ and those from $\\pi$ lies in how actions are selected at each state, which is the difference in the distributions of the policies. By accounting for this difference with importance sampling, we can accurately estimate the expected return of the target policy $\\pi$ using samples generated by the behaviour policy $b$.\n",
    "\n",
    "* Let $\\pi(a_t \\mid s_t)$ be the probability of taking action $a_t$ in state $s_t$ under the target policy $\\pi$.\n",
    "* Let $b(a_t \\mid s_t)$ be the probability of taking the same action under the behaviour policy $b$.\n",
    "\n",
    "At time step $t$, the importance sampling ratio is\n",
    "$$\\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}$$\n",
    "\n",
    "This ratio tells us how to reweight a sample from the behaviour policy $b$ to estimate its expected contribution under the target policy $\\pi$. In other words, it represents the ratio of the likelihood of taking the same action $a$ in the same state $s$ between the two policies.\n",
    "\n",
    "* If $\\rho_t$ is small, it means this action was much more likely under $b$ than under $\\pi$, so this sample is less relevant to what $\\pi$ would do. We downweight it.\n",
    "* If $\\rho_t$ is large, it means this action is more consistent with what $\\pi$ would have done. We upweight it.\n",
    "\n",
    "Essentially, $\\rho_t$ tells us how much a sample from $b$ should count when estimating what $\\pi$ would do. It adjusts for the differences of the distributions between the two policies, so that we can use data from one policy to estimate the behaviour of another.\n",
    "\n",
    "To estimate the return under the target policy $\\pi$ over a full trajectory of length $T$, we compute the cumulative importance sampling ratio from time $t$ onward, where\n",
    "\n",
    "$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)} = \\rho_t \\cdot \\rho_{t+1} \\cdots \\rho_{T-1}$$\n",
    "\n",
    "This cumulative ratio adjusts for the differences between the target policy $\\pi$ and the behaviour policy $b$ at each timestep along the trajectory. It effectively reweights the entire trajectory so that it reflects how likely the actions would be under the target policy. Then, given a trajectory generated by $b$, we can estimate the expected return under the target policy $\\pi$ using:\n",
    "\n",
    "$$v_\\pi(s_t) = \\mathbb{E}_b[\\rho_{t:T-1} \\cdot G_t \\mid s_t] \\approx \\frac{1}{N} \\sum_{i=1}^N \\rho^{(i)} G^{(i)}$$\n",
    "\n",
    "* $G_t$: the observed return from time $t$ onward based on the behaviour policy $b$, where $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots$\n",
    "* $\\rho_{t:T-1}$: the cumulative importance sampling ratio\n",
    "* $\\rho^{(i)} G^{(i)}$: one reweighted return from the $i$-th trajectory.\n",
    "* $N$: the total number of trajectories sampled from the behaviour policy $b$.\n",
    "\n",
    "So if we have multiple trajectories, we compute the adjusted return for each using $\\rho^{(i)} G^{(i)}$, and then average them to estimate the expected return under the target policy.\n",
    "\n",
    "## Off-Policy Monte Carlo Method\n",
    "Off-policy Monte Carlo learning enables us to evaluate and improve a target policy $\\pi$ using episodes generated from a different behaviour policy $b$. The target policy can be either deterministic or stochastic, but the behavior policy must be stochastic. The only essential requirement is the coverage condition as discussed before, where\n",
    "\n",
    "$$\\text{If } \\pi(a \\mid s) > 0, \\text{ then } b(a \\mid s) > 0$$\n",
    "\n",
    "However, a key issue is that the importance sampling ratio may have extremely high variance if $\\rho_{t:T-1}$ is large. This will cause an extremely large update to the estimation, which is undesirable. Therefore, we use a weighted version of importance sampling, where $v_\\pi(s) \\approx \\frac{\\sum_{i=1}^N \\rho^{(i)} G^{(i)}}{\\sum_{i=1}^N \\rho^{(i)}}$. This updates will cause more bias in the estimation because we normalize by the sum of weights to reduce variance and prevent the estimated value from explosion. In practice, we use $c(s, a)$ to track the sum of the ratios of each state-action pair.\n",
    "\n",
    "### Algorithm Steps\n",
    "1. Initialize:\n",
    "   - $q(s, a) \\in \\mathbb{R}$ arbitrarily for all $s, a$\n",
    "   - $c(s, a) = 0$ (Cumulative denominator weight) for all $s, a$\n",
    "   - Target policy $\\pi(s)$ is the greedy policy with respect to $q$\n",
    "\n",
    "2. Loop (for each episode):\n",
    "   - Generate Episode:\n",
    "     Generate an episode using behavior policy $b$ (e.g., $\\epsilon$-soft):\n",
    "     $S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T$\n",
    "\n",
    "   - Initialize Weight:\n",
    "     $w \\leftarrow 1.0$\n",
    "     $G \\leftarrow 0$\n",
    "\n",
    "   - Loop backward (from $t = T-1$ down to $0$):\n",
    "     1. Update Return:\n",
    "     $G \\leftarrow \\gamma G + R_{t+1}$\n",
    "\n",
    "     2. Update Cumulative Weight:\n",
    "        $c(S_t, A_t) \\leftarrow c(S_t, A_t) + w$\n",
    "\n",
    "     3. Update Q-Value (Incremental Weighted Average):\n",
    "        $$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{w}{c(S_t, A_t)} \\big(G - q(S_t, A_t) \\big)$$\n",
    "\n",
    "     4. Policy Improvement (Implicit):\n",
    "        $\\pi(S_t) \\leftarrow \\arg\\max_a q(S_t, a)$\n",
    "\n",
    "     5. Exit Condition:\n",
    "        If $A_t \\neq \\pi(S_t)$:\n",
    "            Break loop (Weight becomes 0, so no past steps learn from this because this action will never happens in the greedy case). This causes lots of waste in data.\n",
    "\n",
    "     6. Update Importance Sampling Weight:\n",
    "        $$w \\leftarrow w \\cdot \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$$\n",
    "        *(Note: Since $\\pi$ is deterministic, $\\pi(A_t|S_t)$ is either 1 or 0. If we didn't break in Step 5, it is 1).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524dbf8",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "Temporal Difference (TD) learning is a fundamental reinforcement learning method that blends the strengths of Dynamic Programming (DP) and Monte Carlo (MC) methods.\n",
    "\n",
    "* Like Monte Carlo, TD learning can learn directly from experience, using the sequences of states, actions, and rewards without requiring a model of the environment’s dynamics.\n",
    "* Like Dynamic Programming, TD learning uses bootstrapping, meaning it updates value estimates using other learned estimates rather than waiting for the final outcome of an episode.\n",
    "\n",
    "This means TD learning can update the value of a state immediately after the next state is observed, making it more efficient and suitable for online, incremental learning, even in continuing (non-episodic) tasks.\n",
    "\n",
    "## TD Prediction\n",
    "TD learning also learns from experience, which are the trajectories generated by following a policy $\\pi$. In MC, the value of a state is updated using the full return from that state until the end of the episode\n",
    "\n",
    "$$v(S_t) \\leftarrow v(S_t) + \\alpha \\left[ G_t - v(S_t) \\right]$$\n",
    "\n",
    "where $G_t$ is the return observed from time step $t$ to the end of the episode. Since this return is computed recursively from the full episode, MC must wait for the episode to finish before updating any values. In contrast, TD learning uses bootstrapping. Instead of waiting for the full return, it updates the value of a state using the observed immediate reward and the estimated value of the next state, where\n",
    "\n",
    "$$v(S_t) \\leftarrow v(S_t) + \\alpha \\left[ R_{t+1} + \\gamma v(S_{t+1}) - v(S_t) \\right]$$\n",
    "\n",
    "This means at each time step, TD can immediately update the previous state’s value using the reward and the estimated value of the next state, without waiting for the episode to complete.\n",
    "\n",
    "The term  \n",
    "$$\\delta_t = R_{t+1} + \\gamma v(S_{t+1}) - v(S_t)$$\n",
    "is called the TD error, which is the difference between the estimated return based on the new sample and the previous estimate. This algorithm is known as TD(0) or one-step TD.\n",
    "\n",
    "\n",
    "### TD(0) Algorithm for Policy Evaluation\n",
    "1. Given:\n",
    "   - A policy $\\pi$ to evaluate.\n",
    "   - A learning rate (step size) $\\alpha \\in (0,1]$.\n",
    "   - A discount factor $\\gamma \\in [0,1]$.\n",
    "\n",
    "2. Initialize:\n",
    "   - Value function $v(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n",
    "\n",
    "3. Repeat (over episodes):\n",
    "   - Initialize starting state $S_0$\n",
    "   - For each timestep $t$ in the episode:\n",
    "     - Choose action $A_t \\sim \\pi(\\cdot \\mid S_t)$\n",
    "     - Observe reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "     - Update the value of the current state:\n",
    "       $$v(S_t) \\leftarrow v(S_t) + \\alpha \\left[ R_{t+1} + \\gamma v(S_{t+1}) - v(S_t) \\right]$$\n",
    "\n",
    "4. Continue:\n",
    "   - Repeat episodes until convergence of $v(s)$\n",
    "\n",
    "## Advantages of TD\n",
    "1. Model-Free: TD methods do not require a model of the environment’s dynamics (transition probabilities or reward function), making them applicable to unknown or complex environments.\n",
    "\n",
    "2. Online and Incremental Updates: TD can update value estimates after every step without waiting for the episode to finish. This is especially beneficial for problems with long episodes or continuing (non-terminating) tasks.\n",
    "\n",
    "3. Faster Convergence in Stochastic Environments: In many stochastic tasks, TD methods tend to converge faster and with less variance than Monte Carlo methods, since they bootstrap from existing estimates rather than relying solely on complete returns so it is more sample efficient.\n",
    "\n",
    "<img src=\"https://towardsdatascience.com/wp-content/uploads/2023/08/1kGWG1Z3TxHg-dGbZ7DLR1g.png\" width=500>\n",
    "\n",
    "## Bias and Variance Tradeoff\n",
    "Bias refers to the systematic error between the agent's target estimation and the true value. Variance refers to how much the target estimation fluctuates across different episodes for the same state.\n",
    "\n",
    "* Monte Carlo (Low Bias, High Variance): MC relies on the actual total return ($G_t$) from a completed episode. Because the target is calculated using real, completed experience, it is an unbiased estimate of the true value. However, the variance is high because the return depends on the accumulated randomness of every single action and state transition in the entire episode. One lucky or unlucky step early on can drastically swing the final total. As you collect more samples, the average of these high-variance samples will eventually converge to the true value.\n",
    "\n",
    "* TD (High Bias, Low Variance): TD relies on bootstrapping, meaning it updates its estimate using its own existing guess ($V(s)$) rather than the real final outcome. Because the existing guess is likely imperfect (especially at the start), the target is systematically incorrect, leading to high bias. The high bias is generally not a problem, but under certain setting the estimation might diverge. However, the variance is low because the target depends on only one step of randomness (the immediate next state and reward), ignoring the randomness of the rest of the episode.\n",
    "   \n",
    "## n-Step TD Algorithm\n",
    "n-Step TD is the generalization of TD(0) and Monte Carlo. While TD(0) updates based on just the immediate next step, n-Step TD looks $n$ steps into the future before bootstrapping.\n",
    "\n",
    "Therefore, the n-step Return ($G_{t:t+n}$) is calculated as the sum of the first $n$ rewards plus the estimated value of the state $n$ steps later\n",
    "\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})$$\n",
    "\n",
    "And the Value Update Rule is\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left[ G_{t:t+n} - V(S_t) \\right]$$\n",
    "\n",
    "By increasing the value of $n$, the algorithm looks further into the future to use more observed real rewards ($R$) instead of relying heavily on the estimation of the next state's value ($V$).\n",
    "* As $n$ increases: The Bias decreases (closer to real data), but the Variance increases (more random steps accumulated).\n",
    "* As $n$ decreases: The Bias increases (more bootstrapping), but the Variance decreases.\n",
    "\n",
    "Conceptually, Monte Carlo (MC) is essentially TD($\\infty$), where the agent looks infinite timesteps into the future (or until the episode terminates), using only real rewards and no bootstrapping.\n",
    "\n",
    "In n-Step TD algorithms, the parameter $n$ must be tuned carefully to find the \"sweet spot\" in the error curve:\n",
    "1. Initial Improvement: As $n$ increases from 1, the prediction error usually drops because the bias is strictly decreasing. We are relying less on the potentially faulty initial value estimates.\n",
    "2. The Turning Point: However, if $n$ becomes too large, the error starts to increase again. At this point, the bias is already very low (very close to pure MC), but the variance begins to dominate. The target becomes too noisy because it includes too many random transitions.\n",
    "\n",
    "Therefore, the optimal $n$ is usually an intermediate value that balances the stability of TD with the accuracy of MC, which means balancing the bias and variance within the estimations.\n",
    "\n",
    "<img src=\"https://lcalem.github.io/imgs/sutton/nstep_graph.png\" width=700>\n",
    "\n",
    "## TD($\\lambda$)\n",
    "n-Step TD looks exactly n steps ahead and then applies bootstrapping. However, this method requires picking a specific $n$ to balance bias and variance, which is brittle and difficult to tune. TD($\\lambda$) solves this by using all available n-step horizons simultaneously. It calculates a weighted average of all n-step returns. This approach utilizes more data and replaces the hard choice of a specific $n$ with a smoother parameter $\\lambda$. The weighting follows an exponential decay, placing more emphasis on shorter n-step returns (targets closer to the current state). This ensures that immediate, lower-variance estimates contribute most to the update, while longer-term returns still help correct bias.\n",
    "\n",
    "For a timestep $t$ in a trajectory, we define its n-step return ($G_t^{(n)}$) to be a target constructed by summing actual rewards for $n$ steps, and then bootstrapping (estimating) the rest of the value function.\n",
    "\n",
    "* 1-Step Return (TD(0)):\n",
    "    $$G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})$$\n",
    "    * Estimate the current states with the estimation of the next state. High bias, low variance.\n",
    "\n",
    "* 2-Step Return:\n",
    "    $$G_t^{(2)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})$$\n",
    "    * Use more real reward from experiment and less estimation of next state. Decreasing bias, increasing variance.\n",
    "\n",
    "* Infinite-Step Return (TD($\\infty$) = Monte Carlo):\n",
    "    $$G_t^{(\\infty)} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-1} R_T$$\n",
    "    * Use pure reward signal without estimation of another state. Zero bias, high variance.\n",
    "\n",
    "n-step TD will only select one of the step to use, but TD($\\lambda$) uses all of the n-step returns to calculate a weighted average of every possible horizon. We define a new target called the $\\lambda$-Return ($G_t^\\lambda$), where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t^\\lambda &= (1 - \\lambda) \\sum_{n=1}^{L-1} \\lambda^{n-1} G_t^{(n)} + \\lambda^{L-1} G_t^{(L)}= (1 - \\lambda) \\cdot \\underbrace{\\left( R_{t+1} + \\gamma V(S_{t+1}) \\right)}_{\\text{1-step return } (G_t^{(1)})} \n",
    "+ (1 - \\lambda)\\lambda \\cdot \\underbrace{\\left( R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2}) \\right)}_{\\text{2-step return } (G_t^{(2)})} + (1 - \\lambda)\\lambda^2 \\cdot \\underbrace{\\left( R_{t+1} + \\gamma R_{t+2} + \\gamma^3 R_{t+3} + \\gamma^3 V(S_{t+3}) \\right)}_{\\text{3-step return } (G_t^{(3)})}\n",
    "+ \\dots + \\lambda^{L-1} G_t^{(L)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* $G_t^{(n)}$: the n-step return for the a state at timestep $t$.\n",
    "* $L$: the length of an episode\n",
    "* $\\lambda$: the decay factor between 0 and 1, so as $n$ increases (looking further into the future), the weight we assign to that return decays because we care more about the more recent returns.\n",
    "* $(1 - \\lambda)$: a normalization constant that ensures that all the weights sum up to exactly 1, so the final target is mathematically valid.\n",
    "\n",
    "Note: the final return of the episode is weighted by $\\lambda^{L-1}$ because this ensures the sum of weight is always 1 for any episode length $L$, where\n",
    "$(1 - \\lambda)\\sum^{L-1}_{n=1}\\lambda^{n-1} + \\lambda^{L-1} = 1$\n",
    "\n",
    "The parameter $\\lambda$ acts as a slider for how much you trust \"deep\" future data versus your current immediate estimate. A smaller $\\lambda$ means putting more empahsis onto the recent data, where a larger $\\lambda$ means putting more weights onto the future data.\n",
    "\n",
    "* If $\\lambda = 0$ (TD-0):\n",
    "    The formula collapses. The weight for the 1-step return becomes 1, and all other weights become 0, so only the immediate next step is considered, which is equivalent to TD-0.\n",
    "\n",
    "* If $\\lambda = 1$ (Monte Carlo):\n",
    "    The formula collapses to $G_t^\\lambda = G_t^{(L)}$. The algorithm only use the longest possible return (the full episode), which is equivalent to Monte-Carlo\n",
    "\n",
    "* If $0 < \\lambda < 1$ (TD Lambda):\n",
    "    Balanced approach, where the immediate steps have the highest weight (low variance) while the distant steps still contribute, correcting the bias of your initial estimates.\n",
    "\n",
    "The algorithm at each timstep uses all valid $n$-step returns available from the current state until the terminal state, so the earlier timestep will have more data avalible and timestep closer to the terminal state will have less data avaliable.\n",
    "\n",
    "* At the beginning of the trajectory, the agent looks ahead at 1-step, 2-step, 3-step... all the way to the final step $T$ (the Monte Carlo return) and calculates a weighted average of all these returns, which blends the low-variance bias of TD(0) with the high-variance unbiasedness of Monte Carlo. This method maximizes the data utilization of the entire trajectory\n",
    "\n",
    "* At the end of the trajectory, the agent does not have any future steps. At step $T-1$, the only valid return is the 1-step return ($R_T + \\gamma V(S_T)$), which effectively behaving like TD(0). This is okay since that's the only data avaliable\n",
    "\n",
    "### The Forward View\n",
    "The reason above explanation is often separated from implementation is because this is the forward view of the algorithm, so in order to compute $G_t^{(n)}$, we need to wait for the entire episode to terminate, making it impossible perform online learning.\n",
    "\n",
    "<img src=\"https://towardsdatascience.com/wp-content/uploads/2019/08/1L-LUOyW5W-0gBxx80GHdHQ.png\" width=500>\n",
    "\n",
    "### The Backward View (Eligibility Tracing)\n",
    "To make the TD($\\lambda$) learning online, we use a technique called eligibility tracing. Instead of \"looking forward\" to compute the return (which is impossible during online learning), eligibility tracing allows the current error to \"look backward\" and update the past states that contributed to the current situation.\n",
    "\n",
    "To do this, the agent maintains an eligibility trace, $E(s)$, for every state, initialized to 0. At each timestep $t$:\n",
    "1. Decay: The eligibility of all states is decayed by a factor of $\\gamma \\lambda \\in (0, 1)$.\n",
    "2. Increment: The eligibility of the current state $S_t$ is increased by 1 (Accumulating Trace), indicating it was just visited.\n",
    "\n",
    "Then, we calculate the current TD error ($\\delta_t$):\n",
    "$$\\delta_t = R_{t+1} + \\gamma v(S_{t+1}) - v(S_t)$$\n",
    "\n",
    "Crucially, we then update the value of every state $s$ (not just the current one), scaled by its eligibility trace:\n",
    "$$v(s) \\leftarrow v(s) + \\alpha \\cdot \\delta_t \\cdot E_t(s)$$\n",
    "\n",
    "This allows online learning because the update at timestep $t$ relies only on available data ($S_t, R_{t+1}, S_{t+1}$). \n",
    "\n",
    "These updates are initially imperfect because they rely on estimated values of future state ($v(S_{t+1})$) that may contain errors. However, the mechanism allows for continuous correction:\n",
    "* When the agent reaches a future state and finds there is an error in its previous estimation, $\\delta_{future}$, this new error is broadcast backwards because  we need to correct all the past estimations that used this estimation that contains an error.\n",
    "* However, the error is scaled down by $\\gamma \\lambda$ because if this state that contains error is in the far future, the impact of its error will reduced by a factor of $\\lambda$ every timestep, so when we're correcting the error, the correction term also need to be scaled down by its eligibility $E(s)$.\n",
    "\n",
    "At the end of an episode, the learned value function will be the same as performing the forward view since both views are mathematically the same.\n",
    "\n",
    "## TD for Control\n",
    "When using TD learning for control, knowing the optimal state-value function from policy evaluation alone is not enough in a model-free setting, because we do not know the environment’s transition dynamics. Without a model, we can’t directly compute which action leads to the best next state from just $v(s)$. Instead, we learn the action-value function $q(s, a)$, which estimates the expected return for taking action $a$ in state $s$ and then following the policy. This allows us to both evaluate and improve the policy directly. We will address 3 different TD control algorithms: SARSA, Q-learning, and expected SARSA\n",
    "\n",
    "\n",
    "### SARSA (On-Policy TD Control)\n",
    "SARSA is an on-policy TD control algorithm that learns the optimal action-value function. It is similar to the TD(0) method for state-value estimation, but instead of updating from state to state, it updates from state-action pair to state-action pair, where\n",
    "\n",
    "$$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\, q(S_{t+1}, A_{t+1}) - q(S_t, A_t) \\right]$$\n",
    "\n",
    "This update requires the five key events:\n",
    "- $(S_t, A_t)$: current state-action pair\n",
    "- $R_{t+1}$: reward from taking $A_t$ in $S_t$\n",
    "- $(S_{t+1}, A_{t+1})$: next state-action pair chosen according to the current policy\n",
    "\n",
    "Because SARSA uses $A_{t+1}$ from the current policy, it learns the value of the policy as it is being followed (on-policy learning). After each update, the policy can be improved by making it greedy or $\\epsilon$-greedy with respect to the updated $q(s, a)$.\n",
    "\n",
    "For the estimation return, SARSA uses $R_{t+1} + \\gamma \\, q(S_{t+1}, A_{t+1})$ instead of $R_{t+1} + \\gamma \\, v(S_{t+1})$ because $q(S_{t+1}, A_{t+1})$ is a random sample of the average return By the law of large numbers, if we sample from the policy enough times, the average of your updates will converge to the true value for $v(s)$.\n",
    "\n",
    "### SARSA Algorithm\n",
    "1. Given:\n",
    "   - Initial policy $\\pi$ (e.g., $\\epsilon$-greedy)\n",
    "   - Learning rate $\\alpha \\in (0, 1]$\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "\n",
    "2. Initialize:\n",
    "   - $q(s, a)$ arbitrarily for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - Initialize starting state $S_0$\n",
    "   - Choose action $A_0 \\sim \\pi(\\cdot \\mid S_0)$\n",
    "   - For each timestep $t$:\n",
    "     - Take action $A_t$, observe the reward $R_{t+1}$ and the next state $S_{t+1}$\n",
    "     - Choose the next action according to the current policy $A_{t+1} \\sim \\pi(\\cdot \\mid S_{t+1})$\n",
    "     - Update:\n",
    "       $$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\, q(S_{t+1}, A_{t+1}) - q(S_t, A_t) \\right]$$\n",
    "     - Update $\\pi$ to be greedy or $\\epsilon$-greedy with respect to $q$\n",
    "\n",
    "4. Repeat until convergence:\n",
    "   - Continue until $q(s, a)$ converges.\n",
    "\n",
    "Note: MC methods update only after an episode finishes. If an agent becomes trapped in a state (or a loop), the episode may never end, preventing any learning from occurring. In contrast, online learning methods such as SARSA update their estimates during the episode at every timestep. This allows the agent to quickly recognize that a policy leading to such traps is poor and adjust its behavior before the episode is over.\n",
    "\n",
    "However, SARSA makes one update to one state-action pair per timestep, this means that the policy barely changes in each iteration because only one entry of the $q$ function changes, making it slow to convergence. To solve this problem, we can use TD($\\lambda$) with eligibility tracing instead of TD(0), which gives SARSA($\\lambda$).\n",
    "\n",
    "### SARSA($\\lambda$) Algorithm\n",
    "1. Given:\n",
    "    - Initial policy $\\pi$ (e.g., $\\epsilon$-greedy)\n",
    "    - Learning rate $\\alpha \\in (0, 1]$\n",
    "    - Discount factor $\\gamma \\in [0, 1]$\n",
    "    - Trace decay rate $\\lambda \\in [0, 1]$\n",
    "\n",
    "2. Initialize:\n",
    "    - $q(s, a)$ arbitrarily for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$\n",
    "    - $E(s, a) = 0$ for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "    - Initialize starting state $S$\n",
    "    - Choose action $A \\sim \\pi(\\cdot \\mid S)$\n",
    "    - Reset eligibility traces: $E(s, a) = 0$ for all $s, a$\n",
    "\n",
    "    - For each timestep (until $S$ is terminal):\n",
    "        - Take action $A$, observe the reward $R$ and the next state $S'$\n",
    "        - Choose the next action according to the current policy $A' \\sim \\pi(\\cdot \\mid S')$\n",
    "     \n",
    "        - Calculate TD Error:\n",
    "            $$\\delta \\leftarrow R + \\gamma q(S', A') - q(S, A)$$\n",
    "\n",
    "        - Increment Eligibility Trace for current pair:\n",
    "            $$E(S, A) \\leftarrow E(S, A) + 1$$\n",
    "\n",
    "        - Update ALL state-action pairs:\n",
    "           For all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$:\n",
    "           $$q(s, a) \\leftarrow q(s, a) + \\alpha \\cdot \\delta \\cdot E(s, a)$$\n",
    "           $$E(s, a) \\leftarrow \\gamma \\lambda E(s, a)$$\n",
    "\n",
    "         - Transition:\n",
    "           $$S \\leftarrow S'$$\n",
    "           $$A \\leftarrow A'$$\n",
    "         - Update $\\pi$ (Implicitly updated via changes in $q$)\n",
    "\n",
    "4. Repeat until convergence.\n",
    "\n",
    "Note: since we need to track the eligibility of all state-action pairs, the dimension of $E$ is $(S, A)$\n",
    "\n",
    "For SARSA($\\lambda$), all previously visited states-action pairs will be updated in each iteration due to eligibility tracing. Therefore, it leads to faster convergence than SARSA.\n",
    "\n",
    "## Q-Learning\n",
    "Q-learning is an off-policy TD control algorithm that directly learns the optimal action-value function $q_*$, rather than the action-value function of the current policy $q_\\pi$. This differs from SARSA (on-policy), which learns $q_\\pi$ for the policy being followed. The Q-learning update rule is\n",
    "\n",
    "$$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a q(S_{t+1}, a) - q(S_t, A_t) \\right]$$\n",
    "\n",
    "* The max operator selects the value of the best possible next action, regardless of the current policy.\n",
    "* This makes Q-learning off-policy: it learns as if the agent is always acting greedily, even if it is exploring in practice.\n",
    "\n",
    "Both SARSA and Q-learning are sample-based solutions to the Bellman equations and can converge to the true action-value functions given enough samples. However there's a key difference in which Bellman equation the algorithm is solving\n",
    "\n",
    "- SARSA solves the Bellman expectation equation for $q_\\pi$, where\n",
    "  $$q_\\pi(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a' \\mid s') q_\\pi(s', a') \\right]$$\n",
    "\n",
    "- Q-learning solves the Bellman optimality equation for $q_*$, where\n",
    "  $$q_*(s, a) = \\sum_{s'} \\sum_{r} p(s', r \\mid s, a) \\left[ r + \\gamma \\max_{a'} q_*(s', a') \\right]$$\n",
    "\n",
    "\n",
    "Therefore, SARSA need to follow policy iteration by first estimate $q_\\pi$ from experience, then make the policy greedy (or $\\epsilon$-greedy) with respect to $q_\\pi$. On the other hand, Q-learning effectively uses value iteration. It updates $q(s,a)$ as if it were already the optimal $q_*$, combining policy evaluation and improvement in a single step during the update.\n",
    "\n",
    "Q-learning is an off-policy algorithm because it always learns from the value of the best possible action it could take in the next state, rather than the value of the action it actually took. In this case, the target policy (the policy Q-learning is trying to learn) is always greedy with respect to the current action-value estimates, which is not needed during learning because we only aim to get the optimal action-value function, $q_*$. Once $q_*$ is obtained, we can directly extract the optimal policy from it by acting greedily with respect to $q_*$. The behaviour policy (the policy used to select actions during learning) can be any exploratory policy, such as $\\epsilon$-greedy or even random, as long as it ensures sufficient exploration. This separation between the behavior policy and the target policy is what makes Q-learning off-policy. \n",
    "\n",
    "Although Q-learning is an off-policy algorithm, it does not require importance sampling to reweight trajectories. Importance sampling is typically used to correct for differences in action-selection probabilities between the target policy and the behavior policy. However, Q-learning’s update rule always evaluates the next state using the greedy action with respect to the current action-value estimates, regardless of the action actually taken by the behavior policy. Because the target in the update is determined solely by the current Q-values and not by the behavior policy’s action probabilities, no such correction is needed.\n",
    "\n",
    "### Q-Learning Algorithm\n",
    "1. Given:\n",
    "   - Learning rate $\\alpha \\in (0, 1]$\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "\n",
    "2. Initialize:\n",
    "   - $q(s, a)$ arbitrarily for all $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - Initialize starting state $S_0$\n",
    "   - Choose initial action $A_0$ using an exploration policy (e.g., $\\epsilon$-greedy based on $q$)\n",
    "   - For each timestep $t$:\n",
    "     - Take action $A_t$, observe reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "     - Update:\n",
    "       $$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a q(S_{t+1}, a) - q(S_t, A_t) \\right]$$\n",
    "     - Choose next action $A_{t+1}$ using the exploration policy\n",
    "\n",
    "4. Until convergence:\n",
    "   - Continue until $q(s, a)$ stabilizes or a predefined stopping condition is met.\n",
    "\n",
    "5. Optimal Policy:\n",
    "   - Extract the optimal deterministic policy by selecting actions greedily based on $q(s, a)$.\n",
    "   \n",
    "Note: in Q-learning, we generally select the behaviour policy to be the $\\epsilon$-greedy version of the greedy policy to ensure sufficient exploration. Therefore, the action-value function, $q(s, a)$, is shared between both policy, and we do not need two instances of the action-value functions.\n",
    "\n",
    "## Expected SARSA  \n",
    "Expected SARSA is another TD control algorithm that improves upon the standard SARSA method by uinsg the expected value of the next state's action-value (weighted by the policy's action probabilities) to update the current estimation. The update rule for expected SARSA is\n",
    "\n",
    "$$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_{a} \\pi(a \\mid S_{t+1}) \\, q(S_{t+1}, a) - q(S_t, A_t) \\right]$$  \n",
    "\n",
    "Unlike SARSA, which updates its estimates using a single sampled action from the next state, Expected SARSA computes a weighted average over all possible actions in the next state, weighted by the policy's action probabilities. This reduces variance in the estimation process because SARSA's reliance on a single sample can lead to high variability, as the update depends heavily on whether the sampled action is favorable or not. In contrast, Expected SARSA’s use of the expected value produces a deterministic and more stable update, often allowing for larger step sizes ($\\alpha$) without destabilizing learning.\n",
    "\n",
    "The reason is that SARSA’s updates are influenced by the randomness of a single action choice, which can make large-step-size update overly aggressive and prone to divergence. Expected SARSA, on the other hand, bases its update on the expected return under the policy, smoothing out randomness across all possible actions. As learning progresses and the action-value estimates converge, the computed expected value approaches the current estimate, where $\\sum_{a} \\pi(a \\mid S_{t+1}) \\, q(S_{t+1}, a) \\approx q(S_t, A_t)$, meaning the update term goes to zero and learning stabilizes after enought training.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/224446259/figure/fig3/AS:667853514625027@1536240094240/Average-return-on-the-cliff-walking-task-over-the-first-n-episodes-for-n-100-and-n.png\" width=500>\n",
    "\n",
    "However, this reduction in variance comes at a computational cost. Since expected SARSA must evaluate and weight all possible actions at each timestep, it becomes computationally expensive, particularly in environments with large action spaces. Despite this trade-off, Expected SARSA is often preferred in scenarios where variance reduction is critical, and computational resources are sufficient to handle the additional overhead.\n",
    "\n",
    "### On-Policy and Off-Policy Flexibility  \n",
    "Expected SARSA can function as both an on-policy and off-policy algorithm, depending on how actions are selected during learning. The key to this flexibility lies in its update rule, which computes the expected action-value over all possible next actions, weighted by the policy $\\pi(a \\mid S_{t+1})$. This update computation is independent of the actual action taken, meaning expected SARSA can function as both an on-policy and off-policy algorithm\n",
    "\n",
    "* On-policy case: If the policy used to select actions (e.g., $\\epsilon$-greedy) is the same as the policy being evaluated ($\\pi$), Expected SARSA operates on-policy. The expectation aligns with the agent's current behavior policy.  \n",
    "* Off-policy case: If actions are selected using a different policy (e.g., a purely exploratory policy) while the expectation is computed under a target policy ($\\pi$), the algorithm becomes off-policy.  \n",
    "\n",
    "Note: Q-learning is a special case of Expected SARSA where the target policy is deterministic, always selecting the action with the highest $q$-value (i.e., $\\pi(a \\mid S_{t+1}) = 1$ for $a = \\arg\\max_a q(S_{t+1}, a)$). This makes Q-learning inherently off-policy, as it learns the optimal policy while potentially following an exploratory policy, making it an special case of Expected SARSA\n",
    "\n",
    "## Summary\n",
    "| Algorithm        | On/Off Policy | Update Target   | Key Idea                                                                                          | Pros                                                                                              | Cons                                                                                              |\n",
    "|------------------|--------------|-------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **SARSA**        | On-policy    | $$R_{t+1} + \\gamma \\, q(S_{t+1}, A_{t+1})$$ | Updates using the *actual* action taken by the current policy.                                    | Simple; naturally incorporates exploration in updates.                                           | Higher variance (depends on a single sample); may require smaller step sizes for stability.      |\n",
    "| **Q-learning**   | Off-policy   | $$R_{t+1} + \\gamma \\, \\max_{a} q(S_{t+1}, a)$$ | Updates towards the *best possible* action value, regardless of the action actually taken.        | Directly learns the optimal policy; converges faster in some settings.                           | Can overestimate action values; requires good exploration policy to cover state-action space.    |\n",
    "| **Expected SARSA** | Both         | $$R_{t+1} + \\gamma \\, \\sum_a \\pi(a \\mid S_{t+1}) \\, q(S_{t+1}, a)$$ | Updates using the *expected* value over all possible actions under the policy.                    | Lower variance; more stable updates; can use larger step sizes without harming convergence.      | Higher computation cost (must compute weighted sum over all actions at each step).               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743061c3",
   "metadata": {},
   "source": [
    "# Models In Reinforcement Learning\n",
    "In reinforcement learning, a model stores knowledge about the environment dynamics, which tells the agent how the environment responds to actions by producing rewards and transitioning to new states. In other words, the model tells the agent how the world works based on its current understanding. By using this knowledge, the model can be used to simulate the environment and produce simulated experience for the agent to learn from. There are two types of models\n",
    "1. Sample models: return a single possible outcome by sampling from the underlying probability distribution of state transitions and rewards.\n",
    "    - Pros: Fast, memory-efficient.\n",
    "    - Cons: Less accurate, as each sample is only an approximation of the full distribution.\n",
    "\n",
    "2. Distribution models: specify the complete probability distribution over all possible outcomes.\n",
    "    - Pros: More accurate and informative.\n",
    "    - Cons: Larger, more computationally expensive.\n",
    "    - Note: A distribution model can also be used to generate a sample model by sampling from it.\n",
    "\n",
    "In reinforcement learning, there are two types of learning methods, model-based and model-free methods. Model-based methods. Model-based methods rely heavily on planning—using the model to simulate possible futures, evaluate them, and improve the policy before acting in the real environment, like DP, which looks ahead for the future rewards before selecting an action. Model-free methods, on the other hand, do not learn or estimate the model and learn the value function directly from experience through trial and error, like MC or TD methods.\n",
    "\n",
    "Note: For learning to happen, the agent must interact with an environment, which itself follows some transition and reward dynamics (this is the “true model” of the world). The key difference between model-based and model-free methods lies in whether the learning algorithm explicitly learns or uses its own approximation of this model. Both approaches aim to improve estimates of value functions (and ultimately the policy), but model-based methods do so by first estimating the transition and reward functions and then planning with them, whereas model-free methods skip this step and learn value functions directly from experience without building an explicit model.\n",
    "\n",
    "In any reinforcement learning setup, there is always one model that truly exists, which is the environment’s real rules. This is the true model of the world, whether it’s physics in the real world or the programmed logic in a simulator. The difference between model-based and model-free methods is about whether the agent also builds a second model of its own:\n",
    "\n",
    "Model-based methods have two models in play\n",
    "* The true model: the real environment’s transition and reward dynamics that the agent interact with.\n",
    "* The agent’s learned model: its internal guess at how the environment works, which is used for planning and action selection\n",
    "\n",
    "The agent first tries to learn this internal model from experience. Once it has it, it can run mental simulations inside its head, planning several steps ahead without touching the real environment.\n",
    "\n",
    "Model-free methods have only one model\n",
    "* The true environment model that the agent interact with\n",
    "\n",
    "The agent doesn’t bother creating its own copy. Instead, it directly learns value functions and policies from trial-and-error experience, without explicitly modeling how states change or what rewards to expect.\n",
    "\n",
    "# Planning with Model Experience\n",
    "In model-based RL, planning takes the model as input and outputs an improved policy for interacting with the environment. The process can also refine the model itself as more real-world data is gathered, creating a continuous loop of improvement between model learning and policy optimization. The key of planning is to obtain a better estimation of the value functions\n",
    "\n",
    "## Random Tabular Q-Planning\n",
    "Random tabular Q-planning is a model-based reinforcement learning algorithm. Instead of interacting with the real environment, it uses a learned or known model of the environment to simulate experiences and update the action-value function. At each step, the algorithm picks a random state–action pair (not necessarily from a real trajectory), queries the model for the resulting reward and next state, and then performs a Q-value update.  \n",
    "By simulating many such updates from the model, the policy can improve without needing to gather new real-world experience, follows the algorithm below\n",
    "\n",
    "1. Initialize:\n",
    "   - For all $ s \\in \\mathcal{S} $ and $ a \\in \\mathcal{A} $, set $ q(s, a) $ arbitrarily (e.g., 0).\n",
    "\n",
    "2. Loop (Planning):\n",
    "   - Randomly select a state $ S_t $ and an action $ A_t $ from the state–action space.\n",
    "   - Use the model to get the predicted reward $ R_{t+1} $ and next state $ S_{t+1} $. (If using a distribution model, you may sample or use expected values.)\n",
    "   - Update:\n",
    "     $$\n",
    "     q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a q(S_{t+1}, a) - q(S_t, A_t) \\right]\n",
    "     $$\n",
    "   - Optionally, update the policy to be greedy or $\\epsilon$-greedy with respect to $ q $.\n",
    "\n",
    "\n",
    "Note: All updates in this algorihtm use simulated experience from the model, not real-world interaction.\n",
    "\n",
    "## Dyna-Q\n",
    "Dyna-Q is an architecture that allows the agent to perform online planning by integrating learning, planning, and acting into a single unified process, allowing the agent to interact with the real environment while simultaneously planning for future steps. In online planning, each real experience is used in two ways:\n",
    "1. Direct Reinforcement Learning (Direct RL): The agent immediately updates its value function and improve the policy using the real experience.\n",
    "2. Model Learning: The agent uses the real experience to improve its internal model of the environment’s dynamics, making it more accurately match the real world for planning purposes.\n",
    "\n",
    "By maintaining a model through model learning, the agent can generate simulated experiences to improve the policy without further real-world interaction. This makes fuller use of limited data, enabling better performance with fewer environmental interactions. On the other hand, direct RL is simpler and free from biases that may arise from an imperfect model since it updates only from actual outcomes.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:554/1*j5chypwunyX00ejvuu10Gw.png\" width=300>\n",
    "\n",
    "Dyna-Q runs the following processes continuously and in parallel:\n",
    "1. Planning through simulated experiences generated by the model\n",
    "2. Acting by interacting with the environment  \n",
    "3. Model learning by updating the internal model from real experiences\n",
    "4. Direct RL through updating the value function from real experiences\n",
    "\n",
    "For direct RL, Dyna-Q uses one-step tabular Q-learning like before, where\n",
    "$$q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a q(S_{t+1}, a) - q(S_t, A_t) \\right]$$\n",
    "\n",
    "For planning, Dyna-Q uses random-sample one-step tabular Q-planning, which consists of:\n",
    "1. Search Control: Randomly select a state–action pair from those the agent has previously experienced. If the agent has never visited a state–action pair, it cannot plan from it because no outcome information is available.\n",
    "2. Model Query: Feed the selected state–action pair into the model, which returns a simulated reward and next state. In the basic Dyna-Q formulation, the model is assumed deterministic, meaning it returns the same result for a given state–action pair as last observed.\n",
    "3. Value Update: Apply the same Q-learning update rule as in direct RL, but using the simulated transition as if it were real.\n",
    "\n",
    "Note: Each Q-planning step simulates and updates only one state–action pair (a single one-step transition) at a time. However, Q-planning is performed many times in succession after every real Q-learning update, meaning the agent will plan for multiple times before it takes every action in real life.\n",
    "\n",
    "Both Q-learning (direct RL) and Q-planning share and update the same action-value function. The difference lies in the source of the experience, where direct RL updates are based on real interactions with the environment and Q-planning updates are based on simulated interactions using the agent’s learned model. In this sense, Q-planning is essentially Q-learning in simulation, meaning Dyna-Q does not reduce the computational effort per update (it actually adds a bit more computation) since planning requires querying and simulating with the model.\n",
    "\n",
    "However, Dyna-Q greatly improves sample efficiency. By reusing real experiences through the model, the agent can perform many extra updates in simulation without needing to gather new real-world data. This allows it to learn a good policy with fewer real episodes, which is especially valuable when real interactions are costly or limited.\n",
    "\n",
    "<img src=\"https://www.mdpi.com/drones/drones-06-00365/article_deploy/html/images/drones-06-00365-g006-550.jpg\" width=500>\n",
    "\n",
    "One limitation of Dyna-Q is its inefficiency in the early stages of learning. When the agent has very limited or no data, most planning steps are not meaningful because the model has little information to simulate with. As a result, many of the early updates contribute little to improving the policy. However, once the agent begins to collect real experiences, planning can replay these experiences in simulation, compounding their value and allowing the agent to improve with far fewer real interactions. This makes Dyna-Q particularly powerful in later phases of training.\n",
    "\n",
    "Another limitation lies in the search control step. In Dyna-Q, state–action pairs are sampled at random for planning, which can be inefficient, especially when the state–action space is very large. Many updates may focus on unimportant or rarely visited states, leading to wasted computation. In such cases, a large number of planning steps is required before the updates significantly impact performance.\n",
    "\n",
    "### Dyna-Q Algorithm\n",
    "\n",
    "1. Initialize:\n",
    "   - For all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, set $q(s,a)$ arbitrarily (e.g., $0$).\n",
    "   - Initialize an empty model $\\mathcal{M}$ (e.g., a table mapping $(s,a)\\!\\to\\!(r,s')$).\n",
    "   - Set hyperparameters: learning rate $\\alpha$, discount factor $\\gamma$, exploration rate $\\epsilon$, and planning steps $n$.\n",
    "\n",
    "2. Loop (for each episode or until stopping):\n",
    "   - Initialize start state $S$.\n",
    "\n",
    "       (A) Acting + Direct RL\n",
    "       - Choose action $A$ using an exploration policy (e.g., $\\epsilon$-greedy).\n",
    "       - Take $A$ in the real environment; observe $R$ and next state $S'$.\n",
    "       - Q-learning update (direct RL):\n",
    "         $$q(S,A) \\leftarrow q(S,A) \\;+\\; \\alpha \\left[ R \\;+\\; \\gamma \\max_{a} q(S',a) \\;-\\; q(S,A) \\right]$$\n",
    "       - Model learning (record the transition):\n",
    "         - Store/update $\\mathcal{M}(S,A) \\leftarrow (R,S')$.\n",
    "       - Set $S \\leftarrow S'$.\n",
    "\n",
    "       (B) Planning (repeat $n$ times):\n",
    "       - Search control: Randomly select a previously observed state–action pair $(\\tilde S,\\tilde A)$ from the keys of $\\mathcal{M}$.\n",
    "       - Model query: retrieve $(\\tilde R,\\tilde S') \\leftarrow \\mathcal{M}(\\tilde S,\\tilde A)$\n",
    "         (for stochastic models, either sample or use expected values).\n",
    "       - Q-planning update (simulated): \n",
    "         $$q(\\tilde S,\\tilde A) \\leftarrow q(\\tilde S,\\tilde A) \\;+\\; \\alpha \\left[ \\tilde R \\;+\\; \\gamma \\max_{a} q(\\tilde S',a) \\;-\\; q(\\tilde S,\\tilde A) \\right]$$\n",
    "\n",
    "3. Repeat Step 2 until convergence or a stopping criterion is met (e.g., max episodes, small change in $q$).\n",
    "\n",
    "<img src=\"https://miro.medium.com/1*a1ReZc7DyscMyo8nQpWuwg.png\" width=400>\n",
    "\n",
    "## Wrong Models\n",
    "Planning in model-based RL depends entirely on the agent’s internal model of the environment. However, this model can often be wrong, meaning the stored transitions differ from what actually happens in the real environment. There are two main ways a model can be wrong:\n",
    "\n",
    "1. Incomplete Model: The agent has not yet experienced certain state–action pairs, so their outcomes are simply missing from the model. This typically happens in the early stages of learning, when exploration is still limited. \n",
    "\n",
    "2. Inaccurate Model: The transitions stored in the model no longer match the environment’s true dynamics. This occurs most often when the environment is non-stationary (changing over time). In this case, planning may actually harm learning: the agent updates its value function based on outdated or incorrect information, pushing the policy in the wrong direction.  \n",
    "\n",
    "### Addressing Wrong Models\n",
    "1. Dealing with Incomplete Models: This can be easily solved by using an exploratory policy. With enough experience, the agent will eventually visit all state–action pairs, filling in the missing parts of the model and making it complete.\n",
    "\n",
    "2. Dealing with Inaccurate Models: This requires continuous exploration to ensure the model stays up to date. If the environment changes, the only way to correct the model is to revisit those states and actions. A common strategy is **Dyna-Q+**, an algorithm that adds an exploration bonus to the reward to encourage the agent to revisit states that have not been seen for a long time in real life, where\n",
    "\n",
    "$$R = r + \\kappa \\sqrt{\\tau}$$\n",
    "\n",
    "* $r$: the original reward \n",
    "* $\\kappa$: a small positive constant (bonus weight)  \n",
    "* $\\tau$: the number of time steps since the state was last visited\n",
    "* $\\sqrt{\\tau}$: a bonus, artificial reward to encourage exploration\n",
    "\n",
    "The artificial reward (exploration bonus) is used only within the agent’s internal model, not in the real environment. It grows over time for states that have been neglected to encourage the agent to revisit less-frequented states. Initially, this would update the action-value function in the wrong direction during planning since the bonus reward doesn’t actually exist in the real environment. But this distortion is intentional because during planning, the action-value function is biased upward for underexplored states. This makes those states look more attractive, encouraging the agent to select them in real interactions. Then, once the agent visits the state in real experience, it receives the true reward from the environment. The action-value function is updated again, correcting any bias introduced by the artificial bonus. In the long run, this process ensures that the value function converges to reflect the true environment dynamics, while the exploration bonus serves only as a temporary motivator. The result is a policy that balances exploration and exploitation, maintaining accuracy while improving sample efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e97152",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Function Approximation\n",
    "So far, all the methods we have discussed rely on tabular representations, where each state or state–action pair has its own entry in a table storing its value. This works for small problems but becomes completely impractical when the state or action space grows large or infinite. In such cases, storing a value for every possible state or state–action pair would require infinite memory and computation, making tabular methods infeasible. To address this, we shift to function approximation. Instead of maintaining an explicit table, we use a parameterized function to approximate the value function. This allows us to represent an enormous set of states using a finite set of parameters. The goal is to find an approximation that is “good enough” given limited data, memory, and compute resources.\n",
    "\n",
    "Function approximation reframes value estimation as a supervised learning problem: given a state or state–action pair, the function predicts its value, and the agent updates this function based on observed returns. This introduces the concept of generalization: leveraging patterns learned from past experiences to predict values for unseen states. Generalization significantly improves sample efficiency and scalability because the agent doesn’t need to experience every state individually. Instead, it can make informed decisions in unfamiliar states based on what it has learned from similar ones, enabling RL to work in complex, real-world environments where tabular methods are infeasible.\n",
    "\n",
    "In contrast to generalization, which enables an agent to apply knowledge from similar states to new situations, discrimination is about deciding when two situations are not similar enough to share the same knowledge. Discrimination ensures that the agent treats states appropriately when they differ in ways that matter for decision-making. For example, tabular methods are an extreme case of perfect discrimination, where each state is treated as entirely unique and independent.\n",
    "\n",
    "However, both extremes can lead to problems:\n",
    "* Over-generalization occurs when the agent assumes that states are more similar than they actually are, leading to poor approximations and oversimplified decision-making.\n",
    "* Over-discrimination happens when the agent treats even very similar states as completely different, which dramatically increases memory and computation requirements and slows learning.\n",
    "\n",
    "In function approximation, the key is to find a balance between generalization and discrimination. The agent must know when to leverage previously learned information and when to treat new situations independently, ensuring efficient learning without sacrificing accuracy.\n",
    "\n",
    "## On-Policy Prediction with Approximation\n",
    "On-policy prediction with approximation means we learn a parameterized function, $\\hat{v}(s; w)$, that estimates the true state-value function $v_{\\pi}(s)$, such that $\\hat{v}(s; w) \\approx v_{\\pi}(s)$. In the parametrized function, $s$ is the current state and $w$ is the weight vector of the function. The parameterized function, $\\hat v$, can be a linear function, a non-linear function, or even a neural network.\n",
    "\n",
    "Unlike tabular methods, where each state has its own entry, function approximation uses a finite set of weights to represent a potentially infinite number of states. Typically, the number of weights is much smaller than the total number of states. This has two major implications:\n",
    "\n",
    "* Generalization: Updating one weight affects many states at once because the same parameters influence multiple predictions.\n",
    "* Scalability: The method can handle large or continuous state spaces, where storing values for every state would be infeasible.\n",
    "\n",
    "This approach allows reinforcement learning to move beyond small, discrete environments and operate effectively in complex real-world settings.\n",
    "\n",
    "### Function Approximation as Supervised Learning\n",
    "Function approximation in reinforcement learning can be viewed as a supervised learning problem where the input is a state $s$ and the output is the predicted value of that state, $v_{\\pi}(s)$. The goal is to make these predictions as close as possible to the true returns under policy $\\pi$. However, applying supervised learning methods directly in RL is not straightforward because RL presents unique challenges.\n",
    "\n",
    "Unlike traditional supervised learning, which relies on a fixed dataset, reinforcement learning is an ongoing process that requires online learning, the ability to adapt continually as new experiences arrive. Furthermore, methods like TD learning and DP introduce additional complexity through bootstrapping, where the algorithm uses its current value estimates to predict future returns. This creates non-stationary targets because as the approximator updates, the targets themselves keep changing. Such instability can lead to divergence if the function approximation method is not designed to handle these conditions. These challenges highlight why not all function approximators are suitable for RL. Techniques must allow incremental updates, remain stable under bootstrapping, and cope with non-stationary data.\n",
    "\n",
    "### Prediction Objective: Mean Squared Value Error\n",
    "The prediction objective, denoted as $\\bar{VE}$, measures how close our approximated value function $\\hat{v}(s; w)$ is to the true value function $v_{\\pi}(s)$ by computing the mean squared value error. In function approximation, adjusting the weights to improve accuracy for one state often affects predictions for other states due to shared parameters. Therefore, the goal is to minimize this objective so that the approximation performs well across all states.\n",
    "\n",
    "However, not all states are equally important. Some states occur more frequently or have higher significance in decision-making. To account for this, we introduce a state distribution $\\mu(s)$, which represents the relative importance (or probability) of each state. The weighted prediction objective is then defined as\n",
    "\n",
    "$$\\bar{VE} = \\sum_{s \\in \\mathcal{S}} \\mu(s) \\big( v_{\\pi}(s) - \\hat{v}(s; w) \\big)^2$$\n",
    "\n",
    "* $\\mu(s)$: A probability distribution over all possible states that indicates the importance or visitation frequency of each state $s$.\n",
    "* $v_{\\pi}(s)$: True value of state $s$ under policy $\\pi$.\n",
    "* $\\hat{v}(s; w)$: Approximate value produced by the parameterized function.\n",
    "\n",
    "Minimizing $\\bar{VE}$ ensures that the approximation prioritizes accuracy for frequently encountered or critical states, resulting in better performance under realistic conditions.\n",
    "\n",
    "### Gradient Descent for Value Function Approximation\n",
    "\n",
    "To minimize the objective, we apply gradient descent, updating the weight vector $w$ in the direction of the negative gradient. Using a step size $\\alpha$, the update rule is\n",
    "\n",
    "$$w \\leftarrow w - \\alpha \\frac{\\partial \\bar{VE}}{\\partial w}$$\n",
    "\n",
    "For a single state $s$, the gradient of the squared error is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} \\big( v_{\\pi}(s) - \\hat{v}(s; w) \\big)^2\n",
    "= -2 \\big( v_{\\pi}(s) - \\hat{v}(s; w) \\big) \\frac{\\partial \\hat{v}(s; w)}{\\partial w}$$\n",
    "\n",
    "So the weight update becomes:\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big( v_{\\pi}(s) - \\hat{v}(s; w) \\big) \\nabla_w \\hat{v}(s; w)$$\n",
    "\n",
    "where $\\nabla_w \\hat{v}(s; w)$ is the gradient of the approximation function with respect to its weights. This term can be calcualted as long as the approximation function is differentiable almost everywhere, even if it is a complex neural network.\n",
    "\n",
    "\n",
    "Notice that the gradient of the overall objective is:\n",
    "\n",
    "$$\n",
    "\\nabla_w \\bar{VE} = \\sum_{s \\in \\mathcal{S}} \\mu(s) \\big( v_\\pi(s) - \\hat{v}(s; w) \\big) \\nabla_w \\hat{v}(s; w),\n",
    "$$\n",
    "\n",
    "which requires summing over all states in the state space. This is infeasible for large or continuous spaces because computing this exact gradient would require enumerating every possible state.\n",
    "\n",
    "To address this, we use stochastic gradient descent by approximating the full gradient using a single sampled state, where\n",
    "\n",
    "$$\n",
    "\\bar{VE}(w) \\;\\approx\\; \\big(v_\\pi(S_t) - \\hat{v}(S_t;w)\\big)^2.\n",
    "$$\n",
    "\n",
    "where $S_t$ is a state sampled from the distribution $\\mu(s)$. In practice, however, the true value function $v_{\\pi}(s)$ is unknown. To address this, we replace it with an empirical target based on experience based on this sample, where\n",
    "\n",
    "$$\\text{Target: } G_t = r_{t+1} + \\gamma r_{t+2} + \\dots$$\n",
    "\n",
    "Since this update only uses one example from a sampled state, $G_t$, which is the sampled observed return of that state, is an unbiased but noisy estimate of that $v_{\\pi}$. The SGD update then becomes\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big( G_t - \\hat{v}(S_t; w) \\big) \\nabla_w \\hat{v}(S_t; w)$$\n",
    "\n",
    "This update is computed for each observed state in real time, allowing the agent to learn online as new data arrives.\n",
    "\n",
    "### Gradient Monte Carlo Algorithm\n",
    "1. Input:\n",
    "    - A policy $\\pi$ for evaluation\n",
    "    - A parametrized function $\\hat v(s; w)$ that is used for estimation\n",
    "    - Step size $\\alpha$\n",
    "2. Initialize all the weights $w$ arbitrarily\n",
    "3. Loop\n",
    "    - Generate an episode by following the policy $\\pi$ and obtains a series {State, Action, Reward} \n",
    "    - Compute the return $G_t$ for each time step\n",
    "    - For each time step, make an gradient update\n",
    "        $$w \\leftarrow w + \\alpha \\big( G_t - \\hat{v}(S_t; w) \\big) \\nabla_w \\hat{v}(S_t; w)$$\n",
    "        \n",
    "Notes: This algorithm uses Monte Carlo targets $G_t$, so it waits until the end of the episode to perform updates, which updates once for each time step in the episode after the episode finishes. This is not an online algorithm since the agent must wait for the episode to finish to learn.\n",
    "\n",
    "### Semi-Gradient TD\n",
    "\n",
    "The Gradient Monte Carlo method is unbiased but suffers from two main issues:  \n",
    "1. It is slow to converge because it waits for full returns.  \n",
    "2. It is not online, as it must wait until the episode ends to compute $G_t$.\n",
    "\n",
    "We can address these issues by applying one-step bootstrapping with the TD algorithm. Instead of using the sampled return $G_t$ as the update target, we replace it with the TD target:\n",
    "\n",
    "$$\\text{TD target: } R_{t+1} + \\gamma \\hat{v}(S_{t+1}; w)$$\n",
    "\n",
    "Thus, the weight update becomes:\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big( R_{t+1} + \\gamma \\hat{v}(S_{t+1}; w) - \\hat{v}(S_t; w) \\big) \\nabla_w \\hat{v}(S_t; w)$$\n",
    "\n",
    "\n",
    "This update does not compute the true gradient of the objective. This is because the TD target, $R_{t+1} + \\gamma \\hat{v}(S_{t+1}; w)$, itself depends on the weights $w$ through bootstrapping $\\hat{v}(S_{t+1}; w)$, which can be changed with a change in weights. This introduces bias, because the update direction is not the true gradient. For this reason, the method is called semi-gradient TD.\n",
    "\n",
    "The semi-gradient TD algorithm does not converge as robustly as gradient methods since it's gradient update is biased, but in most use cases, the convergence is good enough. Also, the semi-gradient TD algorithm has the following advantages\n",
    "1. Online learning: Unlike Monte Carlo, this method can update after every step, not just after episodes.  \n",
    "2. Faster convergence: Bootstrapping typically accelerates learning compared to pure Monte Carlo.  \n",
    "3. Bias vs. variance: Semi-gradient TD introduces bias (due to ignoring part of the gradient), but it greatly reduces variance compared to MC, leading to faster practical convergence.\n",
    "\n",
    "\n",
    "### Semi-Gradient TD Algorithm\n",
    "1. Input:\n",
    "    - A policy $\\pi$ for evaluation\n",
    "    - A parametrized function $\\hat v(s; w)$ that is used for estimation\n",
    "    - Step size $\\alpha$\n",
    "2. Initialize all the weights $w$ arbitrarily\n",
    "3. Loop for each episode\n",
    "    - For each timestep in the episode\n",
    "        - Select action $A$ based on the policy\n",
    "        - Take the action and observe the reward, $R$, and the transition to the next state $S_{t+1}$\n",
    "        - Update the weights\n",
    "          $$w \\leftarrow w + \\alpha \\big( R_{t+1} + \\gamma \\hat{v}(S_{t+1}; w) - \\hat{v}(S_t; w) \\big) \\nabla_w \\hat{v}(S_t; w)$$\n",
    "          \n",
    "### Linear Methods\n",
    "Linear approximation is a special and simpler case of general function approximation in reinforcement learning. Instead of directly using raw states, each state s is mapped to a real-valued feature vector\n",
    "\n",
    "$$x(s) = [x_1(s), x_2(s), x_3(s), \\dots, x_n(s)]$$\n",
    "\n",
    "where the feature vector $x(s)$ has the same dimensionality as the weight vector $w$. The state-value function can then be approximated as a weighted sum (dot product) of these features\n",
    "\n",
    "$$\\hat{v}(s; w) = w^\\top x(s) = \\sum_{i=1}^n w_i , x_i(s)$$\n",
    "\n",
    "For linear functions, the gradient with respect to the weight vector is straightforward, where\n",
    "$$\\nabla_w \\hat{v}(s; w) = x(s)$$\n",
    "\n",
    "This means that the gradient is simply the feature vector itself, which makes linear function approximation computationally efficient and conceptually simple. It also highlights the importance of designing good features, as they directly determine how the weights are updated during learning.\n",
    "\n",
    "For Gradient Monte Carlo methods with linear function approximation, the learning process is guaranteed to converge to the global optimum of the value function approximation if\n",
    "1. If the features are well-designed\n",
    "2. The step size $\\alpha$ is appropriately chosen\n",
    "\n",
    "For semi-gradient TD methods using linear function approximation, convergence is still guaranteed under certain conditions, but not to the global minimum of the Mean Squared Value Error (MSVE). Instead, the algorithm converges to a point near the global optimum known as the TD fixed point.\n",
    "\n",
    "The TD fixed point represents a stable set of weights $w_{TD}$ where updates no longer change significantly. While this point may not perfectly minimize the error, it is provably close to the true minimum, with the performance bound\n",
    "\n",
    "$$\\bar{VE}(w_{TD}) \\leq \\frac{1}{1 - \\gamma} \\, \\bar{VE}(w_{MC})$$\n",
    "\n",
    "* $\\bar{VE}(w_{TD})$: the MSVE at the TD fixed point\n",
    "* $\\bar{VE}(w_{MC})$: the achievable global minimum by the linear Monte Carlo method\n",
    "\n",
    "This inequality guarantees that the asymptotic error of TD will be no more than $\\frac{1}{1 - \\gamma}$ times the minimal error. In practice, when $\\gamma$ is small (far from 1), the bound is tight, and TD performs close to optimal; when $\\gamma$ is near 1 (long-term planning problems), the bound becomes large, meaning TD could potentially be much worse than the Monte Carlo solution. Thus, while TD methods are often preferred due to their online learning capability and lower variance, there is a trade-off in asymptotic performance. In problems with very high discount factors, it is important to carefully consider whether Monte Carlo or TD is more appropriate for the desired level of convergence.\n",
    "\n",
    "### Function Approximation for Action Values\n",
    "Similarly, function approximation can be used to predict action values. The objective is to minimize the Mean Squared Value Error ($\\bar{VE}$), which averages the squared error over both states and actions, weighted by the state distribution $\\mu(s)$ and the policy $\\pi(a|s)$:\n",
    "\n",
    "$$\n",
    "\\bar{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s) \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\big( q_{\\pi}(s, a) - \\hat{q}(s, a; \\mathbf{w}) \\big)^2\n",
    "$$\n",
    "\n",
    "The weight update is derived using Stochastic Gradient Descent (SGD). We sample a state-action pair $(S_t, A_t)$ and update the weights in the direction that reduces the error for that example:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_t + \\alpha \\big[G_t - \\hat{q}(S_t, A_t; \\mathbf{w}_t) \\big] \\nabla \\hat{q}(S_t, A_t; \\mathbf{w}_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\alpha$ is the step-size (learning rate).\n",
    "* $\\nabla \\hat{q}(S_t, A_t; \\mathbf{w}_t)$ is the gradient of the value function with respect to the weights.\n",
    "* $G_t$ is the **target** value. Since the true value $q_{\\pi}$ is unknown, we substitute it with a bootstrapping estimate:\n",
    "    * **For SARSA (TD):** $G_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}; \\mathbf{w}_t)$\n",
    "    * **For Q-Learning:** $G_t = R_{t+1} + \\gamma \\max_{a} \\hat{q}(S_{t+1}, a; \\mathbf{w}_t)$\n",
    "    \n",
    "The network for predicting the action-value is called the deep Q network (DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1610e",
   "metadata": {},
   "source": [
    "## Feature Construction for Function Approximation\n",
    "We use features to represent a state. The way we represent states through features is crucial for reinforcement learning, as it directly impacts training speed, stability, and the ability to incorporate prior domain knowledge. Good feature construction can enable an RL agent to generalize well, especially when dealing with large or continuous state spaces.\n",
    "\n",
    "In linear function approximation, each feature is treated independently, meaning the model cannot naturally capture interactions between features. To address this, we often engineer or combine features to create richer representations that capture these relationships.\n",
    "\n",
    "\n",
    "### State Aggregation\n",
    "State aggregation is one of the simplest techniques for feature construction. It works by grouping similar states together and treating them as if they were the same single state. This drastically reduces the number of distinct states, which makes learning more computationally efficient and improves generalization in large or continuous state spaces.\n",
    "\n",
    "In state aggregation, the core idea is to partition the state space into non-overlapping groups or aggregated states. This means each original state is assigned to exactly one group, and no state can belong to multiple groups at the same time.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303011108051.png\" width=500>\n",
    "\n",
    "However, aggregation must be done carefully because too coarse aggregation makes the agent overgeneralizes by treating very different states as identical, which can lead to poor performance. Too fine aggregation, on the other hand, makes the agent essentially treats every state as unique, losing the benefit of generalization and requiring much more data. The key is to find a balance between generalization and discrimination.\n",
    "\n",
    "### Coarse Coding\n",
    "Coarse coding is a way to represent states using overlapping features, which allows for more flexible and powerful generalization compared to non-overlapping methods like state aggregation. In coarse coding, each feature represents a region of the state space, and multiple features can cover the same state. This means a single state may activate several features at once. When an update occurs, all active features are adjusted, which in turn affects all states that share those features. As a result, learning about one state immediately improves the value estimates of nearby states. This overlapping structure provides smoother generalization, where similar states are treated similarly.\n",
    "\n",
    "The shape and size of these overlapping regions determine how the agent generalizes across states and how well it can discriminate between them:\n",
    "* Small, distinct regions: Provide fine-grained discrimination, allowing the agent to distinguish small differences between states. Suitable for tasks requiring precise control or where small differences are critical.\n",
    "* Large, broad regions: Lead to stronger generalization, as many states share the same features. Useful when the environment is noisy or when learning must be fast with limited data.\n",
    "\n",
    "By tuning the size and shape of these overlapping features, we can control the trade-off between generalization and discrimination\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303011256015.png\" width=500>\n",
    "\n",
    "### Tile Coding\n",
    "Tile coding is a type of coarse coding technique that creates overlapping state representations to improve generalization. It works by applying state aggregation (tiling) multiple times, with each tiling slightly offset from the others. In each tiling, the state space is divided into non-overlapping tiles (like a grid). Therefore, a given state activates exactly one tile in each tiling, meaning multiple features are active for the same state across different tilings. When the agent updates its value estimate, all active tiles are updated, which simultaneously updates the estimates of nearby states that share those tiles. This structure allows the agent to generalize efficiently as updaing to one state propagate to neighboring states through overlapping tiles.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1226/1*itlhmnwLnEtUaalSsivUVA.png\" width=500>\n",
    "\n",
    "The size and direction of the offset between tilings directly affect the degree and direction of generalization. Small offsets provide finer discrimination between nearby states and allow the agent to distinguish subtle differences. While larger offsets promote stronger generalization by grouping states more broadly, which is useful in noisy or high-dimensional environments. Also, the directions of offsets between each tiling directly impact the generalization directions.\n",
    "\n",
    "Tile coding is especially practical for modern reinforcement learning systems because:\n",
    "1. It is computationally efficient as only a fixed, small number of features are active per state.\n",
    "2. It naturally supports incremental updates, making it ideal for online learning.\n",
    "3. Its simplicity makes it easier to debug and visualize compared to complex non-linear methods like neural networks.\n",
    "\n",
    "### Neural Networks for Feature Representation\n",
    "Another way to represent features in reinforcement learning is by using a neural network. In this approach, we can directly feed raw state data into the network rather than relying on hand-crafted features. The network automatically learns feature representations implicitly through the process of backpropagation, discovering patterns and relationships within the data as it trains.\n",
    "\n",
    "The learning process is framed as a supervised learning problem, where:\n",
    "* Input: The raw state or its basic attributes.\n",
    "* Output: The predicted value for that state, such as $\\hat{v}(s; w)$.\n",
    "* Objective: Minimize the value error, so the network’s predictions get closer to the true expected return.\n",
    "\n",
    "\n",
    "Compared to coarse coding or other manual feature engineering methods, the neural networks have the advantages of\n",
    "1. Capture complex, non-linear relationships that simpler methods cannot represent.\n",
    "2. Eliminate the need for manual feature construction, as the network itself discovers useful features during training.\n",
    "3. Suitable for unstructured and high-dimensional inputs, such as images or raw sensor data.\n",
    "\n",
    "But neural network also has the disadvantages of\n",
    "1. Prone to unstable learning, especially in RL where training data is non-stationary and bootstrapped targets are used.\n",
    "2. The learned features are often uninterpretable, making it difficult to understand why certain decisions are made.\n",
    "3. Require careful tuning of hyperparameters and large amounts of data for effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47581c4",
   "metadata": {},
   "source": [
    "## On-Policy Control with Approximation\n",
    "On-policy prediction methods, such as gradient Monte Carlo and semi-gradient TD, allow us to estimate the state-value function $v_\\pi(s)$ for a fixed policy $\\pi$. These methods are useful for evaluating how good a given policy is, but by themselves, they do not improve the policy because knowing the state-value only tells the agent which state is good, but doesn't tell it how to get there.\n",
    "\n",
    "To enable control, the agent must not only evaluate a policy but also improve it iteratively. This requires estimating action-values $q_\\pi(s, a)$ instead of just state values. The action-value function represents the expected return for taking a specific action a in a given state s and then following the policy thereafter.\n",
    "\n",
    "By approximating $q_\\pi(s, a)$ using function approximation techniques, the agent can generalize across large or continuous state-action spaces. This makes it possible to handle environments where storing exact values for every state-action pair would be infeasible.\n",
    "\n",
    "### Feature Representation for State-Action Pairs\n",
    "To approximate the action-value function using function approximation, we extend the idea of state-value approximation. Just like with state values, we estimate the action value as the dot product between the weights and a feature vector\n",
    "\n",
    "$$\\hat{q}(s, a; w) = w^\\top x(s, a)$$\n",
    "\n",
    "Here, $x(s, a)$ is the feature representation of the state-action pair.\n",
    "\n",
    "Given a feature vector $x(s)$ that represents a state $s$ using $n$ features, we also need to inform the function approximator which action was taken. To do this, we can stack multiple copies of the state feature vector, one for each possible action. Suppose there are $m$ possible actions for each state, then the stacked feature vector, $x(s, a)$, will then have size $(n \\times m, 1)$, where\n",
    "\n",
    "$$\n",
    "x(s, a) =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ \\vdots \\\\ 0 \\\\ x(s) \\\\ 0 \\\\ \\vdots \\\\ 0\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\cdot m}\n",
    "$$\n",
    "\n",
    "Note that for a given state-action pair $(s, a)$, only the block corresponding to action $a$ will contain the actual state features $x(s)$, and all other blocks will be filled with zeros. This creates a sparse vector that clearly separates different actions, effectively giving each action its own set of weights.\n",
    "\n",
    "Stacking has the advantages of\n",
    "1. Clear, interpretable separation of actions.\n",
    "2. Effective for linear approximation and structured feature methods like tile coding.\n",
    "\n",
    "But also has the issues of\n",
    "1. Feature vector size grows linearly with the number of actions $n \\cdot m$.\n",
    "2. Inefficient for environments with very large action spaces, leading to many zeros and wasted computation.\n",
    "\n",
    "Thus, the approximator learns separate estimators for each state-action pair, just like in tabular Q-learning but with generalization across states.\n",
    "\n",
    "As an alternative for stacking, we can encode the action directly and append it to the original state feature vector $x(s)$. For example, we can use a one-hot vector or a learned action embedding to represent the action and concatenate it with the state features to form a compact representation.\n",
    "\n",
    "This works especially well with neural networks, which can handle non-linear relationships and automatically learn how state and action features interact. However, for linear function approximators, this approach is much weaker as a linear function cannot easily capture the complex patterns between state and action from a single concatenated vector. In such cases, stacking remains the more effective choice.\n",
    "\n",
    "### Episodic Semi-Gradient SARSA\n",
    "\n",
    "Episodic Semi-Gradient SARSA is a control algorithm that uses function approximation to estimate the action-value function $\\hat{q}(s, a; w)$.  \n",
    "It extends the tabular SARSA method by replacing the table of Q-values with a parameterized function whose weights $w$ are updated using gradient-based methods.\n",
    "\n",
    "The weight update rule is similar to the semi-gradient TD update, but instead of using the estimated state value $v(s)$, we use the esimated action-value $\\hat{q}(s, a; w)$, where\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big[ R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}; w) - \\hat{q}(S_t, A_t; w) \\big] \\nabla_w \\hat{q}(S_t, A_t; w)$$\n",
    "\n",
    "For a fixed policy, this method converges in the same way as TD(0).\n",
    "\n",
    "In this architecture, instead of taking both the state and action $(s, a)$ as inputs to output a single action value for that particular state-action pair, the network is designed to accept only the state $s$ as input. It then outputs a vector containing the action-values $q(s, a)$ for *every* possible action simultaneously.\n",
    "\n",
    "This structure allows us to retrieve all Q-values for a given state in a single forward pass, making it computationally efficient to calculate $\\max_{a} q(s, a)$ for Q-learning or to derive an $\\epsilon$-greedy policy.\n",
    "\n",
    "However, this approach is strictly limited to **discrete action spaces**. It cannot be used for continuous action spaces, where the number of possible actions is infinite. For continuous control tasks, we must use alternative methods, such as discretizing the action space or employing Actor-Critic algorithms.\n",
    "\n",
    "<img src=\"https://cdn.prod.website-files.com/64c83cc4e824393ef4e8c4d0/667f20fed58bea3843bc341e_1*wfKvMsVMkUhEGz1YH7kCQA.png\" width=500>\n",
    "\n",
    "Note: the policy update step is usually not included in the actual implementation because after the action-value function is updated, in the next iteration, we can simply plug in the current state to retrive all possible action values and select action with $\\epsilon$-greedy.\n",
    "\n",
    "\n",
    "### Episodic Semi-Gradient SARSA Algorithm\n",
    "\n",
    "1. Given:\n",
    "   - Initial policy $\\pi$ (e.g., $\\epsilon$-greedy)\n",
    "   - Learning rate $\\alpha \\in (0, 1]$\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "   - A parameterized action-value function $\\hat{q}(s, a; w)$\n",
    "\n",
    "2. Initialize:\n",
    "   - Weights $w$ arbitrarily for all $s \\in \\mathcal{S}, a \\in \\mathcal{A}$.\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - Initialize the starting state $S_0$\n",
    "   - Choose initial action $A_0 \\sim \\pi(\\cdot \\mid S_0)$\n",
    "\n",
    "   For each timestep $t$:\n",
    "   - Take action $A_t$, observe reward $R_{t+1}$ and next state $S_{t+1}$.\n",
    "   - Choose next action $A_{t+1} \\sim \\pi(\\cdot \\mid S_{t+1})$ based on the current policy.\n",
    "   - Update the weights:\n",
    "     $$w \\leftarrow w + \\alpha \\big[ R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}; w) - \\hat{q}(S_t, A_t; w) \\big] \\nabla_w \\hat{q}(S_t, A_t; w)$$\n",
    "   - Update policy $\\pi$ to be greedy or $\\epsilon$-greedy with respect to the current $\\hat{q}(s, a; w)$.\n",
    "\n",
    "4. Repeat until convergence:\n",
    "   - Continue running episodes until $\\hat{q}(s, a; w)$ stabilizes.\n",
    "\n",
    "### Extending to Q-Learning and Expected SARSA\n",
    "The same semi-gradient approach can be applied to other control algorithms like Q-learning and Expected SARSA by simply changing the target value used in the update rule. The structure of the weight update remains the same, but the way we estimate the next state's action value differs.\n",
    "\n",
    "#### 1. Semi-Gradient Q-Learning\n",
    "Q-learning is off-policy, meaning it updates the value of the current state-action pair using the best possible action at the next state, independent of the policy used to select actions during exploration. The update rule is\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big[ R_{t+1} + \\gamma \\max_{a'} \\hat{q}(S_{t+1}, a'; w) - \\hat{q}(S_t, A_t; w) \\big] \\nabla_w \\hat{q}(S_t, A_t; w)$$\n",
    "\n",
    "\n",
    "#### 2. Semi-Gradient Expected SARSA\n",
    "Expected SARSA is a generalization of SARSA that uses the expected action-value under the current policy at the next state instead of relying on a single sampled action. The update rule is\n",
    "\n",
    "$$w \\leftarrow w + \\alpha \\big[ R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|S_{t+1}) \\hat{q}(S_{t+1}, a'; w) - \\hat{q}(S_t, A_t; w) \\big] \\nabla_w \\hat{q}(S_t, A_t; w)$$\n",
    "\n",
    "## Experience Replay in Deep Q-Networks (DQN)\n",
    "\n",
    "In vanilla Deep Q-Learning, training a neural network directly on the stream of experiences as they occur introduces two significant problems: **sample inefficiency** and **data correlation**. When an agent learns sequentially from the environment (i.e., receives $(S_t, A_t, R_{t+1}, S_{t+1})$, updates immediately, and discards the data), two issues arise:\n",
    "\n",
    "1. **Sample Inefficiency**: an agent learns from a sample once and then discards it, which wastes valuable information. \n",
    "\n",
    "2. **High Correlation**: In a continuous episode, successive states are highly correlated. If we train the network on these sequential samples, the gradient updates will consistently push the weights in the same direction for long periods. This causes the network to **overfit** to the current local situation, and the samples are not diverse enough.\n",
    "\n",
    "Experience Replay addresses these problems by decoupling **data collection** from **training**. Instead of learning from the current moment, the agent stores the past samples and will reuse them for training in the future.\n",
    "\n",
    "### Mechanism\n",
    "To implement experience reply, the agent maintains a **Replay Buffer** $\\mathcal{D}$ (a fixed-size queue, e.g., size $10^6$) that stores the past experiences instead of discards them immediately after use.\n",
    "1.  **Store:** As the agent interacts with the environment, every transition is stored in the buffer as a tuple:\n",
    "    $$e_t = (S_t, A_t, R_{t+1}, S_{t+1}, \\text{Done})$$\n",
    "2.  **Training:** To training the Q-network, we add the current transition into the buffer and sample a **random minibatch** of $N$ past transitions uniformly from $\\mathcal{D}$.\n",
    "    $$(S, A, R, S') \\sim U(\\mathcal{D})$$\n",
    "\n",
    "By sampling from the buffer, we effectively convert the unstable RL problem into a standard Supervised Learning task.\n",
    "* **Inputs:** The batch of States $S$.\n",
    "* **Targets:** The calculated TD Targets ($y_i = R + \\gamma \\max_{a'} q(S', a'; w)$).\n",
    "* **Update:** We update the network's parameters $w$ by minimizing the MSE loss over a random mini-batch of $N$ transitions. We calculate the gradient of the loss for each sample in the batch, average them to ensure a stable direction, and perform a Stochastic Gradient Descent (SGD) step.\n",
    "\n",
    "$$\n",
    "w \\leftarrow w + \\alpha \\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\underbrace{y_i - q(S_i, A_i; w)}_{\\text{TD Error } \\delta_i} \\right) \\nabla_{w} q(S_i, A_i; w)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\alpha$ is the learning rate.\n",
    "* $N$ is the batch size.\n",
    "* $y_i$ is the target value: $y_i = R_{i+1} + \\gamma \\max_{a'} q(S'_{i+1}, a'; w)$.\n",
    "* $\\nabla_{w} q(S_i, A_i; w)$ is the gradient of the Q-value with respect to the weights.\n",
    "\n",
    "Despite using past samples, DQN with Experience Replay is technically an **Online Off-Policy** algorithm.\n",
    "* **Online:** The agent is interacting with the environment and updating its policy *during* the episode. The algorithm typically alternates between interacting with the environment and training: take one step in the world, then perform one gradient update.\n",
    "* **Off-Policy:** The data in the buffer was generated by older versions of the policy (the behavior policy $b$), while the network is trying to learn the optimal greedy policy $\\pi^*$ (the target policy). The off-policy nature of the Q-learing algorithm allows past experience to be used. If the algorithm is on-policy, older experience cannot be used since they comes from a older version of the policy.\n",
    "\n",
    "At the very beginning of training ($t=0$), the replay buffer is empty. We cannot sample a batch to train the network.\n",
    "* **Solution:** We implement a **\"Learning Starts\"** phase (e.g., first 1,000 steps).\n",
    "* **Action:** The agent acts purely randomly to populate the buffer with diverse initial data.\n",
    "* **Training:** No gradient updates occur during this phase. Training commences only once `buffer_size > batch_size`.\n",
    "\n",
    "### Summary of Benefits\n",
    "1.  **Data Efficiency:** Experience Replay allows each transition to be reused in multiple updates, allowing the agent to learn more than once from the each sample.\n",
    "2.  **Stability (Correlation Breaking):** By sampling a random minibatch, we obtained diverse samples with in batch, breaking the temporal correlation between samples. The resulting gradient is an **average** over many diverse samples, pointing in a direction that improves the policy globally rather than just locally.\n",
    "\n",
    "## Target Networks in DQN\n",
    "Another issue in vanilla DQN is the moving target. Unlike supervised learning, where we train a model to predict a fixed label. In DQN, the target depends on the network itself.\n",
    "\n",
    "The standard Q-learning loss function using a single set of weights $w$ is:\n",
    "$$L(w) = \\mathbb{E} \\left[ \\left( \\underbrace{R + \\gamma \\max_{a'} Q(S', a'; w)}_{\\text{Target}} - \\underbrace{Q(S, A; w)}_{\\text{Prediction}} \\right)^2 \\right]$$\n",
    "\n",
    "Both the **Prediction** and the **Target** depend on the network parameters $w$, so when we perform a gradient descent update to increase $Q(S, A; w)$, the value of $Q(S', a'; w)$ will also changes, causing unstable learing or divergence.\n",
    "\n",
    "To solve this issue, we introduce a second network called the **Target Network**. We instantiate two separate neural networks with identical architectures:\n",
    "1.  **Online Network (Q Network):** Parameters $w$. Used to select actions and learn.\n",
    "2.  **Target Network:** Parameters $w'$. Used *only* to calculate the TD Target.\n",
    "\n",
    "Initially, we copy the weights exactly: $w' \\leftarrow w$.\n",
    "\n",
    "During training, we calculate the loss using the **Target Network** for the TD target term, while updating the **Online Network** for the prediction term.\n",
    "\n",
    "The new loss function becomes:\n",
    "$$L(w) = \\mathbb{E} \\left[ \\left( \\underbrace{R + \\gamma \\max_{a'} Q(S', a'; w')}_{\\text{Fixed Target}} - \\underbrace{Q(S, A; w)}_{\\text{Prediction}} \\right)^2 \\right]$$\n",
    "\n",
    "* **Prediction:** Calculated using current weights $w$.\n",
    "* **Target:** Calculated using frozen weights $w'$.\n",
    "* **Gradient:** We calculate gradients only with respect to $w$. The target network is not trained.\n",
    "\n",
    "The Target Network weights $w'$ are kept fixed for a specific period (e.g., 1000 steps). This converts the training problem into a stable Supervised Learning task for a period of time. However, the target network's action values are inaccurate, so  periodically, we update the Target Network to match the Online Network for a more accurate estiamte:\n",
    "$$w' \\leftarrow w$$\n",
    "\n",
    "This creates a \"staircase\" effect for the targets: they remain stationary for a while allowing the Online Network to converge, then they jump to the new, more accurate values.\n",
    "\n",
    "Essentially, the target is still moving, but at a significantly slower rate. Instead of letting the targets move instantaneously with every gradient step, the target network allows it to move only once after many steps, which makes the training more stable.\n",
    "\n",
    "## Deep Q-Network (DQN) Algorithm\n",
    "1. Given:\n",
    "   - Learning rate $\\alpha \\in (0, 1]$\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "   - A parameterized action-value function $\\hat{q}(s, a; w)$ (Online Network)\n",
    "   - A target action-value function $\\hat{q}(s, a; w')$ (Target Network)\n",
    "   - Replay memory buffer $D$ with capacity $N$\n",
    "   - Mini-batch size $B$\n",
    "   - Target network update frequency $C$\n",
    "\n",
    "2. Initialize:\n",
    "   - Weights $w$ arbitrarily.\n",
    "   - Target weights $w' \\leftarrow w$.\n",
    "   - Replay memory $D$ to empty.\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - Initialize the starting state $S_0$.\n",
    "\n",
    "   For each timestep $t$:\n",
    "   - Choose action $A_t$ using $\\epsilon$-greedy policy derived from $\\hat{q}(S_t, \\cdot; w)$.\n",
    "   - Take action $A_t$, observe reward $R_{t+1}$ and next state $S_{t+1}$.\n",
    "   - Store transition $(S_t, A_t, R_{t+1}, S_{t+1})$ in replay memory $D$.\n",
    "   \n",
    "   - **Experience Replay & Learning Step:**\n",
    "     - Sample a random minibatch of $B$ transitions $(S_j, A_j, R_{j+1}, S_{j+1})$ from $D$.\n",
    "     - For each transition $j$ in the minibatch, calculate the target $y_j$ using the **target network**:\n",
    "       $$y_j = \\begin{cases} R_{j+1} & \\text{if } S_{j+1} \\text{ is terminal} \\\\ R_{j+1} + \\gamma \\max_{a'} \\hat{q}(S_{j+1}, a'; w') & \\text{otherwise} \\end{cases}$$\n",
    "     - Update weights $w$ by performing a gradient descent step on the loss $\\mathcal{L}(w) = \\frac{1}{B} \\sum_j (y_j - \\hat{q}(S_j, A_j; w))^2$:\n",
    "       $$w \\leftarrow w + \\alpha \\frac{1}{B} \\sum_{j=1}^{B} \\left[ y_j - \\hat{q}(S_j, A_j; w) \\right] \\nabla_w \\hat{q}(S_j, A_j; w)$$\n",
    "\n",
    "   - **Target Network Update:**\n",
    "     - Every $C$ steps, synchronize the target network: $w' \\leftarrow w$.\n",
    "\n",
    "4. Repeat until convergence:\n",
    "   - Continue running episodes until $\\hat{q}(s, a; w)$ stabilizes.\n",
    "   \n",
    "## Double DQN\n",
    "The standard DQN algorithm usually have high postive bias due to the use of the `max` operator, meaning the netowrk action value estimations are always an over estimate. This is because when obtaining the target, its value $Y_t$ is calculated as:\n",
    "$$Y_t^{\\text{DQN}} = R_{t+1} + \\gamma \\max_{a} q(S_{t+1}, a; w_t^-)$$\n",
    "\n",
    "The use of `max` operator will inherently select the action with the highest positive noise, meaning the algorithm systematically overestimates the value of the next state. Because bootstrapping is used, this positive error accumulates and propagates backward through the states, leading to over estimates.\n",
    "\n",
    "Although reducing the bias does not necessarily means to achieve a more optimal policy if all values are overestimated equally such that the relative ranking of actions remains preserved, in practice, the bias are almost always **non-uniform**, meaning the action ranking will be different.\n",
    "\n",
    "To solve this, we must decouple **Action Selection** (choosing the \"best\" action) from **Action Evaluation** (estimating its value).\n",
    "\n",
    "### Tabular Case of Double DQN\n",
    "The original theoretical solution maintaining two independent estimators, $q_A$ and $q_B$, trained on separate experience sets.\n",
    "* **Selection:** Use $q_A$ to find the best action: $a^* = \\text{argmax}_a q_A(s, a)$.\n",
    "* **Evaluation:** Use $q_B$ to estimate the value of that action: Value $= q_B(s, a^*)$.\n",
    "\n",
    "Since $q_A$ and $q_B$ are independent, $E[q_B(s, \\text{argmax} q_A)] = q(s, a^*)$. The overestimation is eliminated because the noise in $q_B$ is uncorrelated with the selection made by $q_A$.\n",
    "\n",
    "### Function Approximation Case of Double DQN\n",
    "Splitting data into two sets is data-inefficient for Deep RL. Instead, Double DQN utilizes the **Target Network** directly as the second network.\n",
    "\n",
    "We use the Online Network to **select** the action (greedy policy) and the Target Network to **evaluate** it.\n",
    "\n",
    "**The Double DQN Update Equation:**\n",
    "\n",
    "$$Y_t^{\\text{DoubleDQN}} = R_{t+1} + \\gamma \\underbrace{q(S_{t+1}, \\overbrace{\\text{argmax}_{a} q(S_{t+1}, a; w_t)}^{\\text{Selection (Online Net)}}; w_t')}_{\\text{Evaluation (Target Net)}}$$\n",
    "\n",
    "1.  **Selection:** The Online network ($w$) determines *which* action is the best.\n",
    "2.  **Evaluation:** The Target network ($w'$) calculates the Q-value of *that specific action*.\n",
    "\n",
    "By using the Target network for evaluation, the value we update toward is not necessarily the maximum value in the Target network, but rather the value of the action chosen by the Online network. This means the action value used for calculating the target is not always an overestimation, and with many samples, the action value estimation will converge closer to the true value (significantly lower bias).\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Jingjing-Jiang-13/publication/365209751/figure/fig5/AS:11431281095620697@1667964716227/Double-DQN-algorithm-structure-Han-et-al-used-Double-DQN-to-train-a-HTEV-based-EMS.ppm\" width=700>\n",
    "\n",
    "## Systematic Exploration in Function Approximation\n",
    "Systematic exploration is critical in reinforcement learning with function approximation because it ensures that the agent explores different parts of the environment instead of getting stuck in a small subset of states or actions. This broader exploration helps the agent build a more accurate understanding of the environment and improves the quality of the learned policy.\n",
    "\n",
    "### Optimistic Initialization\n",
    "One common way to encourage exploration is through optimistic initialization, where the initial value estimates for all states or state-action pairs are set higher than their true values. This makes the agent initially optimistic about every unexplored state or action, motivating it to try them out in the beginning. Over time, as the agent gathers real experience, these optimistic estimates are gradually corrected toward their true values.\n",
    "\n",
    "While optimistic initialization works well in tabular methods, it becomes more challenging with function approximation due to two main issues:\n",
    "\n",
    "1. Weight Initialization Limitation: in function approximation, we don't directly store a separate value for each state or state-action pair. Instead, we initialize the weights $w$ of the function approximator. For simple linear models, we may be able to set weights so that initial predictions are optimistic. However, for complex, non-linear models like neural networks, there is no guarantee that initializing the weights will produce optimistic predictions across all possible states. This makes it hard to ensure consistent optimistic initialization.\n",
    "\n",
    "2. Impact of Generalization: function approximation relies on generalization, so updating the estimate for one state can also affect the values of nearby or similar states. For example, when the agent visits a state and updates its value based on the observed return, neighboring states may also have their values updated indirectly. This can cause the optimistic initial values of unvisited states to disappear prematurely, even though those states have never been explored. As a result, the agent might stop exploring too soon.\n",
    "\n",
    "### $\\epsilon$-Greedy Exploration\n",
    "An alternative approach is $\\epsilon$-greedy exploration since it does not depend on optimistic initialization or how the weights are set. It only relies on the final outputs of the function approximator, making it robust to generalization issues. However, the $\\epsilon$-greedy explorations is random, not systematic.\n",
    "\n",
    "## Average Reward\n",
    "Average reward is another way of formalizing a continuing task without using discounting, which stabilizes the return and focuses on long-term steady-state performance. Instead of reducing the value of future rewards with a discount factor $\\gamma$, we evaluate a policy based on the average reward per time step that the agent receives when following that policy indefinitely. The average reward of a policy $\\pi$ is defined as\n",
    "\n",
    "$$r(\\pi) = \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\, \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "$\\mu_{\\pi}(s)$ is the stationary distribution of states under policy $\\pi$, representing how frequently each state is visited in the long run. By weighting rewards using $\\mu_{\\pi}(s)$, states that are visited more often have greater influence on the average reward. We use the instantaneous reward $R_{t+1}$ instead of the return because the long term behaviour of the policy is encoded in $\\mu(s)$. By knowing the frequency of agent ends up in each state and how good it is immediately $R_{t+1}$ (ignoring future discount), we can calculate the average rate of reward without needing to sum up a trajectory.\n",
    "\n",
    "Since the average reward of a policy, $r(\\pi)$ is a single scalar value, we can easily compares two policies by comparing their average reward, and the policy with the higher average reward is better. The optimal policy, $\\pi_*$, is then the one that maximizes the average reward\n",
    "\n",
    "$$\\pi_* = \\arg\\max_\\pi r(\\pi)$$\n",
    "\n",
    "Since there is no discounting, we redefine returns using the differential return, which measures the cumulative difference between observed rewards and the average reward\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\big( R_{t+k+1} - r(\\pi) \\big)$$\n",
    "\n",
    "This formulation focuses on relative improvements compared to the average reward rather than just raw accumulated reward.\n",
    "\n",
    "Using the differential return, we can write the Bellman equations for the average reward setting. These equations are very similar to the discounted case but without the discount factor, and using the average reward term instead, where\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s', r} p(s', r \\mid s, a) \\big[ r - r(\\pi) + v_{\\pi}(s') \\big]$$\n",
    "\n",
    "$$q_{\\pi}(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\big[ r - r(\\pi) + \\sum_{a'} \\pi(a'|s') q_{\\pi}(s', a') \\big]$$\n",
    "\n",
    "$$v_*(s) = \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\big[ r - r(\\pi_*) + v_*(s') \\big]$$\n",
    "\n",
    "$$q_*(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\big[ r - r(\\pi_*) + \\max_{a'} q_*(s', a') \\big]$$\n",
    "\n",
    "### Using Average Reward for Control\n",
    "The updates using average reward are very similar to those in the discounted case, with a key difference on how the update is computed. In standard Q-learning with discounting, the update rule is\n",
    "\n",
    "$$\n",
    "q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma \\max_{a’} q(S_{t+1}, a’) - q(S_t, A_t) \\big]\n",
    "$$\n",
    "\n",
    "Here, the discount factor $\\gamma$ down-weights future rewards, emphasizing immediate rewards more strongly. In average reward methods, there is no discount factor $\\gamma$. Instead, we subtract the current estimate of the average reward $r(\\pi)$ of the policy at each step, where\n",
    "\n",
    "$$\n",
    "q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\big[ R_{t+1} - r(\\pi) + \\max_{a’} q(S_{t+1}, a’) - q(S_t, A_t) \\big]\n",
    "$$\n",
    "\n",
    "By subtracting the average reward, the updates center the rewards around zero, which keeps the learning process stable and focuses the agent on relative improvements over the long-term steady-state performance rather than absolute return.\n",
    "\n",
    "In addition to updating the action-value function, we must also update the estimate of the average reward after each state–action–reward pair. This is because each new interaction with the environment provides new information about the overall performance of the policy. The update rule for the average reward is\n",
    "\n",
    "$$r(\\pi) \\leftarrow r(\\pi) + \\beta \\big(R_{t+1} - r(\\pi)\\big)$$\n",
    "\n",
    "$\\beta$ is a small step-size parameter. A smaller $\\beta$ ensures that the estimate of the average reward changes gradually, preventing instability or large fluctuations due to single, noisy samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfeb32",
   "metadata": {},
   "source": [
    "# Policy Based Learning\n",
    "So far, all the methods we have introduced are value-based learning methods. In these methods, the agent first learns value functions, such as the state-value function $v(s)$ or the action-value function $q(s, a)$ from experience. Then, the agent selects actions based on these value estimates, and the policy is indirectly derived from the learned value functions. In other words, the value functions act as a bridge between the agent’s current state and its decision-making policy.\n",
    "\n",
    "However, value-based methods, such as DQN, struggle with **continuous actions** space because the network typically outputs a discrete value for every possible action to easily compute $\\max_{a} q(s, a)$, and if the action space is continuous (infinite possibilities), we cannot output a vector of infinite length. Alternatively, if we feed the action as an input (i.e., $q(s, a)$), finding the greedy action becomes an optimization problem: we must solve $\\arg\\max_{a} q(s, a)$ at every single time step, which is computationally too expensive for training.\n",
    "\n",
    "To solve this, we typically use **Policy-Based** approaches:\n",
    "1.  **Policy Gradient :**\n",
    "    * We train a single **Policy Network** that takes the **State** as input and outputs a **probability distribution** over actions (e.g., the mean and variance of a Gaussian) to sample from.\n",
    "    * The network is updated directly using the gradient of the expected return, gradually shifting the distribution probabilities toward optimal policy.\n",
    "\n",
    "2.  **Actor-Critic Method:**\n",
    "    * We train **two separate networks**:\n",
    "        * **Actor (Policy):** Takes State $\\to$ Outputs Action. It learns to select the best action directly.\n",
    "        * **Critic (Value):** Takes State (and sometimes Action) $\\to$ Outputs Value. It evaluates how good the Actor's selection was.\n",
    "    * The Critic provides a low-variance feedback signal (the \"score\") to train the Actor more efficiently than raw returns.\n",
    "\n",
    "## Policy Gradient Method\n",
    "\n",
    "Unlike value-based methods (like DQN) that learn value functions and derive a policy from them (e.g., via $\\epsilon$-greedy), **Policy Gradient methods** learn the policy directly as a parameterized function $\\pi(a \\mid s, \\theta)$. Here, $\\theta$ represents the learnable parameters (weights) of the neural network.\n",
    "\n",
    "For $\\pi(a \\mid s, \\theta)$ to be a valid probability distribution, it must satisfy two constraints:\n",
    "1.  **Non-negativity:** $\\pi(a \\mid s, \\theta) \\geq 0 \\quad \\forall a \\in \\mathcal{A}, s \\in \\mathcal{S}$\n",
    "2.  **Normalization:** $\\sum_a \\pi(a \\mid s, \\theta) = 1 \\quad \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "These ensure that for any given state, the probabilities of all actions sum to 1. The way we parameterize the policy depends on whether the action space is Discrete or Continuous.\n",
    "\n",
    "**A. Discrete Action Space (Softmax Policy)**\n",
    "We define a real-valued function called the **action preference**, denoted as $h(s, a, \\theta)$. This represents the \"unnormalized logit\" or score for taking action $a$. To convert these scores into valid probabilities, we apply the **Softmax function**:\n",
    "\n",
    "$$\\pi(a \\mid s, \\theta) = \\frac{e^{h(s, a, \\theta)}}{\\sum_{b} e^{h(s, b, \\theta)}}$$\n",
    "\n",
    "**B. Continuous Action Space (Gaussian Policy)**\n",
    "When the action space is infinite (continuous), it is impossible to sum over all actions to compute the normalization term. Instead, we model the policy as a known continuous probability distribution, typically a **Gaussian (Normal) distribution**.\n",
    "\n",
    "The network is trained to output the **mean** ($\\mu$) and **variance** ($\\sigma^2$) of the distribution given the state:\n",
    "$$\\pi(a \\mid s, \\theta) = \\frac{1}{\\sigma(s, \\theta)\\sqrt{2\\pi}} \\exp\\left( -\\frac{(a - \\mu(s, \\theta))^2}{2\\sigma(s, \\theta)^2} \\right)$$\n",
    "The agent then samples an action from this distribution.\n",
    "\n",
    "### Objective Formulations\n",
    "\n",
    "To optimize the policy parameters $\\theta$, we must define a performance objective $J(\\theta)$ to maximize. There are two primary formulations, depending on the nature of the task.\n",
    "\n",
    "1.  **Episodic Formulation (Start-State Value)**\n",
    "    * **Use Case:** Episodic tasks where the agent interacts in distinct episodes that eventually terminate (although we can also cut the trajectory after some steps when dealing with inifinte horizon tasks).\n",
    "    * **Goal:** Maximize the expected total discounted return from the initial start state $S_0$. The agent cares about the cumulative sum of rewards for the specific path taken from start to finish.\n",
    "    * **Objective Function:**\n",
    "        $$J(\\theta) = V_{\\pi}(S_0) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t R_{t+1} \\mid S_0 \\right]$$\n",
    "    * **Method:** We sample full trajectories (episodes) and use the return $G_t$ to estimate the gradient.\n",
    "\n",
    "2.  **Average Reward Formulation (Steady-State Value)**\n",
    "    * **Use Case:** Continuing tasks where the agent runs indefinitely without a natural endpoint (Infinite Horizon).\n",
    "    * **Goal:** Maximize the average **rate** of reward per time step over the long run. Since the task never ends, we look at the rewards obtained in the stationary distribution (where the agent spends its time eventually).\n",
    "    * **Objective Function:**\n",
    "        $$J(\\theta) = r(\\pi) = \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a|s, \\theta) \\mathbb{E}[R \\mid s, a]$$\n",
    "        *(Where $\\mu_{\\pi}(s)$ is the stationary distribution of states under policy $\\pi$.)*\n",
    "    * **Method:** We rely on the ergodic property of the MDP, optimizing the policy to shift the stationary distribution $\\mu(s)$ toward high-reward states.\n",
    "\n",
    "### Episodic Formulation in Policy Gradient Methods\n",
    "The primary is to find an optimal policy $\\pi_\\theta$ that maximizes the expected cumulative reward. In the episodic formulation, this corresponds to **maximizing** the expected value over all possible states.\n",
    "\n",
    "We define the objective function $J(\\theta)$ as the expected value of the state-value function $v_{\\pi_\\theta}(s)$ averaged over the state distribution $p(s)$. Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int_{S} p(s) v_{\\pi_\\theta}(s) \\, ds = \\mathbb{E}_{s \\sim p(s)} [v_{\\pi_\\theta}(s)]\n",
    "$$\n",
    "\n",
    "* $p(s)$ is state distribution of the environment.\n",
    "* $v_{\\pi_\\theta}(s)$ is the value of state $s$ and following policy $\\pi_\\theta$ thereafter.\n",
    "\n",
    "A key challenge here is that the true state distribution $p(s)$ is typically unknown. Therefore, we cannot calculate this integral analytically; instead, we must rely on sampling techniques to estimate it.\n",
    "\n",
    "To proceed, we first need a formal expression for the value of a single state $v_{\\pi_\\theta}(s_0)$. The value of a state is defined as the expected return obtained by following the policy $\\pi_\\theta$ starting from that state. Let a trajectory $\\tau$ be a sequence of states, actions, and rewards: $\\tau = (s_0, a_0, s_1, r_1, a_1, s_2, r_2, \\dots, s_T, r_T)$. The probability of a specific trajectory occurring depends on both the policy (agent) and the dynamics (environment):\n",
    "\n",
    "$$\n",
    "P(\\tau|\\pi_\\theta) = p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) p(s_{t+1}, r_{t+1}|s_t, a_t)\n",
    "$$\n",
    "\n",
    "The value function $v_{\\pi_\\theta}(s_0)$ can be written as the integral of the return $G(\\tau)$ weighted by the probability of the trajectory occurring (integrate over all possbile trajectory starting from $s_0$), conditioned on starting at $s_0$:\n",
    "\n",
    "$$\n",
    "v_{\\pi_\\theta}(s_0) = \\int_{\\tau(s_0)} G(\\tau(s_0)) \\left( \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) p(s_{t+1}, r_{t+1}|s_t, a_t) \\right) \\, d\\tau\n",
    "$$\n",
    "\n",
    "We can express this compactly as an expectation:\n",
    "\n",
    "$$\n",
    "v_{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [G(\\tau) | s_0]\n",
    "$$\n",
    "\n",
    "This equation is exactly the definition of the state value, which is the **expected return of all possible trajectories starting from that state**, and it depends on:\n",
    "1.  **The Policy $\\pi_\\theta$:** Determines the probability of actions.\n",
    "2.  **The Environment $p$:** Determines transition dynamics and rewards.\n",
    "\n",
    "Now we combine the state distribution $p(s)$ with the single-state value expression to reformulate the global objective $J(\\theta)$. Substituting the definition of $v_{\\pi_\\theta}(s)$ back into our original objective:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int_{S} p(s) \\left( \\int_{\\tau(s)} G(\\tau) P(\\tau | s, \\pi_\\theta) \\, d\\tau \\right) \\, ds\n",
    "$$\n",
    "\n",
    "By combining the integral over all states $p(s)$ and the integral over trajectories starting from those states, we can unify this into a single expectation over the distribution of all full trajectories induced by the policy:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int_{\\tau} P(\\tau|\\pi_\\theta) G(\\tau) \\, d\\tau\n",
    "$$\n",
    "\n",
    "Or in expectation notation:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [G(\\tau)]\n",
    "$$\n",
    "\n",
    "In other words, the objective is just to maximize the expected return over the distribution of all possible trajectory. Therefore, instead of finding the distribution $p(s)$, we can directly approximate the expected value by Monte Carlo sampling.\n",
    "\n",
    "### Training Policy Networks\n",
    "To maximize $J(\\theta)$, we use **Gradient Ascent**. We need to compute the gradient of the objective with respect to the parameters, $\\nabla_\\theta J(\\theta)$.\n",
    "\n",
    "Starting with the integral definition:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int P(\\tau|\\theta) G(\\tau) \\, d\\tau\n",
    "$$\n",
    "\n",
    "Assuming the return $G(\\tau)$ is a scalar value determined by the trajectory (and not directly differentiable with respect to $\\theta$), we move the gradient inside the integral:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int \\nabla_\\theta P(\\tau|\\theta) G(\\tau) \\, d\\tau\n",
    "$$\n",
    "\n",
    "We apply the identity $\\nabla_\\theta P(\\tau|\\theta) = P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta)$. This allows us to re-introduce the probability term $P(\\tau|\\theta)$ required to form an expectation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) G(\\tau) \\, d\\tau = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log P(\\tau|\\theta) G(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "To compute $\\nabla_\\theta \\log P(\\tau|\\theta)$, we expand the probability of a trajectory. A trajectory's probability is the product of the start state probability, the policy probabilities, and the transition dynamics:\n",
    "\n",
    "$$\n",
    "P(\\tau|\\theta) = p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) p(s_{t+1}, r_{t+1}|s_t, a_t)\n",
    "$$\n",
    "\n",
    "Taking the log of both sides converts the product into a sum:\n",
    "$$\n",
    "\\log P(\\tau|\\theta) = \\log p(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T-1} \\log p(s_{t+1}, r_{t+1}|s_t, a_t)\n",
    "$$\n",
    "\n",
    "Now, we take the gradient with respect to $\\theta$:\n",
    "1.  $\\nabla_\\theta \\log p(s_0) = 0$ (Initial state distribution does not depend on $\\theta$).\n",
    "2.  $\\nabla_\\theta \\log p(s_{t+1}, r_{t+1}|s_t, a_t) = 0$ (Environment dynamics do not depend on $\\theta$).\n",
    "3.  The only term that remains is the policy term.\n",
    "\n",
    "Thus, the gradient of the log-probability of the trajectory simplifies to just the sum of the gradients of the policy:\n",
    "$$\n",
    "\\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "> This derivation is crucial because it allows us to optimize the policy without needing a model of the environment dynamics $p(s', r'|s,a)$, and $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ can be computed easily using backpropgation.\n",
    "\n",
    "\n",
    "Substituting the decomposed log-probability back into our expectation, we get the final expression for the Policy Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) G(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "This equation states that the gradient is the expected value of the **score function** ($\\nabla \\log \\pi$) scaled by the **return** $G(\\tau)$.\n",
    "* If $G(\\tau)$ is high, we increase the probability of the actions taken in that trajectory.\n",
    "* If $G(\\tau)$ is low, we decrease their probability.\n",
    "\n",
    "We cannot compute the expectation over all trajectories analytically. Instead, we approximate it using **Monte Carlo sampling**. If we sample a single trajectory $\\tau^{(i)}$ using the current policy $\\pi_\\theta$, we obtain an unbiased estimate of the gradient:\n",
    "$$\n",
    "\\hat{g} = \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) G(\\tau^{(i)})\n",
    "$$\n",
    "\n",
    "Using this estimate, we update the parameters $\\theta$ using Stochastic Gradient Ascent (SGD):\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G(\\tau^{(i)})\n",
    "$$\n",
    "\n",
    "This specific implementation using a full trajectory return $G(\\tau)$ to estimate the value and updating via the score function is known as the **REINFORCE** algorithm.\n",
    "\n",
    "### REINFORCE (Monte Carlo Policy Gradient) Algorithm\n",
    "1. Given:\n",
    "   - Learning rate $\\alpha \\in (0, 1]$\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "   - A parameterized differentiable policy function $\\pi(a|s; \\theta)$ (Policy Network)\n",
    "\n",
    "2. Initialize:\n",
    "   - Weights $\\theta$ arbitrarily.\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - **Generate Episode:**\n",
    "     - Initialize $S_0$.\n",
    "     - Generate a full trajectory $\\tau = S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T$ by following policy $\\pi(\\cdot|\\cdot; \\theta)$ until a terminal state $S_T$ is reached.\n",
    "\n",
    "   - **For each timestep $t = 0, 1, \\dots, T-1$:**\n",
    "     - Calculate the return $G_t$ (cumulative discounted reward from step $t$ onwards):\n",
    "       $$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$$\n",
    "\n",
    "     - **Policy Update Step:**\n",
    "       - Update weights $\\theta$ by performing a stochastic gradient ascent step. Note this step is performed for every single timestep in a trajectory.\n",
    "         $$\\theta \\leftarrow \\theta + \\alpha G_t \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)$$\n",
    "\n",
    "4. Repeat until convergence:\n",
    "   - Continue running episodes until policy $\\pi(a|s; \\theta)$ stabilizes or average return is maximized.\n",
    "   \n",
    "> In practice, we often batch the gradient of an entire trajectory to perform one update only with\n",
    " $$\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G(\\tau^{(i)})\n",
    "$$ The gradient here is the sum of gradient of all timestep of the trajectory. The two approach is exactly equivalent of each other, and this one is more efficient.\n",
    "\n",
    "### Average Reward Formulation in Policy Gradient Methods\n",
    "\n",
    "\n",
    "## The Policy Gradient Theorem\n",
    "The REINFORCE algorithm is effectively a **sample-based implementation** of the theoretical Policy Gradient goal. The general objective in RL is to maximize the expected value of states under the policy's state distribution:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\int_{\\mathcal{S}} p(s) v_{\\pi_\\theta}(s) \\, ds\n",
    "$$\n",
    "\n",
    "Computing the gradient of this objective, $\\nabla_\\theta J(\\theta)$, analytically is intractable for two reasons:\n",
    "1.  **Unknown State Distribution $p(s)$:** We do not know the true distribution of states reachable by the policy, nor how that distribution shifts ($\\nabla_\\theta p(s)$) when the policy changes.\n",
    "2.  **Unknown Value Function $v_{\\pi_\\theta}(s)$:** We do not have the true value function; we only have samples of returns.\n",
    "\n",
    "REINFORCE bypasses these intractable integrals by using **Monte Carlo sampling**. It estimates the gradient using the **actual return** $G_t$ of a sampled trajectory as an unbiased estimator of the state value and the average over $N$ trajectories as the estimation for the state distribution.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi(a_t|s_t) G_t\n",
    "$$\n",
    "\n",
    "By the Law of Large Numbers, as the number of sampled trajectories $N \\to \\infty$, this estimate converges to the true gradient derived by the Policy Gradient Theorem. In practice, we use a few samples to compute a stochastic gradient for the update.\n",
    "\n",
    "To justify why algorithms like REINFORCE work, we must derive a computable form of the gradient that eliminates the dependency on the unknown environment dynamics.\n",
    "\n",
    "We start by expanding the value of a single state $v_\\pi(s)$ using the action-value function $q_\\pi(s,a)$:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\int_{a} \\pi_\\theta(a|s) q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "We take the gradient with respect to $\\theta$. Using the product rule, this splits into two terms:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta v_\\pi(s) = \\underbrace{\\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) q_\\pi(s, a)}_{\\text{Term 1: Policy Change}} + \\underbrace{\\int_{a} \\pi_\\theta(a|s) \\nabla_\\theta q_\\pi(s, a)}_{\\text{Term 2: Q-Value Change}}\n",
    "$$\n",
    "\n",
    "* **Term 1 (Known):** Represents the change in value due to the policy selecting actions differently. This is computable (we know $\\pi_\\theta$) as long as we know $q$, and in practice, we estiamte $q$ with the actual return observed fromt the trajectory $G_t$.\n",
    "* **Term 2 (Unknown):** Represents the change in the Q-values themselves as the policy changes future interactions, which is unknown.\n",
    "\n",
    "To handle Term 2, we expand $\\nabla_\\theta q_\\pi(s, a)$ using the Bellman equation:\n",
    "$$q_\\pi(s, a) = \\int_{s', r} p(s', r | s, a) [r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "Taking the gradient:\n",
    "$$\n",
    "\\nabla_\\theta q_\\pi(s, a) = \\int_{s', r} \\underbrace{\\nabla_\\theta p(s', r | s, a)}_{0} [r + \\gamma v_\\pi(s')] + \\int_{s', r} p(s', r | s, a) \\underbrace{\\nabla_\\theta [r + \\gamma v_\\pi(s')]}_{\\gamma \\nabla_\\theta v_\\pi(s')}\n",
    "$$\n",
    "\n",
    "*Note: The gradient of the environment dynamics $p(s'|s,a)$ and reward $r$ is **zero** because the environment does not depend on $\\theta$.*\n",
    "\n",
    "Substituting this back, we get a recursive relationship for the state-value gradient, so the gradient of the current state depends on the gradient of all possible next states:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta v_\\pi(s) = \\phi(s) + \\gamma \\int_{s'} P(s'|s, \\pi) \\nabla_\\theta v_\\pi(s')\n",
    "$$\n",
    "\n",
    "Where $\\phi(s) = \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) q_\\pi(s, a)$.\n",
    "\n",
    "By repeatedly expanding the term $\\nabla_\\theta v_\\pi(s')$ (the gradient of the next state), we can express the gradient as an infinite sum over time steps:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta v_\\pi(s) = \\phi(s) + \\gamma \\int_{s'} P(s'|s) (\\phi(s') + \\gamma \\int_{s''} P(s''|s') \\nabla_\\theta v_\\pi(s''))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta v_\\pi(s) = \\int_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\gamma^k P(s \\to x, k, \\pi) \\phi(x)\n",
    "$$\n",
    "\n",
    "* $P(s \\to x, k, \\pi)$: The visitation probability. It asks, \"If I start at state $s$, what is the probability I will land in state $x$ exactly $k$ steps later?\"\n",
    "\n",
    "This equation states that the gradient of a state's value is the sum of $\\phi(x)$ for all future states $x$, weighted by how likely we are to visit them ($P$).\n",
    "\n",
    "We substitute this into the gradient of the original objective function, which is the average value over all possible start states $s_0$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{s_0} p(s_0) \\nabla_\\theta v_\\pi(s_0) \\, ds_0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{s_0} p(s_0) \\left( \\int_{x} \\sum_{k=0}^{\\infty} \\gamma^k P(s_0 \\to x, k, \\pi) \\phi(x) \\, dx \\right) ds_0\n",
    "$$\n",
    "\n",
    "We rearrange the integrals to group terms by the **visited state** $x$ rather than the start state $s_0$. This is the crucial step:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{x} \\phi(x) \\underbrace{\\left( \\int_{s_0} p(s_0) \\sum_{k=0}^{\\infty} \\gamma^k P(s_0 \\to x, k, \\pi) \\, ds_0 \\right)}_{\\text{Total Discounted Visitation Density}} \\, dx\n",
    "$$\n",
    "\n",
    "The term in the brackets represents the total discounted probability of visiting state $x$ (sometimes denoted $d^\\pi(s)$). We define the **normalized discounted state distribution** $p(s)$, which represents the overall probablity of being in a **single** state under policy $\\pi$. This term depends on both the policy and the environment dynamics.\n",
    "\n",
    "$$\n",
    "p(s) = \\frac{1}{1-\\gamma} \\int_{s_0} p(s_0) \\sum_{k=0}^{\\infty} \\gamma^k P(s_0 \\to s, k, \\pi) ds_0\n",
    "$$\n",
    "\n",
    "Substituting this definition back into our equation (absorbing the constant $1-\\gamma$ into the proportionality or learning rate):\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\propto \\int_{x} p(x) \\phi(x) \\, dx\n",
    "$$\n",
    "\n",
    "We expand $\\phi(x)$ using the log-derivative trick: $\\nabla \\pi = \\pi \\nabla \\ln \\pi$.\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\int_{a} \\nabla_\\theta \\pi_\\theta(a|x) q_\\pi(x, a) \\, da = \\mathbb{E}_{a \\sim \\pi} [\\nabla_\\theta \\ln \\pi_\\theta(a|x) q_\\pi(x, a)]\n",
    "$$\n",
    "\n",
    "### Final Policy Gradient Theorem\n",
    "Combining the state distribution $p(s)$ and the action expectation, we arrive at the final theorem:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\int_{\\mathcal{S}} p(s) \\left( \\int_{a} \\pi_\\theta(a|s) \\nabla_\\theta \\ln \\pi_\\theta(a|s) q_\\pi(s, a) \\, da \\right) ds\n",
    "$$\n",
    "\n",
    "Written compactly as an expectation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim p, a \\sim \\pi} \\left[ q_{\\pi_\\theta}(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a|s) \\right]\n",
    "$$\n",
    "\n",
    "**Significance:**\n",
    "This theorem proves that we can compute the gradient of the performance $J(\\theta)$ **without** differentiating the state distribution or the environment dynamics. We only need:\n",
    "1.  **$\\nabla \\ln \\pi$:** The gradient of the policy itself (computed via backpropagation).\n",
    "2.  **$q(s,a)$:** The estimated return (obtained via interaction/sampling from the environment).\n",
    "\n",
    "This provide us to a way to train the policy network.\n",
    "\n",
    "## Implementation of Policy Gradient Methods\n",
    "There are three primary families of policy pradient implementations, categorized by how they estimate the return and stabilize the gradient:\n",
    "1.  **Vanilla Version (REINFORCE)**\n",
    "2.  **Baseline Version (Advantage)**\n",
    "3.  **Trust Region Methods** (e.g., TRPO, PPO, etc)\n",
    "\n",
    "### 1. Vanilla Version (REINFORCE)\n",
    "The Vanilla Policy Gradient algorithm (the REINFORCE algorithm), relies on **Monte Carlo sampling**. It uses the actual observed return $G_t$ from a complete trajectory as the unbiased estimator for the action-value $q(s_t, a_t)$.\n",
    "\n",
    "#### Update Formula\n",
    "For a trajectory $\\tau$ consisting of steps $t=0 \\dots T$, the parameters $\\theta$ are updated via gradient ascent:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t\n",
    "$$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the discounted return from time $t$.\n",
    "\n",
    "While $G_t$ is an unbiased estimator of $q(s, a)$, this method suffers from severe **High Variance**.\n",
    "* **Magnitude Sensitivity:** The gradient update scales linearly with the magnitude of the rewards. If rewards are large (e.g., +1000 vs +1010), the gradient updates will be massive, leading to unstable training.\n",
    "* **All-Positive Rewards:** If all rewards are positive, the gradient tries to push the probabilities of *all* sampled actions up. The optimal action is only found because it is pushed up *slightly more* than the others. This \"tug-of-war\" slows down convergence significantly.\n",
    "Ideally, we want the target value to be centered around 0. This ensures that bad actions produce negative gradients (decreasing probability) and good actions produce positive gradients (increasing probability).\n",
    "\n",
    "### 2. Baseline Version (Advantage)\n",
    "To address the variance and magnitude issues, we introduce a **Baseline** $b(s)$. The goal is to center the return estimator by subtracting a value that depends on the state but **not** on the action in order to \"access\" the quality of the action. \n",
    "\n",
    "$$q'(s, a) = q(s, a) - b(s)$$\n",
    "\n",
    "Ideally, we want to give **better-than-average** actions a positive score and **worse-than-average** actions a negative score. Crucially, even if an action yields a positive absolute reward (e.g., +10), if it performs worse than other possible actions (e.g., average is +50), the baseline ensures it receives a negative advantage score, discouraging the agent from taking it, so the agent learns to take better actions.\n",
    "\n",
    "We can prove that subtracting a baseline $b(s)$ does not alter the true gradient of the objective. Starting with the expected gradient form where we introduce the baseline:\n",
    "$$\n",
    "\\nabla J(\\theta) = \\mathbb{E}_{s \\sim \\pi} \\left[ \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) (q_\\pi(s, a) - b(s)) \\right]\n",
    "$$\n",
    "\n",
    "We expand the term using the distributive property:\n",
    "$$\n",
    "\\nabla J(\\theta) = \\underbrace{\\mathbb{E} \\left[ \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) q_\\pi(s, a) \\right]}_{\\text{Original Policy Gradient}} - \\underbrace{\\mathbb{E} \\left[ \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) b(s) \\right]}_{\\text{Baseline Term}}\n",
    "$$\n",
    "\n",
    "Let's analyze the **Baseline Term**. Since $b(s)$ is constant with respect to the integral over actions $a$, we can move it outside the summation:\n",
    "$$\n",
    "\\text{Baseline Term} = \\mathbb{E}_{s} \\left[ b(s) \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) \\right]\n",
    "$$\n",
    "\n",
    "Using the linearity of the derivative (since the integral is with respect to $a$ and the gradient is with respect to $\\theta$), $\\int \\nabla = \\nabla \\int$:\n",
    "$$\n",
    "\\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) = \\nabla_\\theta \\left( \\int_{a} \\pi_\\theta(a|s) \\right)\n",
    "$$\n",
    "\n",
    "Since probabilities distribution must integrate to 1 ($\\int \\pi = 1$), and the gradient of a constant is 0:\n",
    "$$\n",
    "\\nabla_\\theta (1) = 0\n",
    "$$\n",
    "\n",
    "The baseline term vanishes. Therefore, we can subtract any function $b(s)$ (as long as it does not depend on $a$) without introducing bias to the gradient estimation, where\n",
    "\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}_{s \\sim \\pi} \\left[ \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) (q_\\pi(s, a) - b(s)) \\right] =  \\mathbb{E}_{s \\sim \\pi} \\left[ \\int_{a} \\nabla_\\theta \\pi_\\theta(a|s) q_\\pi(s, a)\\right] = \\mathbb{E}_{s \\sim p, a \\sim \\pi} \\left[ q_{\\pi_\\theta}(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a|s) \\right]$$\n",
    "\n",
    "#### The Advantage Function\n",
    "We select the baseline to be the **value function** of the state, $b(s) = v_\\pi(s)$. This transforms our estimator into the **advantage function**:\n",
    "\n",
    "$$\n",
    "A_\\pi(s, a) = q_\\pi(s, a) - v_\\pi(s)\n",
    "$$\n",
    "\n",
    "* **Interpretation:** The Advantage assesses how much better (or worse) the chosen action is compared to the **average** performance in that state, which is represented by $v_\\pi(s)$.\n",
    "* **Centering:** If an action is average, $A \\approx 0$. If it is effectively good, $A > 0$. If poor, $A < 0$. This stabilizes the gradient magnitude.\n",
    "\n",
    "#### Computing $v(s)$\n",
    "In practice, we do not know the true $v_\\pi(s)$. We approximate it using **Function Approximation** with a seperate neural network:\n",
    "1.  Initialize a separate neural network (Value Network) $v_\\phi(s)$ with parameters $\\phi$.\n",
    "2.  Train this network using Mean Squared Error (MSE) regression against the actual observed returns $G_t$:\n",
    "    $$L(\\phi) = \\frac{1}{N} \\sum (G_t - v_\\phi(s_t))^2$$\n",
    "\n",
    "#### Final Update Formula\n",
    "The Policy Gradient update using the Advantage estimate (where $G_t$ estimates $q$ and $v_\\phi$ estimates baseline), which provides a more stable, low variance training.\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\ln \\pi_\\theta(a_t|s_t) \\underbrace{(G_t - v_\\phi(s_t))}_{\\text{Advantage Estimate } \\hat{A}_t}\n",
    "$$\n",
    "\n",
    "In actor-critic method, since we have the value network, we can also approximated the sample return using TD instead of Monte Carlo to enable online learing, where\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\ln \\pi_\\theta(a_t|s_t) \\underbrace{(R_{t+1} + \\gamma v_\\phi(s_{t+1}) - v_\\phi(s_t))}_{\\text{Advantage Estimate } \\hat{A}_t}\n",
    "$$\n",
    "\n",
    "### Baseline Algorithm (Monte Carlo Policy Gradient)\n",
    "1. Given:\n",
    "   - Learning rates $\\alpha \\in (0, 1]$ (for Policy) and $\\beta \\in (0, 1]$ (for Value)\n",
    "   - Discount factor $\\gamma \\in [0, 1]$\n",
    "   - A parameterized differentiable **Policy function** $\\pi(a|s; \\theta)$ (Policy Network)\n",
    "   - A parameterized differentiable **State-Value function** $\\hat{v}(s; w)$ (Value Network)\n",
    "\n",
    "2. Initialize:\n",
    "   - Policy weights $\\theta$ and Value weights $w$ arbitrarily.\n",
    "\n",
    "3. Repeat (for each episode):\n",
    "   - **Generate Episode:**\n",
    "     - Initialize $S_0$.\n",
    "     - Generate a full trajectory $\\tau = S_0, A_0, R_1, S_1, A_1, R_2, \\dots, S_{T-1}, A_{T-1}, R_T$ by following policy $\\pi(\\cdot|\\cdot; \\theta)$ until a terminal state $S_T$ is reached.\n",
    "\n",
    "   - **For each timestep $t = 0, 1, \\dots, T-1$:**\n",
    "     - Calculate the return $G_t$ (cumulative discounted reward from step $t$ onwards):\n",
    "       $$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$$\n",
    "\n",
    "     - Calculate the **Advantage / Prediction Error** $\\delta_t$ (the difference between actual return and the baseline prediction):\n",
    "       $$\\delta_t = G_t - \\hat{v}(S_t; w)$$\n",
    "\n",
    "     - **Value Network Update Step:**\n",
    "       - Update weights $w$ to minimize the Mean Squared Error between prediction $\\hat{v}(S_t)$ and target $G_t$ (Stochastic Gradient Descent):\n",
    "         $$w \\leftarrow w + \\beta \\delta_t \\nabla_w \\hat{v}(S_t; w)$$\n",
    "\n",
    "     - **Policy Update Step:**\n",
    "       - Update weights $\\theta$ using the Advantage $\\delta_t$ to scale the gradient (Stochastic Gradient Ascent):\n",
    "         $$\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta \\log \\pi(A_t|S_t; \\theta)$$\n",
    "\n",
    "4. Repeat until convergence:\n",
    "   - Continue running episodes until policy stabilizes or average return is maximized.\n",
    "\n",
    "> Note: This results in moving target, but the error is managable because the value network is always trained on unbiased estimation (observed return) unlike DQN, which is trained based on its own value estimates. Therefore, the training of the policy net will eventually converge as the value network gives more accurate prediction over time.\n",
    "\n",
    "### Trusted Region\n",
    "One issue about baseline policy gradient algorithm is it is sample inefficient because for each observed trajectory, we only use it once and throw it away. We might want to apply methods similar to experience reply in DQN, but unlike, DQN, the policy gradient method is on-policy and we cannot use samples collected from the old policy directly for future training.\n",
    "\n",
    "In order to be able to use past examples, we can rewrite our objective using the surrogate fuction, where we reweight the current policy with respect to an older version of the policy\n",
    "\n",
    "Mathematically, the policy gradient objective is the same as using the surrogate functions\n",
    "step by step manipulation to show the two objective are equvalent at a particular set of theta.\n",
    "\n",
    "This is a local approximation, and if theta moves far away from the approximation point, the approximation becomes unreliable.\n",
    "\n",
    "Therefore we can use the update objective to be\n",
    "update objective using surrogate\n",
    "\n",
    "Since the objective is to find the parameters that gives the highest average return, so instead of using the step gradient update, we can turn the problem into an maximation problem of the surrogate function and run optimizers on it, so we do not need to tune the learning rate anymore\n",
    "\n",
    "#### Policy Collapse Issue\n",
    "In both baseline policy gradient and the surrogate version of it, we observe that through training, the average reward getting improved up to some point, then it could drop drastically afterward. We called this the policy collapse issue.\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNUcLArPrBI-SXo1iQYFaSmSRhLxmy6yIcFg&s\" width=300>\n",
    "\n",
    "This is because the update step has high variance, and through experiement, we found that the high variance does not come from the sampling estimation because the problem still persist even if we perform batched update by averging gradient from multiple trajectory.\n",
    "\n",
    "In fact, the problem come from the importance sampling, and the varaince of the estiamted surrogate function is proportional to the importance sampling ratio of the policy weighting. \n",
    "\n",
    "Using log derivative trick to show that a high variance in importance sampling ratio carries into the gradient.\n",
    "\n",
    "This means the update variance will be huge when the two policy are very different from each other, causing the update to be unstable.\n",
    "\n",
    "The root cause of is that, even if we update the parameter theta by a small amount every step, a small change in parameter may result in a huge chanage in the output policy (sensitivity of the policy), causing high variance importance sampling ratio, which could potentially cause another bigger gradient update, to make the policy even more different. This forms an unstable optimization problem \n",
    "\n",
    "Therefore, to prevent the policy collpase issue from high variance update, we need to make sure the policy before and after the update are close (not the parameter theta are close)enough so the importance sampling ratio is managable and woun't cause too aggresive update\n",
    "\n",
    "in essence, the key problem is that because we try to reuse past data, the importance sampling ratio can be high and lead to high variance update.\n",
    "\n",
    "\n",
    "\n",
    "To make sure the two policies are similar, we can use KL divergence as to measure the differences between the two policy, and regularize it under a certain distance, which we called the trusted region to represents the update is safe with an acceptable Variance.\n",
    "\n",
    "### Trust Regions\n",
    "Standard Policy Gradient methods, such as REINFORCE and baseline, suffer significantly from **sample inefficiency** because of their **on-policy** nature: the mathematical validity of the gradient update relies on the expectation $\\mathbb{E}_{\\tau \\sim \\pi_\\theta}$, meaning data must be generated by the exact policy currently being optimized. \n",
    "\n",
    "Once a gradient update is performed ($\\theta \\to \\theta'$), the trajectory data collected under $\\theta$ becomes mathematically stale. Consequently, for every training step, the agent must interact with the environment to collect new data, and the old data is immediately discarded.\n",
    "\n",
    "While **Off-Policy** methods like DQN utilize **Experience Replay** to reuse past samples, standard PG cannot simply pull data from a replay buffer because it is on-policy. To address this, we must use importance sampling to correct the distribution shift, allowing us to reuse past data.\n",
    "\n",
    "\n",
    "#### The Surrogate Objective Function\n",
    "To utilize data collected from an older policy $\\pi_{\\theta_{old}}$, we employ **Importance Sampling (IS)**. We can rewrite the standard objective using a **Surrogate Function** that re-weights the probability of actions under the current policy relative to the old policy.\n",
    "\n",
    "The standard Policy Gradient is given by:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\ln \\pi_\\theta(a_t|s_t) A_t \\right]\n",
    "$$\n",
    "\n",
    "We aim to construct a surrogate objective $L(\\theta)$ whose gradient equals the policy gradient at the specific point where $\\theta = \\theta_{old}$.\n",
    "\n",
    "Let the probability ratio be $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$. The Surrogate Objective is defined as:\n",
    "$$\n",
    "L^{CPI}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}} \\left[ r_t(\\theta) A_t \\right] = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t \\right]\n",
    "$$\n",
    "\n",
    "We can prove that maximizing this surrogate is locally equivalent to maximizing the original objective by taking the gradient with respect to $\\theta$ and evaluating it at $\\theta_{old}$:\n",
    "\n",
    "1.  Take the gradient of $L^{CPI}$:\n",
    "    $$\n",
    "    \\nabla_\\theta L^{CPI}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t \\right]\n",
    "    $$\n",
    "\n",
    "2.  Evaluate at $\\theta = \\theta_{old}$. At this point, the denominator $\\pi_{\\theta_{old}}$ cancels with the value of the numerator $\\pi_{\\theta}$ (which is $\\pi_{\\theta_{old}}$), but we must be careful with the derivation. Using the log-derivative trick $\\nabla_\\theta \\pi = \\pi \\nabla_\\theta \\ln \\pi$:\n",
    "    $$\n",
    "    \\nabla_\\theta L^{CPI}(\\theta)\\Big|_{\\theta_{old}} = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta_{old}}(a_t|s_t) \\nabla_\\theta \\ln \\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t \\right]\n",
    "    $$\n",
    "\n",
    "3.  Simplify:\n",
    "    $$\n",
    "    \\nabla_\\theta J(\\theta)\\Big|_{\\theta_{old}} = \\nabla_\\theta L^{CPI}(\\theta)\\Big|_{\\theta_{old}} = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}} \\left[ \\nabla_\\theta \\ln \\pi_\\theta(a_t|s_t) A_t \\right]\n",
    "    $$\n",
    "\n",
    "This demonstrates that the gradient of the surrogate function is identical to the true policy gradient at $\\theta_{old}$. However, this is strictly a **local approximation**. Even the gradient is still with respect to $\\theta$, as $\\theta$ moves away from $\\theta_{old}$, the distributions diverge, the IS weights deviate from 1, and the gradient approximation becomes inaccurate.\n",
    "\n",
    "The **key** here is that we try to make an on-policy algorithm off-policy, which results in an estimation that is only accurate within a certain region.\n",
    "\n",
    "This equivalence allows us to fundamentally change the optimization approach. Instead of calculating a single gradient step and discarding data, we can view this as a **maximization problem**. We can run an optimizer (like Adam) on the Surrogate Function $L^{CPI}(\\theta)$ for multiple epochs (updates) using the same batch of data. \n",
    "\n",
    "#### The Policy Collapse Issue\n",
    "A recurring failure mode in Policy Gradient methods—both vanilla and unconstrained surrogate versions—is **Policy Collapse**. During training, the agent's average reward often improves steadily up to a certain point. Suddenly, performance drops drastically, often falling to zero or random-level performance, and the agent fails to recover.\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNUcLArPrBI-SXo1iQYFaSmSRhLxmy6yIcFg&s\" width=300>\n",
    "\n",
    "This instability is driven by **high variance** in the update. Importantly, experiments show that this variance is not solely due to the stochasticity of the environment (sampling error from Monte Carlo returns). The issue persists even when using large batch sizes to average gradients across multiple trajectories.\n",
    "\n",
    "The primary source of this destructive variance is the **Importance Sampling (IS) ratio** itself: $\\rho_t = \\frac{\\pi_\\theta}{\\pi_{old}}$.\n",
    "\n",
    "The variance of an IS estimator is roughly proportional to the expected **square** of the weights ($\\rho_t^2$). If the policies $\\pi_\\theta$ and $\\pi_{old}$ differ significantly, $\\rho_t$ can fluctuate wildly, causing the variance of the estimator to explode.\n",
    "\n",
    "We can see how this carries into the gradient using the log-derivative expansion of the surrogate gradient:\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta) \\approx \\mathbb{E} \\left[ \\underbrace{\\left( \\frac{\\pi_\\theta}{\\pi_{old}} \\right)}_{\\text{Ratio } \\rho_t} \\cdot \\nabla_\\theta \\ln \\pi_\\theta \\cdot A_t \\right]\n",
    "$$\n",
    "\n",
    "The ratio $\\rho_t$ acts as a **scalar multiplier** on the gradient vector. \n",
    "* If $\\rho_t$ spikes, the gradient vector is scaled up.\n",
    "* This results in an excessively large parameter update, effectively kicking the policy into a region of parameter space that has not been explored.\n",
    "\n",
    "The root cause of the high variance gradient is the sensitivity mismatch between **Parameter Space ($\\theta$)** and **Policy Space ($\\pi$)**.\n",
    "1.  **Sensitivity:** Neural networks are non-linear. Gradient descent results in a small change in parameters $\\theta$, but small change in parameters can result in a massive change in the output distribution $\\pi$.\n",
    "2.  **The Loop:** We update $\\theta$ slightly. \n",
    "    * Due to sensitivity, $\\pi_{new}$ drifts far from $\\pi_{old}$.\n",
    "    * This causes the IS ratio $\\rho_t$ to deviate far from 1 (High Variance).\n",
    "    * The high ratio generates a massive gradient update.\n",
    "    * This massive update pushes $\\pi_{new}$ even further away, which means the gradient estimation more inaccurate.\n",
    "\n",
    "This creates an unstable positive feedback loop where the gradient will become more unstable overtime, causing the policy to collapse.\n",
    "\n",
    "To prevent Policy Collapse, we must impose a constraint on the update by keeping the two policy relatively close. In fact, we do not strictly care if the parameters $\\theta$ change significantly. We care that the **policy distribution** $\\pi$ remains close to the data-generating distribution $\\pi_{old}$.\n",
    "\n",
    "If we ensure the policies are \"close enough\":\n",
    "1.  The IS ratio $\\rho_t$ remains close to 1.\n",
    "2.  The variance of the update remains bounded.\n",
    "3.  The local approximation of the Surrogate remains accurate.\n",
    "\n",
    "This leads to **Trust Region** methods (like TRPO and PPO), which explicitly constrain the update to ensure that the divergence between $\\pi_\\theta$ and $\\pi_{old}$ stays within a safe threshold, effectively preventing the variance explosion caused by reusing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d28fae",
   "metadata": {},
   "source": [
    "\n",
    "### Advanages of Policy Gradient Method\n",
    "The optimal policy for a given problem can be deterministic or stochastic. In value-based methods, improving the policy usually involves making it greedy or $\\epsilon$-greedy with respect to the current action-value estimates.\n",
    "\n",
    "However, this approach has two major limitations:\n",
    "1. Difficulty in Learning Stochastic Policies: in many problems, the optimal policy is stochastic, meaning the agent must randomize its actions to perform well (e.g., Rock-Paper-Scissors, exploration in partially observable environments). Value-based methods struggle here because they force a near-deterministic improvement step by greedily selecting the best action or occasionally exploring using $\\epsilon$-greedy. This makes it difficult for the agent to naturally converge to a well-calibrated stochastic policy, as the randomness is artificially imposed instead of being learned.\n",
    "\n",
    "2. Limitations of $\\epsilon$-Greedy Exploration: $\\epsilon$-greedy is used in both tabular and function approximation settings to ensure exploration. However, because exploration is purely random, the agent always explores, even when it has already learned the optimal behavior. This creates a performance ceiling, as the agent never fully transitions to optimal exploitation and continues to take suboptimal random actions occasionally.\n",
    "\n",
    "\n",
    "Policy gradient methods directly parameterize and optimize the policy. This allows them to adapt naturally to the type of policy required. If the optimal policy is deterministic, the learned policy will gradually converge to a pure deterministic form without requiring artificial exploration like $\\epsilon$-greedy. If the optimal policy is stochastic, the method will self-adjust and discover the correct distribution of actions through experience. This flexibility makes policy gradient methods more expressive and often more suitable for complex or uncertain environments.\n",
    "\n",
    "Also, in some problems, the value function can be extremely complex, while the policy itself is simple. In such cases, directly learning the policy is more efficient and practical than learning a full value function and deriving the policy indirectly. However, this is problem-dependent as there are also situations where value-based methods are simpler and more stable.\n",
    "\n",
    "### Policy Gradient Objective\n",
    "The goal of reinforcement learning is to find a policy that maximizes the agent’s cumulative rewards. For policy gradient methods, we directly optimize a parameterized policy $\\pi(a \\mid s, \\theta)$. The objective function for this approach can be defined as the average reward under a given policy, where\n",
    "\n",
    "$$r(\\pi) = \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s’, r} p(s’, r \\mid s, a) , r$$\n",
    "\n",
    "* $\\sum_{s’, r} p(s’, r \\mid s, a) r$: the expected immediate reward for taking action a in state s.\n",
    "* $\\pi(a \\mid s) \\sum_{s’, r} p(s’, r \\mid s, a) r$: the expected reward at state s, weighted by the probability of selecting action a under policy $\\pi$.\n",
    "* $\\mu_{\\pi}(s)$: The stationary distribution over states under policy $\\pi$, representing how frequently each state is visited when the agent follows $\\pi$.\n",
    "\n",
    "\n",
    "This objective function captures the long-term performance of a policy, where it first accounts for how often each state is visited $\\mu_{\\pi}(s)$. Then, for each state, it considers the probability of taking each action and the expected reward for that action.\n",
    "\n",
    "By maximizing $r(\\pi)$ with respect to the policy parameters $\\theta$, the agent gradually adjusts its behavior to favor actions and states that yield higher rewards.\n",
    "\n",
    "Formally, the optimization problem is:\n",
    "\n",
    "$$\\theta^* = \\arg \\max_\\theta r(\\pi_\\theta)$$\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "Once we define the learning objective, our next step is to optimize it by adjusting the policy parameters $\\theta$. Since our goal is to maximize the objective, we use gradient ascent to update the parameters in the direction of increasing performance. The gradient of the average reward objective is\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} r(\\pi) = \\nabla_{\\theta} \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s’, r} p(s’, r \\mid s, a) , r\n",
    "$$\n",
    "\n",
    "Once we know the gradient of the objective, we can update the policy's parameter, $\\theta$, with gradient ascent with step size of $\\alpha$, where\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} r(\\pi)$$\n",
    "\n",
    "However, in the gradient of the average reward, the term $\\mu_{\\pi}(s)$ represents the state distribution under policy $\\pi$ because his distribution changes whenever the policy changes, making its gradient extremely difficult to compute accurately, which is impractical.\n",
    "\n",
    "The policy gradient theorem resolves this issue by providing a clean, analytic expression for the gradient that avoids differentiating the state distribution. It shows that\n",
    "\n",
    "$$\\nabla_{\\theta} r(\\pi) = \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a \\mid s) , q_{\\pi}(s, a)$$\n",
    "\n",
    "* $q_{\\pi}(s, a)$: the action-value function, representing the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$.\n",
    "* $\\nabla_{\\theta} \\pi(a \\mid s)$: the gradient of the policy with respect to its parameters.\n",
    "\n",
    "This form bypasses the need to compute $\\nabla_{\\theta} \\mu_{\\pi}(s)$ and gives a practical way to estimate the gradient from experience.\n",
    "\n",
    "In the policy gradient theorem expression, $\\nabla_{\\theta} \\pi(a \\mid s)$ describes how the probability of selecting each action changes as the policy parameters $\\theta$ are adjusted. \n",
    "* If a small increase in a parameter raises the probability of selecting an action, the gradient is positive for that action.\n",
    "* If a small increase lowers the probability, the gradient is negative.\n",
    "\n",
    "The action-value $q_{\\pi}(s, a)$ serves as a signal that guides the gradient update, indicating whether the probability of selecting an action should be increased or decreased to improve the policy.\n",
    "* If an action's expected reward is better than average, $q_{\\pi}(s, a)$ will be high. This increases the probability of selecting that action in the future.\n",
    "* If an action's expected reward is worse than average, $q_{\\pi}(s, a)$ will be low or negative. This decreases the probability of selecting that action.\n",
    "\n",
    "Thus, with the two term together, the policy gradient theorem naturally pushes the policy toward actions that yield higher rewards, improving the agent’s behavior over time.\n",
    "\n",
    "The update rule is given by\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a \\mid s) , q_{\\pi}(s, a)$$\n",
    "\n",
    "### Stochastic Policy Gradient\n",
    "In the current policy gradient expression, it requires summing over all possible state action pairs to computes the full gradient, which is inpractical since the state action space is often very large.\n",
    "$$\\nabla_{\\theta} r(\\pi) = \\sum_{s \\in \\mathcal{S}} \\mu_{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi(a \\mid s) , q_{\\pi}(s, a)$$\n",
    "\n",
    "Therefore, we can simplify the gradient computation by computing a stochastic gradient of only based on only one samples. This can be done by replacing $s$ and $a$ with an observed state action pair $(S_t, A_t)$.\n",
    "\n",
    "$$\\nabla_{\\theta} r(\\pi) \\approx \\frac{\\nabla_{\\theta} \\pi(A_t \\mid S_t)}{\\pi(A_t \\mid S_t)} q_{\\pi}(S_t, A_t) = \\nabla_{\\theta} \\ln{\\pi(A_t \\mid S_t)}q_{\\pi}(S_t, A_t)$$\n",
    "\n",
    "This formula approximates the true grtadient using only one sample, and the update rule becomes\n",
    "\n",
    "$$\\theta_{t+1} = ...$$\n",
    "\n",
    "\n",
    "## Stochastic Policy Gradient\n",
    "\n",
    "In the current policy gradient expression, the exact policy gradient sums over all state–action pairs, which is intractable in large spaces:\n",
    "\n",
    "$$\\nabla_\\theta r(\\pi_\\theta)\n",
    "= \\sum_{s\\in\\mathcal S}\\mu_{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\mid s),\\nabla_\\theta \\ln \\pi_\\theta(a\\mid s) q_{\\pi_\\theta}(s,a)$$\n",
    "\n",
    "Instead, we use a stochastic estimation by sampling a single state–action pair $(S_t,A_t)$ from the on-policy distribution and replacing the expectation with that sample\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta r(\\pi_\\theta)\n",
    "\\approx \\nabla_\\theta \\ln \\pi_\\theta(A_t\\mid S_t) q(S_t,A_t)\n",
    "$$\n",
    "\n",
    "where $q(S_t,A_t)$ is an observed sample at a timesteps. Now, the agent is able to update the policy online after each timestep with the newly observed state-action pair. Therefore, the update rule becomes\n",
    "\n",
    "$$\n",
    "\\theta_{t+1}\n",
    "= \\theta_t + \\alpha, \\nabla_\\theta \\ln \\pi_\\theta(A_t\\mid S_t) q(S_t,A_t)\n",
    "$$\n",
    "\n",
    "## Actor-Critic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1afc4",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/0*P8RnG_xRY8sThfyp.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c184892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
