{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4166efe",
   "metadata": {},
   "source": [
    "# Anomaly detection\n",
    "Anomaly detection detect unusual events by comparing it to normal data\n",
    "\n",
    "## Gaussian distribution\n",
    "For a number $x$, it's probability, $p(x)$, is determined by normal distribution with a mean value ($\\mu$) and a standard deviatioin ($\\sigma$), where $\\mu$ determines the center of the curve and $\\sigma$ determines the spread of the data\n",
    "\n",
    "The probability curve is given by\n",
    "$$ p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x - \\mu)^2}{2 \\sigma ^2} },$$\n",
    "where $p(x)$ determines the probability of the given feature being \"normal\"\n",
    "\n",
    "<img src=\"https://www.inchcalculator.com/wp-content/uploads/2021/11/normal-distribution-graph.png\" width=500>\n",
    "\n",
    "Note: the total area under the curve always equals 1\n",
    "\n",
    "## Density estimation\n",
    "Given a threshold $\\epsilon$ and a test data ($x_{test}$), if $p(x_{test}) \\geq \\epsilon$, the data is considered \"normal\". If $p(x_{test}) < \\epsilon$, the data is considered an anomaly\n",
    "\n",
    "## Anomaly detection algorithm\n",
    "Given a training set $\\{(\\vec x^{(1)}, \\vec x^{(2)}, ... \\vec x^{(m)})\\}$ with $m$ training example, where each training example is a column vector containing $n$ features\n",
    "\n",
    "To determine if a test data is \"normal\", we want to first determine how \"normal\" is each feature of the test example\n",
    "\n",
    "### Calculating $\\mu$ and $\\sigma$ for each feature\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n",
    "\n",
    "$\\mu_i$: the mean of $i$th feature\n",
    "\n",
    "$x_i^{(j)}$: the value of $i$th feature for $j$th training example \n",
    "\n",
    "$\\sum_{j=1}^m x_i^{(j)}$: the sum of $i$th feature across all training examples\n",
    "\n",
    "$\\sigma_i^2$: the variance $i$th feature\n",
    "\n",
    "\n",
    "### Vectorzied implementation\n",
    "\n",
    "$$\\vec \\mu = \\frac{1}{m} \\sum_{i=1}^m \\vec x^{(i)}$$\n",
    "\n",
    "$\\vec \\mu$: a vector with $n$ entries representing the mean of all features\n",
    "\n",
    "$x^{(i)}$: $i$th training example that contains $n$ features\n",
    "\n",
    "### Final probability\n",
    "After calculating the mean and variance for all $n$ features ($\\mu_1, \\mu_2, ..., \\mu_n, \\sigma_1^2, \\sigma_2^2, ..., \\sigma_n^2$), the probability is\n",
    "\n",
    "$$p(\\vec x) = p(x_1, \\mu_1, \\sigma_1^2) * p(x_2, \\mu_2, \\sigma_2^2) * ... * p(x_n, \\mu_n, \\sigma_n^2)$$\n",
    "$$= \\Pi_{j=1}^{n} p(x_j, \\mu_j, \\sigma_j^2)$$\n",
    "$$= \\Pi_{j=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma_j^2}}\\exp^{ - \\frac{(x_j - \\mu_j)^2}{2 \\sigma_j^2} }$$\n",
    "\n",
    "$\\vec x$: a given test data\n",
    "\n",
    "$p(x_j, \\mu_j, \\sigma_j^2)$: the probability of $j$th feature being \"normal\"\n",
    "\n",
    "$\\Pi_{j=1}^{n} p(x_j, \\mu_j, \\sigma_j^2)$: the consecutive multiplication of the probabilty of each feature being \"normal\"\n",
    "\n",
    "We can then take $p(\\vec x)$ and compare it to the threshold, $\\epsilon$\n",
    "\n",
    "## Selecting thershold $\\epsilon$\n",
    "To select the threshold, we can set multiple $\\epsilon$ values and calculate the F1 score using precision and recall. Then, select the $\\epsilon$ value with the highest F1 score (highest precision and recall) as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c979626",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593c27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19da7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance for all features\n",
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of training examples and features\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize an array with n entries\n",
    "    mu = np.zeros(n)\n",
    "    var = np.zeros(n)\n",
    "    \n",
    "    # Calculating mu\n",
    "    # Looping through each feature\n",
    "    for j in range(n):\n",
    "        # Looping through each training examples\n",
    "        for i in range(m):\n",
    "            mu[j] += X[i, j]\n",
    "    mu /= m\n",
    "    \n",
    "    # Calculating variance\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            var[j] += (X[i, j] - mu[j]) ** 2\n",
    "    \n",
    "    var /= m\n",
    "    \n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5885f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of each feature: [14.26889914 15.26989617]\n",
      "Variance of each feature: [0.83657963 0.66966256]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_train = np.array([[15.79025979, 14.9210243 ],\n",
    " [13.63961877, 15.32995521],\n",
    " [14.86589943, 16.47386514],\n",
    " [13.58467605, 13.98930611],\n",
    " [13.46404167, 15.63533011],])\n",
    "\n",
    "mu, var = estimate_gaussian(X_train)              \n",
    "\n",
    "print(\"Mean of each feature:\", mu)\n",
    "print(\"Variance of each feature:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1833400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probablity density function\n",
    "def multivariate_gaussian(X, mu, var):\n",
    "    \"\"\"\n",
    "    Computes the probability \n",
    "    density function of the examples X under the multivariate gaussian \n",
    "    distribution with parameters mu and var. If var is a matrix, it is\n",
    "    treated as the covariance matrix. If var is a vector, it is treated\n",
    "    as the var values of the variances in each dimension (a diagonal\n",
    "    covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    k = len(mu)\n",
    "    \n",
    "    if var.ndim == 1:\n",
    "        var = np.diag(var)\n",
    "        \n",
    "    X = X - mu\n",
    "    p = (2* np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * \\\n",
    "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb1191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the best threshold to use for selecting outliers based on the results from a validation set (p_val) and the ground truth (y_val)\n",
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "    \n",
    "       # prediction is a binary array that tell if each training example is normal or anomalous\n",
    "        prediction = (p_val < epsilon)\n",
    "        \n",
    "        # Calculation for F1 score\n",
    "        tp = np.sum((y_val == 1) & (prediction == 1))\n",
    "        fp = np.sum((y_val == 0) & (prediction == 1))\n",
    "        fn = np.sum((y_val == 1) & (prediction == 0))\n",
    "        \n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "        \n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "        # Update F1 if there is a better one\n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05078a96",
   "metadata": {},
   "source": [
    "# Real number evaluation\n",
    "To evaluate how well an anomaly detection algorithm performs, we can assume or collect some labelled data\n",
    "\n",
    "Steps\n",
    "1. Seperate the labelled data by only putting normal training examples into the training set and splitting anomalous examples into the validation and testing set\n",
    "2. Train the algorithm using the training set (only normal traing examples)\n",
    "3. Check how well the algorithm predict anomalous data using the validation set and fine tuning the algorithm (eg. change the threshold)\n",
    "4. Evaluate the algorithm with the test set\n",
    "\n",
    "Note: if the anomalous examples are really rare, the methods to evaluate skewed data can be applied\n",
    "\n",
    "\n",
    "# Anomaly detection vs Supervised learning\n",
    "Anomaly detection should be used when the number of anomalous examples are small, there are multiple types of anomalies, and future anomalies may not looks like any given training data\n",
    "\n",
    "Supervised learning should be used when there is enough normal and anomalous examples and future examples will look very similar to the training data\n",
    "\n",
    "# Selecting features to use for anomaly detection\n",
    "Anomaly detection algorithm works well with gaussian features (features that follow the normal distribution). When having non-gaussian features, we can transform them by\n",
    "* Taking the square root of the feature\n",
    "* Taking the log of the feature\n",
    "* Taking $log(x + c)$, where x is the value of the feature and c is an arbitary constant\n",
    "\n",
    "# Error analysis\n",
    "Sometimes, an anomalous example can have $p(x)$ similar to normal examples. We can manually inspect the anomaly to determine why it is an anomaly and creating new features based on the given data to help the algorithm detect it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
