{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab0a3e0",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "VAE is a type of generative neural network architecture that consists a encoder and a decoder\n",
    "\n",
    "* Traditional autoencoder\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1234/format:webp/1*qVx8wDUpqIqpEWy6-fW3Cg.png\" width=400>\n",
    "\n",
    "* VAE\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXiln_TbF15oVg7AjcUEkQ.png\" width=400>\n",
    "\n",
    "* Encoder: the encoder takes an input and compress it into a latent representation. Traditional autoencoder outputs a single point (vector) in the latent space. A VAE output a probability distribution over the latent space\n",
    "\n",
    "* Decoder: the decoder samples from the distribution in the latent space given by the encoder and try to reconstruct the input as accurately as possible by upsampling the encoding\n",
    "\n",
    "After training, we will remove the encoder and randomly sample from the distribution in the latent space to produce output\n",
    "\n",
    "### Issue with traditional autoencoder and advantages of VAE\n",
    "Traditional autoencoder will convert input data into points in the latent space, which are discrete. However, during generation, we can only sample from those discrete points, meaning we're essentially replicating the same set of images. If we use a data point that's not in this discontinuous latent space, the decoder output will be unrealistics since it has never been trained on those inputs.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-i8cp3ry4XS-05OWPAJLPg.png\" width=300>\n",
    "\n",
    "Compare to traditional autoencoder, VAE has the advantage of:\n",
    "1. VAE learns the distribution of the input data and can generate sample similar to original input\n",
    "2. Since the VAE encoder outputs a distribution, its latent space is smooth and continuous, meaning we can interpolate between 2 points and generate outputs with smooth transition\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96ho7qSyW0nKrLvSoZHOtA.png\">\n",
    "\n",
    "### Encoder\n",
    "In order to output a distribution, the encoder outputs will output 2 vectors, one for the means, $\\mu$, and one for the standard deviations, $\\sigma$, where each pair of $\\mu$ and $\\sigma$ represents the distribution of 1 feature in the embedding of the input. With these 2 vectors, we can randomly sample each feature to form an encoding and pass it on to the decoder\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1314/format:webp/1*CiVcrrPmpcB1YGMkTF7hzA.png\" width=500>\n",
    "\n",
    "VAEs has density estimation, invertiable, and has more stable training compared to GANs, but it also generates lower quality results\n",
    "\n",
    "## Loss function\n",
    "### Loss for traditional autoencoder\n",
    "The loss function for a traditional autoencoder is simply the binary cross entropy loss or MSE loss. This loss function tells the model how different is the generated image and the input image. This loss is called the reconstruction loss\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*zLOeTMUQ67OrqjLp.png\" width=600>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:808/1*-e1QGatrODWpJkEwqP4Jyg.png\" width=300>\n",
    "\n",
    "$y_i$: the given input image/its feature map\n",
    "\n",
    "$\\hat{y_i}$: the generated image by the decoder/its feature map\n",
    "\n",
    "$N$: number of pixels in the image/number of element in the extracted feature map\n",
    "\n",
    "The reconsturction loss can either be calculated on based on the pixel distance between the input and generated images or the distance of their feature map. The feature map of the image can be obtained by passing it into a pre-trained network and extract the activation from a middle layer. In general, the feature map is more realiable than the pixel distance\n",
    "\n",
    "\n",
    "### KL divergence term \n",
    "One issue the encoder may experience using reconstruction loss is that the range of $\\mu$ and $\\sigma$ for each class can be very different, which means there will be discontinuity between in between the distributions. However, we want to make the mean of each class somewhat close to each other so the model will be able to generate output in between classes\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xCjoga9IPyNUSiz9E7ao7A.png\" width=500>\n",
    "\n",
    "To resolve this problem, we need to introduce a KL divergence term to the loss function. This term measures the difference between the learned probability distribution over the latent space and a predefined prior distribution (usually a standard normal distribution). Minimizing this term will ensure that the learned distribution over the latent space is close to the prior distribution, which means the mean and standard deviation of each class will have very similar value.\n",
    "\n",
    "Note: the predefined prior distribution is arbitrary; it only tells the model where we want all the distribution to be centered at\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1040/format:webp/1*uEAxCmyVKxzZOJG6afkCCg.png\" width=300>\n",
    "\n",
    "$n$: number of elements in the $\\mu$ or $\\sigma$ vector (they have the same size)\n",
    "\n",
    "$\\mu_i$: the $i$th element in the $\\mu$ vector\n",
    "\n",
    "$\\sigma_i$: the $i$th element in the $\\sigma$ vector\n",
    "\n",
    "In this case, we choose the predefined distribution to be the normal distribution with mean of 0 and standard deviation of 1. Therefore, this loss function forces the output $\\mu$ and $\\sigma$ vector to take on similar values\n",
    "\n",
    "However, if we only use the KL divergence loss without the reconstruction loss, the distribution for all classes are densely placed and the decoder cannot decode any valuable information from it, so the reconstruction term and the KL divergence term should be used together to make up the loss function\n",
    "\n",
    "* Distribution by only using KL divergence term as loss function (results in posterior collapse issue)\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1294/format:webp/1*XdSPoB3rcb7LymviDJBJUg.png\" width=350>\n",
    "\n",
    "* Distribution by using both reconstruction cost and KL divergence term as loss function (results in dense, but distinguishable distribution)\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1286/format:webp/1*BIDBG8MQ9-Kc-knUUrkT3A.png\" width=350>\n",
    "\n",
    "### Full VAE loss function\n",
    "The loss function used for VAE is reconstruction loss and KL divergence term; we want to minimize both term at the same time\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DJeT99qHwbF7iewk.png\" width=700>\n",
    "\n",
    "$L$: ELBO (Expectation Lower BOund), which is essentially the total loss that we try to minimize\n",
    "\n",
    "$x^{(i)}$: the input image\n",
    "\n",
    "$\\theta, \\phi$: parameters of the decoder and the encoder respectively\n",
    "\n",
    "$z$: sampled latent variable\n",
    "\n",
    "$p_{\\theta}(x^{(i)}|z)$: the expectation of the log probability of the data given the latent variable $z$. It measures that given a input latent vector $z$, how well the decoder can reconstruct it to match the input image, $x^{(i)}$\n",
    "\n",
    "$p_{\\theta}(z)$: the predefined prior distribution (normal distribution is most commonly used)\n",
    "\n",
    "$q_{\\phi}(z|x^{(i)})$: the output distribution by the encoder given a input image $x^{(i)}$\n",
    "\n",
    "## Training process\n",
    "1. Feed an image to the encoder, which outputs parameters $\\mu$ and $\\sigma$ of the latent distribution\n",
    "2. Sample a latent variable $z$ based on $\\mu$ and $\\sigma$ using the reparametrization trick\n",
    "3. Feed the latent variable to the decoder to generate a output image $\\hat{x}$\n",
    "4. Compute the loss using ELBO loss function\n",
    "5. Use gradient descent to minize the ELBO loss and update the model's parameters\n",
    "\n",
    "### Reparametrization trick\n",
    "To generate a latent variable, we sample a random variable from the distribution based on $\\mu$ and $\\sigma$. However, backpropagation requires calculating the gradients with respect to the modelâ€™s parameters ($\\mu$ and $\\sigma$), but calculating the gradient with respect to z (a randomly sampled variable) is mathematically undefined. The reparametrization trick solve this problem\n",
    "\n",
    "Instead of directly sampling $z$ from its distribution, we decompose z into a deterministic component and a stochastic component by introducing a new random variable $\\epsilon$, where $\\epsilon$ is randomly sampled from the standard normal distribution. So the latent variable is now calculated as\n",
    "\n",
    "$$z = \\mu + \\epsilon * \\sigma$$\n",
    "\n",
    "Now, $z$ still captures the original distribution based on $\\mu$ and $\\sigma$ to properly perform backpropagation while $\\epsilon$ introduce randomness to the sampling process\n",
    "\n",
    "## Flow model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d50402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
