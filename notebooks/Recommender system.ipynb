{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1412d20e",
   "metadata": {},
   "source": [
    "# Recommender system\n",
    "Recommender system provide recommendations based on user data\n",
    "\n",
    "## Notations\n",
    "$n_u$: number of users\n",
    "\n",
    "$n_m$: number of items\n",
    "\n",
    "$n$: number of features\n",
    "\n",
    "$r(i,j)$: if user $j$ has rated $i$th item (0 if not, 1 if rated)\n",
    "\n",
    "$y(i,j)$: rating given by $j$th user to $i$th item\n",
    "\n",
    "$\\vec w^{(j)}, b^{(j)}$: weights and bias for $j$th user\n",
    "\n",
    "$\\vec x^{(i)}$: a vector contains the features for $i$th item\n",
    "\n",
    "$X$: a matrix of vectors $\\vec x^{(i)}$\n",
    "\n",
    "$W$: a matrix of vectors $\\vec w^{(j)}$\n",
    "\n",
    "$b$: a vector of bias $b^{(j)}$\n",
    "\n",
    "$R$: a binary indicator matrix of elements $r(i,j)$\n",
    "\n",
    "$Y$: a $n_m \\times n_u$ matrix that stores users' rating to each item, where each row represents an item and each column represents a user's rating to each item \n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\mathbf{x}^{(0)})^T --- \\\\\n",
    "--- (\\mathbf{x}^{(1)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\mathbf{x}^{(n_m-1)})^T --- \\\\\n",
    "\\end{bmatrix} , \\quad\n",
    "\\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\mathbf{w}^{(0)})^T --- \\\\\n",
    "--- (\\mathbf{w}^{(1)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\mathbf{w}^{(n_u-1)})^T --- \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{ b} = \n",
    "\\begin{bmatrix}\n",
    " b^{(0)}  \\\\\n",
    " b^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(n_u-1)} \\\\\n",
    "\\end{bmatrix}\\quad\n",
    "$$ \n",
    "\n",
    "* The $i$-th row of $\\mathbf{X}$ corresponds to the feature vector, $\\vec x^{(i)}$, for the $i$th item\n",
    "* The $j$th row of $\\mathbf{W}$ corresponds to the weight vector, $\\vec {w}^{(j)}$, for the $j$th user\n",
    "* $\\vec x^{(i)}$ and $\\vec{w}^{(j)}$ are $n$ dimensional vectors, where $n$ is number of features\n",
    "\n",
    "\n",
    "# Collaborative filtering\n",
    "Collaborative filtering gives recommendation based on rating of users who gave similar ratings as you\n",
    "\n",
    "The predicted rating for the $j$th user on the $i$th item is\n",
    "$$\\text{Predicted rating} = \\vec w^{(j)} \\cdot \\vec x^{(i)} + b^{(j)}$$\n",
    "\n",
    "In collaborative filtering algorithm, we want the algorithm to learn both the features of the items ($\\vec x^{(i)}$) and user preferences (depending on $\\vec w^{(j)}$ and $b^{(j)}$) at the same time\n",
    "\n",
    "## Cost function\n",
    "\n",
    "The collaborative filtering cost function is given by\n",
    "$$J({\\mathbf{x}^{(0)},...,\\mathbf{x}^{(n_m-1)},\\mathbf{w}^{(0)},b^{(0)},...,\\mathbf{w}^{(n_u-1)},b^{(n_u-1)}})= \\left[ \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right]\n",
    "+ \\underbrace{\\left[\n",
    "\\frac{\\lambda}{2}\n",
    "\\sum_{j=0}^{n_u-1}\\sum_{k=0}^{n-1}(\\mathbf{w}^{(j)}_k)^2\n",
    "+ \\frac{\\lambda}{2}\\sum_{i=0}^{n_m-1}\\sum_{k=0}^{n-1}(\\mathbf{x}_k^{(i)})^2\n",
    "\\right]}_{regularization}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left[ \\frac{1}{2}\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right]\n",
    "+\\text{regularization}\n",
    "$$\n",
    "\n",
    "$(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2$: the squared error between predicted rating and actual rating\n",
    "\n",
    "$\\sum_{j=0}^{n_u-1} \\sum_{i=0}^{n_m-1}r(i,j)*(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 $: the sum of squared errors. If the $i$th user gives $j$th item a rating, $r(i,j) = 1$, and the error will count towards the total cost. If $i$th user does not give $j$th item a rating, $r(i,j) = 0$, this term will be ignored\n",
    "\n",
    "$\\sum_{j=0}^{n_u-1}\\sum_{k=0}^{n-1}(\\mathbf{w}^{(j)}_k)^2$: regularize the weights for all users ($n_u$ users in total), each user have $n$ features\n",
    "\n",
    "$\\sum_{i=0}^{n_m-1}\\sum_{k=0}^{n-1}(\\mathbf{x}_k^{(i)})^2$: regularize the weights for all items ($n_m$ users in total), each item have $n$ features\n",
    "\n",
    "Note: $\\mathbf{w}^{(j)}$ and $\\mathbf{x}^{(i)}$ must have the same number of features\n",
    "\n",
    "Minimize this cost function will provide the best fit for a recommender system\n",
    "\n",
    "## Gradient descent\n",
    "Since the algorithm is learning both the features of the items ($\\vec x^{(i)}$) and user preferences (depending on $\\vec w^{(j)}$ and $b^{(j)}$) at the same time, each iteration of gradient descent should update $w$, $b$, and $x$ simultaneously\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w^{(j)}_{i} = w^{(j)}_{i} -  \\alpha \\frac{\\partial J({w},b,x)}{\\partial w^{(j)}_{i}} \\  \\; \\newline\n",
    "&b^{(j)}\\ \\ = b^{(j)} -  \\alpha \\frac{\\partial J(\\vec {w},b,x)}{\\partial b^{(j)}}  \\newline \n",
    "& x^{(i)}_{k} = x^{(i)}_{k} -  \\alpha \\frac{\\partial J({w},b,x)}{\\partial x^{(i)}_{k}} \\  \\; \\newline\n",
    "\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "$w^{(j)}_{i}$: the weight for the $i$th feature for $j$th user\n",
    "\n",
    "$x^{(i)}_{k}$: the $k$th feature for $i$th item\n",
    "\n",
    "## Binary application\n",
    "Binary application classify whether an user like an item or not (0 if not, 1 if yes)\n",
    "\n",
    "Prediction function:\n",
    "$$f_{w,b,x}(x) = g(\\vec w^{(j)} \\cdot \\vec x^{(i)} + b^{(j)})$$\n",
    "\n",
    "$g$: sigmoid function that output a value between 0 and 1\n",
    "\n",
    "Loss funcion for a single user prediction:\n",
    "$$Loss = L(f_{w,b,x}(x), y^{(i,j)}) = (-y^{(i,j)}) \\log\\left(f_{w,b,x}\\left(x\\right) \\right) - \\left( 1 - y^{(i,j)}\\right) \\log \\left( 1 - f_{w,b,x}\\left(x\\right) \\right)$$\n",
    "\n",
    "Cost function:\n",
    "$$J(w,b,x) = \\sum_{(i:j):r(i,j)=1}L(f_{w,b,x}(x), y^{(i,j)})$$\n",
    "\n",
    "$\\sum_{(i:j):r(i,j)=1}$: sub in $i$ and $j$ if a rating is given ($r(i,j) != 0$)\n",
    "\n",
    "\n",
    "## Limitation\n",
    "Collaborative filtering is not good at\n",
    "* Cold start problem: predict rating for a new item with very few rating or predict the preference of a new user who rated very few items\n",
    "* Can not use side information from items and users: age, gender, location,etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5cb5f",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c95dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67cb9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering cost\n",
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    # Get number of items, users and features\n",
    "    nm, nu = Y.shape\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Define variables\n",
    "    J = 0\n",
    "    regularization = 0\n",
    "    \n",
    "    # Calucalate cost \n",
    "    for i in range(nm):\n",
    "        for j in range(nu):\n",
    "            \n",
    "            # If the item has a ratiing\n",
    "            if R[i, j] == 1:\n",
    "                # Add the squared error\n",
    "                J += (np.dot(W[j], X[i]) + b[0][j] - Y[i, j])**2\n",
    "    J /= 2   \n",
    "    \n",
    "    # Calculate regularzation for w\n",
    "    for i in range(nu):\n",
    "        for j in range(n):\n",
    "            regularization += (W[i, j])**2\n",
    "    \n",
    "    # Calculate regularzation for w\n",
    "    for i in range(nm):\n",
    "        for j in range(n):\n",
    "            regularization += (X[i, j])**2\n",
    "            \n",
    "    J += lambda_ * regularization / 2\n",
    "            \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790f88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation for cost function\n",
    "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27809052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implmenting collaborative filtering   \n",
    "num_movies, num_users = 4778, 443 #Y.shape\n",
    "num_features = 100\n",
    "\n",
    "# Set Initial Parameters (W, X), use tf.Variable to allow tensorflow to track these variables\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W = tf.Variable(tf.random.normal((num_users, num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1, num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# Apply Adam optimization\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)\n",
    "\n",
    "# Normalize the Dataset\n",
    "# Ynorm, Ymean = normalizeRatings(Y, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff722ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Use TensorFlow’s GradientTape\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# to record the operations used to compute the cost \u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Compute the cost (forward pass included in cost)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         cost_value \u001b[38;5;241m=\u001b[39m cofi_cost_func_v(X, W, b, \u001b[43mY\u001b[49m, R, lambda_)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Use the gradient tape to automatically retrieve\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# the gradients of the trainable variables with respect to the loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient( cost_value, [X,W,b] )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "iterations = 200\n",
    "lambda_ = 1\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlow’s GradientTape to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to get the graident with respect to X, W, b and organize it in an array\n",
    "    grads = tape.gradient(cost_value, [X,W,b])\n",
    "\n",
    "    # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, [X,W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2d15c",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Mean normalization\n",
    "When an user does not rate many item, we can use initialize a rating for that user based on other users' ratings on that item. This helps the algorithm to make better predictions\n",
    "\n",
    "To apply mean normalization, for each item, calculate the average rating for each item and subtract individual rating by the average rating of that item. Then, initialize the unknown rating to be the average rating of that item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15b5ba",
   "metadata": {},
   "source": [
    "# Determine related items\n",
    "Each feature $x^{(i)}$ of item $i$ is hard to interpret. To find similar items, find item $k$ with $x^{(k)}$ close to $x^{(i)}$, which means the distance between two vectors are small\n",
    "\n",
    "$$Distance = \\sum^{n}_{l=1}(x^{(k)}_{l}-x^{(i)}_{l})^2 = ||x^{(k)}-x^{(i)}||$$\n",
    "\n",
    "The smaller the distance, the more similar the two items are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4553e1",
   "metadata": {},
   "source": [
    "# Content-based filtering\n",
    "Content_based filtering recommend items based on features of user and items to find a good match\n",
    "\n",
    "## Notations\n",
    "$x^{(j)}_u$: a vector that contains the features about the $j$th user \n",
    "\n",
    "$x^{(i)}_m$: a vector that contains the features about the $i$th item\n",
    "\n",
    "$v^{(j)}_u$: a vector that represents the preference of $j$th user\n",
    "\n",
    "$v^{(i)}_m$: a vector that represents the features of $i$th item\n",
    "\n",
    "* $v^{(j)}_u$ and $v^{(i)}_m$ are compuated based on $x^{(j)}_u$ and $x^{(i)}_m$\n",
    "* $x^{(j)}_u$ and $x^{(i)}_m$ do not need to be the same size, but $v^{(j)}_u$ and $v^{(i)}_m$ must be the same size\n",
    "* The predicted rating that the $j$th user given on the $i$th item is\n",
    "$$\\text{Predicted rating} = v^{(j)}_u \\cdot v^{(i)}_m$$\n",
    "\n",
    "## Getting $v_u$ and $v_m$\n",
    "\n",
    "To obtain $v_u$ and $v_m$, we can apply two neural netowrks, one user network and one item network. The user network will have a input layer of $x_u$, and the item network will have a input layer of $x_m$. Two networks can have different architectures but the output layers must have the same size. We can then use $v_u$ and $v_m$ to make a prediction\n",
    "\n",
    "## Cost function\n",
    "The cost function for the final prediction\n",
    "$$J = \\sum_{(i:j):r(i,j)=1}(v^{(j)}_u \\cdot v^{(i)}_m - y^{(i,j)})^2 + regularization$$\n",
    "\n",
    "## Retrieval and ranking\n",
    "Retrieval and ranking allows us to provide recommendations from a large set\n",
    "\n",
    "* Retrival \n",
    "1. Generate a large list of plausible item candidates (closet to user preference/trending items) \n",
    "2. Combine items into list and remove duplicates\n",
    "\n",
    "* Ranking\n",
    "1. Take the retrieval list and rank based the items cloest to user preference\n",
    "2. Display items to user based on ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90e8ef",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a85013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5024b737",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_user_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m item_NN \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     15\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     16\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     17\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39mnum_outputs, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m   \n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# create the user input vector, point it to the user network, and normalize the output\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m input_user \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[43mnum_user_features\u001b[49m))\n\u001b[1;32m     23\u001b[0m vu \u001b[38;5;241m=\u001b[39m user_NN(input_user)\n\u001b[1;32m     24\u001b[0m vu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39ml2_normalize(vu, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_user_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Construct neural network\n",
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# User network \n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_outputs, activation='linear')\n",
    "\n",
    "])\n",
    "\n",
    "# Item network\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_outputs, activation='linear')\n",
    "  \n",
    "])\n",
    "\n",
    "# create the user input vector, point it to the user network, and normalize the output\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# create the item input vector, point it to the item network, and normalize the output\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d570842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
