{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f9a100",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "Train the learning algorithm with inputs and corresponding correct output labels\n",
    "\n",
    "#### Regression\n",
    "Predict a number out of infinitely many numbers\n",
    "\n",
    "#### Classification\n",
    "Predict a result out of a small number of possible categories "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e31c3e",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "Train the learning algorithm with inputs without output labels to predict a pattern in the data\n",
    "\n",
    "#### Clustering\n",
    "Take input data without labels and group them into clusters\n",
    "\n",
    "#### Anomaly detection\n",
    "Detect unusual data points\n",
    "\n",
    "#### Dimensionality reduction\n",
    "Compress large data set with small loss in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced85649",
   "metadata": {},
   "source": [
    "# Debugging a learning algorithm\n",
    "Diagnostics: tests that can tell what is working/not working for a learning algorithm\n",
    "\n",
    "## Evaluate a learning algorithm\n",
    "1. Split all data into training set, validation set, and testing set\n",
    "2. Determine weights and bias using the training set with regularization\n",
    "3. Use the weights and bias to calculate the cost of the training set and validation set and compare. For classification problems, the model can be evaluate by calculating the fraction of data the model has been missclassified\n",
    "3. Use the validation set cost to evaluate the performance of each model for fine tuning \n",
    "\n",
    "To find a better prediction model, we can train different models (eg. different polynomials or neural network structure), and pick the model with lowest validation set cost after fine tuning\n",
    "\n",
    "As the total number training examples becomes larger, the percentage for the validation and testing set can be smaller since they are only used to evaluate the model\n",
    "\n",
    "Only use the test set until a final model is confirmed to ensure the a relative accurate prediction on the model's precision\n",
    "\n",
    "Note: the validation and test set should come from the same distribution\n",
    "\n",
    "If the data from the training and validation set are not from the same distribution, a training-dev set, which contains data that has the same distribution as the training set, will be added for bias and variance evaluation\n",
    "\n",
    "\n",
    "## Bias and variance\n",
    "For data with lots of features, it is difficult to visualize if the model has high bias/variance. Thus, we can look at the the cost for training set and validation set to determine if the model overfits or underfits.\n",
    "* Overfitting (high variance): low cost for the training set, but significantly higher cost for the validation set\n",
    "* Underfitting (high bias): high cost for both the training and the validation sets\n",
    "* High bias and variance: high cost for the training set and even higher cost for the validation set\n",
    "* Just right: low cost for the training and slightly higher cost for the validation set\n",
    "\n",
    "As the degree of polynomial for fitting increases, the cost for the training set decreases and the cost for the validation set first decreases, then increases\n",
    "\n",
    "## Regularization\n",
    "When training a model with regularization, if $\\lambda$ is too large, the model will underfit (high cost for both the training and the validation sets), and if $\\lambda$ is too small, the model will overfit (low cost for the training set but significantly higher cost for the validation set)\n",
    "\n",
    "As $\\lambda$ increases, the cost for the training set increases (towards underfitting), and the cost for the validation set first decreases, then inncreases\n",
    "\n",
    "## Setting baseline performance\n",
    "To judge if the training performs well, we can set a baseline performance by seeing the performance of human or other similar algorithm on the data set. If the difference between the baseline performance and training error is high, the algorithm may have high bias (underfit); if the difference between the training error and the validation error is high, the algorithm may have high variance (overfit)\n",
    "\n",
    "## Learning curve\n",
    "In general, as the number of training example increases, the cost for the training set increases since it is difficult to fit all data perfectly with a model, and the cost for the validation set decreases since the model will make more accurate predictions to new data\n",
    "\n",
    "* High bias: the trainig and validation errors plateau as the number of training example increases since the model just cannot fit the data well, and increases the number of training example will not help\n",
    "\n",
    "<img src=\"https://assets-global.website-files.com/63f902d79a33f7ff016cde0b/63f902d89a33f72d236ce685_plot_bias_variance_trainingsize-1024x550.png\" width=500>\n",
    "\n",
    "* High variance: the training error will increase and the validation error will decrease as the number training example increases, so collecting more training data may help \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*wIaoQ-vXhW-Oxbf9.png\" width = 500>\n",
    "\n",
    "## Addressing high bias and variance\n",
    "### High bias\n",
    "* Use additional features\n",
    "* Use polynomial features\n",
    "* Decrease $\\lambda$\n",
    "\n",
    "### High variance\n",
    "* Get more training data\n",
    "* Use a smaller set of features\n",
    "* Increase $\\lambda$\n",
    "\n",
    "## Neural network\n",
    "\n",
    "### High bias\n",
    "* Use a larger neural network (can be computationally expensive)\n",
    "* Train the network with more iterations of gradient descend\n",
    "* Use a better neural network architecture\n",
    "\n",
    "A large neural network almost always fits the training set well and can always do as well or even better than a smaller network if the regularization is chosen correctly\n",
    "\n",
    "### High variance\n",
    "* Get more training data and retrain the model\n",
    "\n",
    "## Error analysis\n",
    "Manually examine the data that the model predicted wrong and categorize them based on the commons. Then, create solutions to based on categories (eg. getting more data for a specific category). In general, we start from fixing the reasons/categorizes that cause the most errors\n",
    "\n",
    "## Adding data\n",
    "Collecting data can be difficult and time consuming, so instead of adding data of everything, adding data related to the categories indicated by error analysis can lower the cost\n",
    "\n",
    "## Data mismatch\n",
    "When the data from the training set and validation set are not from the same ditribution, we evaluate the model based on the training-dev set\n",
    "\n",
    "* If the difference between the baseline performance and training set error is large, the model has high bias\n",
    "* If the difference between the training set error and training-dev set error is large, the model has high variance\n",
    "* If the difference between the training-dev set error and validation set error is large, the model is experiencing a data mismatch problem, which mean the model can perform well on data similar to the training set but not as well on data from a different distribution\n",
    "\n",
    "## Addressing data mismatch\n",
    "To address data mismatch, we can first perform error analysis to understand the difference between the training set and the validation set, then make the training set more similar to the validtion set by collecting more training data similar to the validation set through artificial data synthesis. When performing data synthesis, be caution about overfitting on a specific subset of all possible data\n",
    "\n",
    "* Data augmentation (mostly used for images and audio): modifying an existing training example to create new training examples (eg. rotating, mirroring images or adding noise to audio)\n",
    "* Data synthesis (mostly used for computer vision): creating brand new data\n",
    "\n",
    "## Transfer learning\n",
    "Transfer learning is used when the data set is not large enough to train a large neural network. Instead, we take another pretrained model that can complete similar tasks and fine tune the paramemter of the output layer or the entire network to obtain a better model. This method works because the network is able to pick up similar features with similar inputs during the pretraining process, which can be carried over to the fine tuning process.\n",
    "\n",
    "Steps:\n",
    "1. Supervised pretraing: train a network's parameters or obtain the parameters of an existing model trained on a large data set with the same input type as the actual data\n",
    "2. Fine tuning: further train the network with the acutal data\n",
    "\n",
    "## Multi-task learning\n",
    "Multi-task learning can predict multiple labels for each training data at once, which is performing multiple tasks at the same time. It it used when the labels shares lower level features \n",
    "\n",
    "## End to end deep learning\n",
    "End to end deep learning refers to train a single neural network for complex tasks using as input directly the raw input data without any manual feature extraction. ALternatively, we can accomplish the same task by building multiple neural network with each completing a smaller task in sequential order\n",
    "\n",
    "* End to end learning requires a lot more data than a sequential sub-task model and also limits the hand-design components\n",
    "\n",
    "\n",
    "## Skewed dataset\n",
    "The ratio of the positive and negative cases are signficantly skewed \n",
    "\n",
    "### Precision and recall\n",
    "* True positive: acutal class = 1, predicted class = 1\n",
    "* False positive: acutal class = 0, predicted class = 1\n",
    "* True negative: acutal class = 0, predicted class = 0\n",
    "* False negative: acutal class = 1, predicted class = 0\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pOtBHai4jFd-ujaNXPilRg.png\" width=500>\n",
    "\n",
    "This method helps to detect if an algorithm only predict one class all the time\n",
    "\n",
    "Precision measures the proportion of positive predictions that are actually correct, which useful when the cost of misclassifying a positive case is high. Recall measures the proportion of actual positive cases that are correctly identified by the model, which is useful when the cost of missing a positive case is high\n",
    "\n",
    "A model should aim for high precision and recall\n",
    "\n",
    "\n",
    "\n",
    "### Precision recall tradeoff\n",
    "The tradeoff between precision and recall occurs because increasing the threshold for classification will result in fewer false positives (increasing precision) but also more false negatives (decreasing recall), and vice versa\n",
    "\n",
    "<img src=\"https://jamesmccaffrey.wordpress.com/wp-content/uploads/2014/11/precisionrecallgraph.jpg\" width=500>\n",
    "\n",
    "In general, the threshold will be manaully picked or determined through F1 score\n",
    "\n",
    "### F1 score\n",
    "F1 score between 0 and 1 that indicates the precision and recall of a model. The higher the F1 score, the higher the precision and recall of the model\n",
    "\n",
    "$$F_1 = \\frac{2\\cdot precision \\cdot recall}{precision + recall}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f83802",
   "metadata": {},
   "source": [
    "# ML development cycle \n",
    "1. Scope the project: define the problem that the project will solve\n",
    "2. Collect data\n",
    "3. Development cycle:\n",
    "    1. Choose architecture (model, data, etc)\n",
    "    2. Train model\n",
    "    3. Run diagnostics (bias, variance, error analysis, etc)\n",
    "    4. Iterate from step A if the model does not work well enough\n",
    "    5. Collect more data if needed\n",
    "4. Deploy the model (implement the model on an inference server), monitoring and maintaining the system\n",
    "5. Improve the model with data collected during application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa8b2d",
   "metadata": {},
   "source": [
    "# Orthogonalization\n",
    "Orthogonalization is a system design property that ensures that modification of an instruction or an algorithm component does not create or propagate side effects to other system components. Orthogonalization makes it easier to independently verify the algorithms, thus reducing the time required for testing and development\n",
    "\n",
    "# Single number evaluation\n",
    "A single number metric that assesses the performance of a model, whic allows faster evaluation and comparison among algorithms\n",
    "\n",
    "When there are multiple metrics that are significant, we can use satisfying and optimizing metric to help us evaluate the models. In general, we have one optimizing metric and multiple satisfying metrics\n",
    "\n",
    "Satisfying metric: a metric that set the lower bound for an algorithm to be considered (e.g. minimum runtime)\n",
    "\n",
    "Optimizing metric: a metric that we want to let the algorithm optimize as much as possible (e.g. accuracy of predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
