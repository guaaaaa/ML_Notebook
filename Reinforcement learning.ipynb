{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67463e23",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "Reinforcement learning trains the algorithm to assess the current environment then perform the most optimal action through a rewarding system\n",
    "\n",
    "## Terminology\n",
    "\n",
    "Agent: the entity that can interact with the environment and perform actions to get rewards\n",
    "\n",
    "State ($s$): the current situation returned by the environment, which contains the information needed for the algorithm to assess in order to perform an action\n",
    "\n",
    "Action ($a$): the action taken by the agent after assessing the current state\n",
    "\n",
    "Reward ($R$): an immediate feedback given to an agent after it performs an action. The reward can be positive or negative based on the action taken and the result\n",
    "\n",
    "Discount factor ($\\gamma$): a number between 0 and 1 that balances the future reward value based on the number of actions required. The more actions required, the smaller the rewards and the smaller the penalties, so the algorithm will delay the penalty as much as possible. Larger $\\gamma$ means the algorithm is impatient and will look for short term reward, where smaller $\\gamma$ means the algorithm is patient and will look for long term reward\n",
    "\n",
    "Policy ($\\pi(s)$): a function that takes in the current state of the agent, $s$, and return an action, $a$ to perform\n",
    "\n",
    "Return: the sum of reward with discount (the first reward does not include a discount factor)\n",
    "$$Return = R_1 + \\gamma^2 R_2 + \\gamma^3 R_3 + ... + \\gamma^n R_n,$$\n",
    "where $R_i$ represents the reward after $i$th action is taken with $n$ total actions\n",
    "\n",
    "<img src=\"https://www.scribbr.com/wp-content/uploads/2023/08/the-general-framework-of-reinforcement-learning.webp\" width=500>\n",
    "\n",
    "## Markov Decision Process (MDP)\n",
    "The future action decision will only depend on the current state of the agent but not the previous states\n",
    "\n",
    "\n",
    "## State-action value function ($Q$)\n",
    "The state-action value fuction ($Q(s, a)$)\n",
    "1. Takes in the current state and set of all possible actions\n",
    "2. Take a random action among all possibilities\n",
    "3. Returns the return as if the agent behave optimally after the random action is taken\n",
    "\n",
    "* The best possible return from state $s$ is the max possible value of $Q(s, a)$\n",
    "* The best possible action in state $s$ is the action $a$ that gives $\\text{max} Q(s, a)$, which will maximize the return\n",
    "\n",
    "## Bellman equation\n",
    "$$Q(s,a) = R(s) + \\gamma max_{a'}Q(s', a')$$\n",
    "\n",
    "$s$: current state\n",
    "\n",
    "$a$: current action\n",
    "\n",
    "$s'$: state after action $a$ is performed\n",
    "\n",
    "$a'$: action that will be taken in state $s'$\n",
    "\n",
    "$R(s)$: reward of the current state\n",
    "\n",
    "The Bellman equation shows that the optimal return for a state equals the reward from the current state plus the discount factor times the optimal return from the next state, which is a recursive implementation\n",
    "\n",
    "## Stochastic environment\n",
    "Stochastic environment is random in nature so the agent has a probability to fail to perform the action as expected, so the algorithm will perform the policy multiple times and maximize the expected return (average return) based on different reward sequence\n",
    "\n",
    "$$\\text {Expected return} = Average(R_1 + \\gamma^2 R_2 + \\gamma^3 R_3 + ... + \\gamma^n R_n) = E[R_1 + \\gamma^2 R_2 + \\gamma^3 R_3 + ... + \\gamma^n R_n]$$\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma E[max_{a'}Q(s', a')]$$\n",
    "\n",
    "The policy will selected an action that maximize the expected return in state $s$\n",
    "\n",
    "\n",
    "## Continuous states space\n",
    "In a continuous states space, the state of the agent, $s$, will be a vector that contains all the information needed\n",
    "\n",
    "## Deep reinforcement learning\n",
    "\n",
    "1. Randomly initalize the neural network with a guess on $Q(s,a)$\n",
    "2. Repeat {\n",
    "    * Generate a training set\n",
    "        1. From state $s$, perform a random action $a$ that results in state $s'$. Then, construct a tuple $(s, a, R(s), s')$\n",
    "        2. Create and store sufficient amount of tuple (replay buffer) for training\n",
    "        Calculate $Q(y = R(s) + \\gamma max_{a'} (s',a')$ by using the guessed function give by the neural network\n",
    "    * Train the neural netowrk\n",
    "        1. Create training set with the tuples, where $x=(s,a)$ and $y = R(s) + \\gamma max_{a'} Q(s',a')$\n",
    "        2. Using the training set to train a network $Q_{new}$ such that $Q_{new}(s,a)\\approx y$        \n",
    "    * Set $Q = Q_{new}$\n",
    "    \n",
    "   }\n",
    " \n",
    " With each interation, the neural network can get a better approximation of the $Q$ function\n",
    "   \n",
    "The neural network takes in a state, action pair and approximates the return a list $Q(s,a)$ values that contains the return for each action performed, so the algorithm can pick the action that maximize $Q(s,a)$ in the list\n",
    "\n",
    "## $\\epsilon$ greedy policy\n",
    "Greedy (exploitation) step: with a high probability ($p=1-\\epsilon$), pick the action that maximizes $Q(s,a)$ based on the current $Q$ function\n",
    "Exploration step: with a low probability ($p=\\epsilon$), pick a random action. This allows the algorithm to explore  action space even some actions do not maximize the return\n",
    "\n",
    "Start with a high $\\epsilon$ value and decrease it gradually as the approximation for $Q$ becomes better\n",
    "\n",
    "\n",
    "## Mini batch\n",
    "When the training set is large, the algorithm only takes a subset of the training example to compute the cost and a different subset next time. Thus, the algorithm will not take the most optimal step each time, but it will speed up the training process significantly \n",
    "\n",
    "## Soft update\n",
    "When updating $Q = Q_{new}$, the new network may be worse than the old one. To prevent full overwritting, we update the weigths and bias through $w = p \\times w + (1-p) \\times w_{new}$ and $b = p \\times b + (1-p) \\times b_{new}$ instead of $w = w_{new}$  and $b = b_{new}$, where $b$ is a value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f489848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
