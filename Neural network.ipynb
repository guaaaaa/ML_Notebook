{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76d41f4",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Neurons\n",
    "Neurons are the fundamental building block of neural network. A neuron takes in a set of input, a set of weights, and an activation function (eg. linear/logistic regression) and produce a set of output.\n",
    "\n",
    "The input of a neuron will be a vector that contains all features\n",
    "\n",
    "The output of a neuron can be the input of another neuron\n",
    "\n",
    "## Layers\n",
    "A layer consists one or more neuron(s) that does not interact with each other.\n",
    "\n",
    "* Input layer (Layer 0): first layer where the data is first introduced to the network\n",
    "* Hidden layer(s): layers between the input and output layer\n",
    "* Output layer: the final layer of the neural network that produces a final result\n",
    "\n",
    "Each layer is performing \"feature engineering\" to create some new features based on the input that allows the model to make better predictions. This process is automatic.\n",
    "\n",
    "## Activation function\n",
    "The activation function normalizes the weights, it can be a linear regression model, logistic regression model, or other functions.\n",
    "\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:1358/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png\" width = 500>\n",
    "\n",
    "## Notations\n",
    "$n^{[l]}$: number of neurons in the $l$th layer\n",
    "\n",
    "$\\vec w^{[l]}_j$, $b^{[l]}_j$: the weights and bias of the $j$th neuron in $l$th layer\n",
    "\n",
    "$\\vec a^{[l]}$: the output vector for $l$th layer and input for $l + 1$th layer\n",
    "\n",
    "$\\vec a^{[0]}$: the input vector\n",
    "\n",
    "$g$: the activation function\n",
    "\n",
    "\n",
    "$a^{[l]}$: a column vector that has number of rows equals to the number of neurons in the current layer\n",
    "\n",
    "General equation for output of $j$th neuron in $l$th layer: $\\vec a^{[l]}_j = g(\\vec w^{[l]}_j \\cdot \\vec a^{[l-1]}_j + b^{[l]}_j)$\n",
    "\n",
    "$$ a^{[l]} = \\begin{bmatrix} a^{[l]}_1 \\\\ a^{[l]}_2 \\\\ \\vdots \\\\ a^{[l]}_n \\end{bmatrix}$$\n",
    "\n",
    "# Forward propagation\n",
    "The neural network model where data moves from left (input layer) to right (output layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4a57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid, linear, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea33e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow convension: must use matrix not 1D array\n",
    "x = np.array([[200, 17]]) # a 1x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5290cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense layer (there are other types of layers)\n",
    "layer_1 = Dense(units = 3, activation='sigmoid')\n",
    "# units: number of nuerons in this layer\n",
    "# activation: the activation function used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a9da58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the weights of a layer\n",
    "layer_1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d69dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Output after the first layer\n",
    "a1 = layer_1(x)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4aab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network (a series of layers)\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(1,)), # Input shape\n",
    "    Dense(units = 1, activation = 'sigmoid', name = 'layer1'), # Layer1\n",
    "    Dense(units = 2, activation = 'sigmoid', name = 'layer2')  # Layer2\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775bd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 1)                 2         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 2)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model.build()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ba1105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.]], dtype=float32), array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get layers\n",
    "layer1 = model.get_layer('layer1')\n",
    "\n",
    "# Set weights \n",
    "w = np.array([[0]])\n",
    "b = np.array([0])\n",
    "layer1.set_weights([w,b])\n",
    "\n",
    "# Get weights\n",
    "layer1.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88681ec",
   "metadata": {},
   "source": [
    "# Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de80f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create data\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def load_coffee_data():\n",
    "    \"\"\" Creates a coffee roasting data set.\n",
    "        roasting duration: 12-15 minutes is best\n",
    "        temperature range: 175-260C is best\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(2)\n",
    "    X = rng.random(400).reshape(-1,2)\n",
    "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
    "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
    "    Y = np.zeros(len(X))\n",
    "    \n",
    "    i=0\n",
    "    for t,d in X:\n",
    "        y = -3/(260-175)*t + 21\n",
    "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0\n",
    "        i += 1\n",
    "\n",
    "    return (X, Y.reshape(-1,1))\n",
    "\n",
    "X, Y = load_coffee_data()\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7714278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 03:53:53.055098: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868cd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500) # prevent overflow\n",
    "    g = 1.0 / (1.0 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62b2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer function\n",
    "def dense(a_in, W, b):\n",
    "    units = W.shape[1] # get the number of neurons\n",
    "    a_out = np.zeros(units) # create output array\n",
    "    \n",
    "    # loop through the array\n",
    "    for j in range(units):\n",
    "        w_j = W[:,j] # get weights for jth neuron\n",
    "        b_j = b[j] # get bias for jth neuron\n",
    "        z = np.dot(w_j, a_in) + b[j]\n",
    "        a_j = sigmoid(z) # get jth value using the sigmoid function\n",
    "        # print(a_j)\n",
    "        a_out[j] = a_j\n",
    "    #print(a_out)\n",
    "    return (a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47fa5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of layers\n",
    "def network(x, W1, b1, W2, b2):\n",
    "    a1 = dense(x, W1, b1)\n",
    "    a2 = dense(a1, W2, b2)\n",
    "    \n",
    "    return (a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c10e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights and bias\n",
    "W1_tmp = np.array( [[-8.93,  0.29, 12.9 ],   # the weights of jst neuron is the jth column of the matrix\n",
    "                    [-0.1,  -7.32, 10.81]] )\n",
    "\n",
    "b1_tmp = np.array( [-9.82, -9.28,  0.96] )\n",
    "\n",
    "W2_tmp = np.array( [[-31.18],\n",
    "                    [-27.59],\n",
    "                    [-32.56]] )\n",
    "\n",
    "b2_tmp = np.array( [15.41] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae774a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def model(X, W1, b1, W2, b2):\n",
    "    m = X.shape[0] # get the number of training examples\n",
    "    p = np.zeros((m,1)) # create an output for probability\n",
    "    for i in range(m):\n",
    "        p[i,0] = network(X[i], W1, b1, W2, b2)\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfd7c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.72e-01]\n",
      " [3.29e-08]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/s2xlp24j2w5fg_6tp3y9g33h0000gq/T/ipykernel_17533/4063415801.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  p[i,0] = network(X[i], W1, b1, W2, b2)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_tst = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17]])   # negative example\n",
    "X_tstn = norm_l(X_tst)  # remember to normalize\n",
    "predictions = model(X_tstn, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30901524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[2]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a9475",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Vectorization allows us to compute the prediction for all training examples at the same time, which is a more efficent method for implementation\n",
    "\n",
    "$X$: a matrix containing all the training examples with number of rows equals to the number of features for the training examples and number of columns equals to the number of training examples. The $i$th column of $X$ represents the $i$th training example\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "(\\mathbf{x}^{(0)}) & (\\mathbf{x}^{(1)}) & \\cdots & (\\mathbf{x}^{(m)}) \\\\\n",
    "| & | &  & | \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$W^{[l]}$: a matrix containing all the weights for the $l$th layer, where the number of rows equals the number of output features (number of neurons in the current layer) and the number of columns equals the number of input features (number of neurons in the previous layer), where $\\mathbf{w}^{[l](j)}$ is a column vector containing the weights for the $j$th neuron in the $l$th layer\n",
    "\n",
    "$$\n",
    "\\mathbf{W^{[l]}} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\mathbf{w}^{[l](1)})^T --- \\\\\n",
    "--- (\\mathbf{w}^{[l](2)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\mathbf{w}^{([l](n))})^T --- \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$\\mathbf b^{[l]}$: a column vector containing all the bias for the $l$th layer, where $\\mathbf b^{[l](j)}$ is the bias for the $j$th neuron in the $l$th layer\n",
    "\n",
    "$$\n",
    "\\mathbf{ b^{[l]}} = \n",
    "\\begin{bmatrix}\n",
    " b^{[l](1)}  \\\\\n",
    " b^{[l](2)} \\\\\n",
    "\\vdots \\\\\n",
    "b^{[l](n)} \\\\\n",
    "\\end{bmatrix}\\quad\n",
    "$$\n",
    "\n",
    "$g^{[l]}()$: the activation function for the $l$th layer\n",
    "\n",
    "$A^{[l]}$: a matrix containing all the output from the $l$th layer for all training examples with number of rows equals to the number of neurons in the current layer and columns equals to the number of training examples. The $i$th column of $A^{[l]}$ represents the output vector from the $l$th layer of the $i$th training example\n",
    "\n",
    "$$\\mathbf{A^{[l]}} = \n",
    "\\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "(\\mathbf{A}^{[l](1)}) & (\\mathbf{A}^{[l](2)}) & \\cdots & (\\mathbf{A}^{[l](m)}) \\\\\n",
    "| & | &  & | \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Note:\n",
    "* $A^{[0]} = X$ the output for the 0th layer is the matrix of training examples\n",
    "* $A^{[L]}$ is the output of the final layer, which is the final prediction \n",
    "\n",
    "## Vectorized computation\n",
    "$$A^{[l]} = g^{[l]}(W^{[l]}A^{[l-1]} + b^{[l]})$$\n",
    "\n",
    "Repeatedly calculate the output of the next layer using the output from the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "502cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized implementation\n",
    "def dense_vec(a_in, W, b):\n",
    "    z = np.dot(W, a_in) + b # Matrix multiplication\n",
    "    a_out = sigmoid(z)\n",
    "    \n",
    "    return (a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ac8a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55 0.56 0.57 0.57]\n",
      " [0.61 0.62 0.64 0.65]\n",
      " [0.66 0.68 0.7  0.73]]\n"
     ]
    }
   ],
   "source": [
    "X_tst = 0.1*np.arange(1,9,1).reshape(2,4) # (4 examples, 2 features)\n",
    "W_tst = 0.1*np.arange(1,7,1).reshape(3,2) # (2 input features, 3 output features)\n",
    "b_tst = 0.1*np.arange(1,4,1).reshape(3,1)\n",
    "\n",
    "print(dense_vec(X_tst, W_tst, b_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615efcb6",
   "metadata": {},
   "source": [
    "# Neural netowork training\n",
    "Steps:\n",
    "1. Define the models and sequentical layers\n",
    "2. Compile the model and determine which loss function to use\n",
    "3. Call a function and minimize the cost using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3e48b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n",
      "(25, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create training example\n",
    "x = np.arange(1,26)\n",
    "X = np.array([x]).T\n",
    "y = np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,]]).T\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b962628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model1 = Sequential([\n",
    "    Dense(units=25, activation='sigmoid'),\n",
    "    Dense(units=15, activation='sigmoid'),\n",
    "    Dense(units=1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99fb0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "# Binary crossentropy function is the same as the loss function for logistic regression with exactly 2 classes\n",
    "model1.compile(loss = BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089ccc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.5831\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5708\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5632\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5575\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5529\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5491\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5458\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5429\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5403\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5380\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5359\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5340\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5322\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5306\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5291\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5278\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5265\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5253\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5242\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5231\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5221\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5212\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5203\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5195\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5187\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5179\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5172\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5165\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5158\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5152\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5145\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5139\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5133\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5127\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5121\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5115\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5109\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5103\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5097\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5092\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5086\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5080\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5074\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5068\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5062\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5057\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5051\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5045\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5039\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5033\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5027\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5021\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5015\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5008\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5002\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4996\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4990\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4984\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4977\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4971\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4964\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4958\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4951\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4945\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4938\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4932\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4925\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4919\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4912\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4905\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4899\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4892\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4885\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4878\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4871\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4864\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4858\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 955us/step - loss: 0.4851\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4844\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4837\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4830\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4823\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4816\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4808\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4801\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4794\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4787\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4780\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4773\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4765\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4758\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4751\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4744\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4736\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4729\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4722\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4714\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4707\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4699\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1549c75b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimize the cost\n",
    "# X, y are training examples\n",
    "# Epochs is the number of iteration for gradient descent \n",
    "model1.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d246ad7",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Issue: Sigmoid function predict things in binary manner (0 or 1), but sometimes, things are not limited in binary form\n",
    "\n",
    "## Linear (No activation function)\n",
    "$$g(z) = z = \\vec w \\cdot \\vec x + b$$\n",
    "Since $g(z) = z$, this is equivalent to no activation function\n",
    "\n",
    "## Sigmoid\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Derivatives\n",
    "$$\\frac{\\partial g(z)}{\\partial z} = \\frac{1}{1 + e^{-z}}  (1 - \\frac{1}{1 + e^{-z}}) = g(z) (1-g(z))$$\n",
    "\n",
    "## Tanh\n",
    "$$g(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "<img src=\"https://vidyasheela.com/web-contents/img/post_img/39/tanh%20activation%20function-new.png\" width=500>\n",
    "\n",
    "Derivatives\n",
    "$$\\frac{\\partial g(z)}{\\partial z} = 1 - (tanh(z))^2 = 1 - g(z)^2$$\n",
    "\n",
    "## ReLU\n",
    "$$g(z) =  \\begin{cases}\n",
    "0 & \\text{if $z<0$}\\\\\n",
    "max(0, z) & \\text{if $z \\geq 0$}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DfMRHwxY1gyyDmrIAd-gjQ.png\" width=500>\n",
    "\n",
    "ReLU has the ability to deactivate a function when $z < 0$, so we can use ReLU to build more complex, piecewise functions for decision boundaries\n",
    "\n",
    "Derivatives\n",
    "$$\\frac{\\partial g(z)}{\\partial z} = \\begin{cases}\n",
    "0 & \\text{if $z<0$}\\\\\n",
    "1 & \\text{if $z \\geq 0$}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Leaky ReLU\n",
    "$$g(z) =  \\begin{cases}\n",
    "az & \\text{if $z<0$}\\\\\n",
    "max(0, z) & \\text{if $z \\geq 0$}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "where $a$ is a very small positive number\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_3.09.45_PM.png\" width=400>\n",
    "\n",
    "Derivatives\n",
    "$$\\frac{\\partial g(z)}{\\partial z} = \\begin{cases}\n",
    "a & \\text{if $z<0$}\\\\\n",
    "1 & \\text{if $z \\geq 0$}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Select activation functions\n",
    "For the output layer:\n",
    "* For binary classification, use sigmoid\n",
    "* For regression (when the result can be negative value), use linear activation function\n",
    "* For regression (when the result cannot be negative), use ReLU\n",
    "\n",
    "For the hidden layers:\n",
    "* ReLU is the most commonly used since ReLU is faster to train\n",
    "* Tanh is almost always better than sigmoid since it allows the data to center at 0\n",
    "* Leaky ReLU takes longer to train than ReLU\n",
    "\n",
    "# Random initialization\n",
    "When initializing $W$, if all the weights are intialized to 0, the neurons in the same layer will have the same weights after training. Therefore, we intialize all the weights to random values close to 0, so each neuron will end up with learning differently\n",
    "\n",
    "## Vanishing/Exploding gradient\n",
    "When a neural netowrk is very deep, the coefficient for the function of the output can grow or diminish exponentially. This can cause gradient descent to have a very larger or small slope to each iteration, which is not ideal\n",
    "\n",
    "As a partial solution, we can set the the variance of each weight matrix $W^{[l]}$ equals to one when performing random initialization to reduce the speed of vanishing or exploding gradient. This method is called Xavier initialization\n",
    "\n",
    "# Mean normalization\n",
    "Normalizing input data ensures the all the feature values are in similar ranges, so the gradient descent can run faster\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:992/format:webp/1*DK6tNx7Ke_27-CdLT3_1Ug.png\">\n",
    "\n",
    "Ensures the validation ans test set are included during the normalization so the mean and variance used is the same as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa32a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "layer2 = Dense(units=10, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d196885",
   "metadata": {},
   "source": [
    "# Vectorized cost function for binary classification\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "J &= f(A^{[L]}, Y) \\\\\n",
    "&= -\\frac{1}{m} \\sum_i (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)) \\\\\n",
    "&= -\\frac{1}{m} \\sum_i (y_i \\log(a_i^{[L]}) + (1 - y_i) \\log(1 - a_i^{[L]})),\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation}\n",
    " = -\\frac{1}{m} \\underbrace{\\sum_{\\text{axis} = 1} (Y \\odot \\log(A^{[L]}) + (1 - Y) \\odot \\log(1 - A^{[L]}))}_\\text{scalar}.\n",
    "\\end{equation}\n",
    "\n",
    "$Y$: a column vector that contains the true lable for each training example (0 or 1)\n",
    "\n",
    "$A^{[L]}$: predicted value for each training example by the model\n",
    "\n",
    "# Regularized cost function\n",
    "$$J = - \\frac{1}{m} \\sum_{axis = 1} (Y \\odot \\log(A^{[L]}) + (1 - Y) \\odot \\log(1 - A^{[L]})) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} ||W^{[l]}||^2$$\n",
    "\n",
    "$\\lambda$: regularization parameter\n",
    "\n",
    "$||W^{[l]}||^2$: Forbenius norms for the weights of the $l$th layer\n",
    "\n",
    "# Dropout regularization\n",
    "During the training process, the neurons in each layer will have a probability of being dropped, so its weights and bias will not be updated. The output of each layer will then be divided by that probability to ensure the correct scaling. During back propagation, the same set of neurons will be dropped. After each iteration of gradient descent, another random set of neurons will be dropped\n",
    "\n",
    "This can be done by creating a matrix $D^{[l]}$ that has the same size as the output from the $l$th layer, $A^{[l]}$, where all entries are 0 or 1 based on the probability, where $\\frac {D^{[l]} * A^{[l]}} {Probability}$ is the output of the $l$th layer after dropout\n",
    "\n",
    "Dropout regularization forces the algorithm to spread out its weights instead of relying on some features heavily, which decreases the overall cost\n",
    "\n",
    "\n",
    "Dropout is only applied during training but not during validation or testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17690c3a",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "Classification problems that have more than two classes, meaning the output($y$) can have more than two possible values\n",
    "\n",
    "## Softmax\n",
    "Softmax activation function is only used as the final layer of a neural network\n",
    "\n",
    "For a classification problem that has $n$ different classes,\n",
    "$$\n",
    "\\begin{cases}\n",
    "z_1 = \\vec w_1 \\cdot \\vec x + b_1\\\\\n",
    "z_2 = \\vec w_2 \\cdot \\vec x + b_2\\\\\n",
    "...\\\\\n",
    "z_n = \\vec w_n \\cdot \\vec x + b_n\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "For weights $\\vec w_1, \\vec w_2, ..., \\vec w_n$ and bias $b_1, b_2, ..., b_n$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "a_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + ... + e^{z_n}}\\\\\n",
    "a_2 = \\frac{e^{z_2}}{e^{z_1} + e^{z_2} + ... + e^{z_n}}\\\\\n",
    "...\\\\\n",
    "a_n = \\frac{e^{z_n}}{e^{z_1} + e^{z_2} + ... + e^{z_n}}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "Thus,\n",
    "$$a_1 + a_2 + ... + a_n = 1$$\n",
    "where $a_j$ is the probability of the given data to be the $j$th class\n",
    "\n",
    "Summary:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{n}{e^{z_k} }} $$\n",
    "\n",
    "The output $\\mathbf{a}$ is a vector of length $n$ that contains the probability of the input being each class\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{n}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{n}} \\\\\n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "## Cost function for softmax\n",
    "Loss function:\n",
    "\\begin{equation}\n",
    "  L(\\vec{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_n), & \\text{if $y=n$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$L$ calculates the loss for each training example\n",
    " \n",
    "For the loss function, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Cost:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} (\\sum_{j=1}^{n}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^n e^{z^{(i)}_k} })\\right]\n",
    "\\end{align}\n",
    "\n",
    "$\\sum_{j=1}^{n}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^n e^{z^{(i)}_k} }$: the loss of $a_j$ term\n",
    "\n",
    "Where $m$ is the number of examples, $n$ is the number of outputs. This is the average of all the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ead07e",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "815470b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(z):     # Input z is an array of values\n",
    "    ez = np.exp(z)\n",
    "    sm = ez / np.sum(ez)\n",
    "    return (sm)\n",
    "# sm is an array that has the size equals to the total number of class containing the probability of the given data being each class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fc3561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "# Create data set\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc597a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 336us/step - loss: 1.0351\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 292us/step - loss: 0.5603\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 295us/step - loss: 0.2770\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 289us/step - loss: 0.1289\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 290us/step - loss: 0.0810\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 297us/step - loss: 0.0631\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 297us/step - loss: 0.0534\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 301us/step - loss: 0.0473\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 295us/step - loss: 0.0429\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.0395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x142088bb0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow implementation that prevents roundoff errors\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=4, activation='linear') # The last layer produce a set of z values, which must be converted to probability using softmax\n",
    "])\n",
    "\n",
    "# SparseCategoricalCrossentropy is the loss function for multiclass classification\n",
    "# from_logits informs the loss function that is operation is for a softmax implementation\n",
    "# optimizer - Adam algorithm (see below)\n",
    "model2.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = tf.keras.optimizers.Adam(0.001),)\n",
    "\n",
    "\n",
    "model2.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d35a52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 234us/step\n",
      "[0.   0.01 0.97 0.02] 2\n",
      "[9.97e-01 2.55e-03 4.03e-05 2.45e-06] 0\n",
      "[9.75e-01 2.43e-02 5.66e-04 6.04e-05] 0\n",
      "[0.01 0.99 0.   0.  ] 1\n",
      "[3.90e-03 2.77e-04 9.96e-01 1.18e-04] 2\n",
      "[6.83e-04 1.26e-03 9.95e-01 3.29e-03] 2\n",
      "[0.   0.99 0.   0.  ] 1\n",
      "[1.00e+00 1.89e-05 2.85e-06 7.95e-08] 0\n",
      "[4.26e-03 9.93e-01 1.59e-03 7.49e-04] 1\n",
      "[9.77e-05 2.27e-04 2.12e-03 9.98e-01] 3\n"
     ]
    }
   ],
   "source": [
    "z_value = model2.predict(X_train) # The model predict a matrix of z values\n",
    "prob = tf.nn.softmax(z_value).numpy() # Convert z values to probability using softmax function\n",
    "\n",
    "for i in range(10):\n",
    "    print(prob[i], np.argmax(prob[i])) # Print probability (each row adds up to 1) and predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4e0e3",
   "metadata": {},
   "source": [
    "# Multilabel classification\n",
    "Contains multiple lables in a single input\n",
    "\n",
    "# Additional layer type\n",
    "## Convolutional layer\n",
    "Convolutional layer only allows each neuron to get part of the complete input, which can\n",
    "* Speed up computation\n",
    "* Require less training data\n",
    "* Reduce overfitting\n",
    "\n",
    "# Back propagation\n",
    "Back propagation trains a neural network effectively using chain rule. It determines how the weights and bias should change to better fit a single training example to lower the cost function by calculating the gradient. Back propagation should be performed for each training example to find the most suitable set of parameters for the model.\n",
    "\n",
    "Back propagation computes the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule. This allows the calculation of derivatives to be faster and more efficient\n",
    "\n",
    "Since a weight $W$ only effect the cost through its effect on the next layer, we only need the derivatives of later layers to determine the effect of $W$ on the cost, then the previous layer can be computed and repeated recursively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea2784",
   "metadata": {},
   "source": [
    "# Advanced optimization\n",
    "\n",
    "## Mini-batch gradient descent (Stochastic gradient descent)\n",
    "Mini-batch allows the training to be faster by dividing the training set into multiple mini-batches, where each min-batch contains some training examples\n",
    "\n",
    "For each iteration of gradient descent, the training is only performed on one mini-batch instead of the entire training set. After each iteration, we move on to the next mini-batch\n",
    "\n",
    "In mini-batch gradient descent, each iteration does not necessary reduce the cost, but the general trend should be a decreasing cost function.\n",
    "\n",
    "### Notations\n",
    "\n",
    "$X$: the matrix of all training examples\n",
    "\n",
    "$Y$: the matrix for all labels of $X$\n",
    "\n",
    "$X^{\\{t\\}}$: a $n$ by $k$ matrix for the $t$th mini-batch, where $n$ is the number of features for each training example and $k$ is the number of training examples in this mini-batch\n",
    "\n",
    "$Y^{\\{t\\}}$: a $1$ by $k$ matrix for the labels of the $t$th mini-batch\n",
    "\n",
    "### Choosing mini-batch size\n",
    "In general, the smaller the batch size, the faster the training speed, but the cost function will be noisier\n",
    "\n",
    "If the training set is small, use batch gradient descent. Otherwise, set the size of mini-batch as the power of two (typically 64, 128, 256, 512 examples per mini-batch)\n",
    "\n",
    "Note: the training examples and their labels should be randomly shuffled before splitting into mini-batches\n",
    "\n",
    "\n",
    "## Exponetially weighted average\n",
    "Exponentially weighted average calculates the average of data, where older values gives less weights to the current average\n",
    "\n",
    "$$v_{t} = \\beta v_{t-1} + (1 - \\beta) \\theta_{t}$$\n",
    "$$v_0 = 0$$\n",
    "\n",
    "$t$: number of iterations\n",
    "\n",
    "$\\beta$: the weight parameter (between 0 and 1) that determines how important is the current value, $\\theta_{t}$, to the overall avreage. Larger $\\beta$ means current value is less significant\n",
    "\n",
    "$\\theta_{t}$: the current value\n",
    "\n",
    "### Bias correction\n",
    "When initialization the value for $v_0 = 0$, the first few calculation may result in inaccurate predictions since the value is multiplied by $\\beta$, and the following formula can improve this \n",
    "$$v_{t} = \\frac{\\beta v_{t-1} + (1 - \\beta) \\theta_{t}}{{1-\\beta^t}}$$\n",
    "\n",
    "## Momentum\n",
    "The momentum algorithm reduces the oscillations in gradient descent by using the eponetially weighted average\n",
    "\n",
    "To do this, we first calculate $dw$ and $db$. Then, apply the eponetially weighted average formula, where\n",
    "$$v_{dW} = \\beta v_{dW} + (1 - \\beta) dW$$\n",
    "$$v_{db} = \\beta v_{db} + (1 - \\beta) db$$\n",
    "\n",
    "After, updates the weights and bias\n",
    "$$W = W - \\alpha v_{dW}$$\n",
    "$$b = b - \\alpha v_{db}$$\n",
    "\n",
    "## RMSprop\n",
    "Root mean square prop is another algorithm that reduces the oscillations in gradient descent and speed up the learning\n",
    "\n",
    "To do this, we first calculate $dw$ and $db$. Then, apply the eponetially weighted average formula, where\n",
    "$$s_{dW} = \\beta_2 s_{dW} + (1 - \\beta_2) dW^2$$\n",
    "$$s_{db} = \\beta_2 s_{db} + (1 - \\beta_2) db^2$$\n",
    "\n",
    "After, updates the weights and bias\n",
    "$$W = W - \\alpha \\frac{dW}{\\sqrt{(s_{dW})} + \\epsilon}$$\n",
    "$$b = b - \\alpha \\frac{db}{\\sqrt{(s_{db})} + \\epsilon}$$\n",
    "\n",
    "$\\epsilon$: a very small number to prevent division by 0\n",
    "\n",
    "\n",
    "## Adam algorithm\n",
    "Adam algorithm is a combination of Momentum and RMSprop algorithm for better performance\n",
    "\n",
    "To do this, we first calculate $dw$ and $db$. Then, apply the eponetially weighted average formula, where\n",
    "$$v_{dW} = \\frac {\\beta_1 v_{dW} + (1 - \\beta) dW} {{1-(\\beta_1)^t}}$$\n",
    "\n",
    "$$v_{db} = \\frac {\\beta_1 v_{db} + (1 - \\beta) db} {{1-(\\beta_1)^t}}$$\n",
    "\n",
    "$$s_{dW} = \\frac {\\beta_2 s_{dW} + (1 - \\beta_2) dW^2} {1-(\\beta_2)^t}$$\n",
    "\n",
    "$$s_{db} = \\frac {\\beta_2 s_{db} + (1 - \\beta_2) db^2} {1-(\\beta_2)^t}$$\n",
    "\n",
    "After, updates the weights and bias\n",
    "$$W = W - \\alpha \\frac{v_{dW}}{\\sqrt{(s_{dW})} + \\epsilon}$$\n",
    "$$b = b - \\alpha \\frac{v_{db}}{\\sqrt{(s_{db})} + \\epsilon}$$\n",
    "\n",
    "$\\alpha$: learning rate\n",
    "\n",
    "$\\beta_1, \\beta_2$: two weight parameters\n",
    "\n",
    "$t$: number of iterations\n",
    "\n",
    "$\\epsilon$: a very small number to prevent division by 0\n",
    "\n",
    "In tensorflow, the adam algorithm can automatically select an appropriate learning rate $\\alpha$ for each step taken when updating the weights and bias. When the steps are taken in the same direction, the algorithm will use a larger learning rate to speed up the learnin; when the stpes taken are oscillating, the algorithm will use a smaller learning rate to improve the accuracy\n",
    "\n",
    "\n",
    "## Learning rate decay\n",
    "Learning rate decay algorithm reduces the oscillation by using smaller learning rate as the training proceed, which can speed up the training\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + \\text{Decay Rate} \\times \\text{Epoch Number}}$$\n",
    "\n",
    "$\\alpha$: current learning rate\n",
    "\n",
    "$\\alpha_0$: initial learning rate\n",
    "\n",
    "$\\text{Dacay rate}$: a hyperparameter\n",
    "\n",
    "$\\text{Epoch Number}$: the number of iterations through the entire training set. After going through all the training examples, epoch number will increase by 1 (if mini-batch is used, epoch number will increase when all mini-batches are used once)\n",
    "\n",
    "Note: there are other leanring rate decay algorithm, but the general purpose is the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea8612",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "When trying out different values of hyperparameters, use random sampling (random values for hyperparameters) instead of a grid search (combining some set values) to better explore the space of hyperparameters\n",
    "\n",
    "After finding a region of values works well for the model, we can zoom in to that region and perform random sampling more densely\n",
    "\n",
    "## Hyperparameter scaling\n",
    "When the hyperparameter can be an exponential ranged values (e.g. learning rate), instead of randomize the hyperparameters on a linear scale, we can randomize the values on a log scale to better explore the hyperparameter space \n",
    "\n",
    "# Batch normalization\n",
    "Batch normalization increases the training speed by performing mean normalization on each hidden layer of the neural netowrk. Also, since each hidden layer is normalized, the change in input data will have smaller effect as the network goes deeper, which allows it to provide more stable performances even when the distribution of the input data changes\n",
    "\n",
    "When performing batch normalization with mini-batches, each mini-batch is scaled by its own mean and variance instead of those of the entire training set. This adds some noise to the prediction, which has a slight regularization effect to prevent overfitting. However, it is not a substitute for regularization algorithm\n",
    "\n",
    "## Implementation\n",
    "For some intermdiate values $Z^{[l]}$ in $l$th layer of a neural network, where\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$Z^{[l]}$: a matrix with the number of rows equals the number of nuerons in the current layer and number of columns equals the number of training examples in the mini-batch\n",
    "\n",
    "$$\\mathbf{Z^{[l]}} = \n",
    "\\begin{bmatrix}\n",
    "| & | &  & |\\\\\n",
    "(\\mathbf{z}^{(1)}) & (\\mathbf{x}^{(2)}) & \\cdots & (\\mathbf{z}^{(m)}) \\\\\n",
    "| & | &  & | \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m} z^{(i)}$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (z^{(i)} - \\mu)^2$$\n",
    "\n",
    "$$z^{(i)}_{\\text{norm}} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "$$\\tilde{z}^{(i)} = \\gamma^{[l]} z^{(i)}_{\\text{norm}} + \\beta^{[l]}$$\n",
    "\n",
    "$z^{[l](i)}_{\\text{norm}}$: normalized data with mean of 0 and variance of 1\n",
    "\n",
    "$\\gamma^{[l]}, \\beta^{[l]}$: column vectors with the number of entires equals to the number of neurons in the $l$th layer. $\\gamma^{[l]}$ and $\\beta^{[l]}$ are learnable parameters from gradient descent that controls the mean and variance of the data\n",
    "\n",
    "$\\tilde{z}^{[l](i)}$: normalized $z$ value to pass into the activation function for the $l$th layer for the $i$th training example\n",
    "\n",
    "### Update $\\gamma^{[l]}, \\beta^{[l]}$\n",
    "\n",
    "$$\\gamma^{[l]} = \\gamma^{[l]} - \\alpha * d\\gamma^{[l]}$$\n",
    "$$\\beta^{[l]} = \\beta^{[l]} - \\alpha * d\\beta^{[l]}$$\n",
    "\n",
    "Note: optimization algorithms also work in batch normalization when updating $\\gamma^{[l]}$ and $\\beta^{[l]}$\n",
    "\n",
    "\n",
    "## Batch normalization during testing\n",
    "When testing, each single test example will not have a mean & variance value for forward propgation. Thus, we obtain $\\mu$ and $\\sigma^2$ values by calculating the exponentially weighted average of those values across all min-batches\n",
    "\n",
    "During training, the $t$th mini-batch will produce $\\mu^{\\{t\\}[l]}$ and $\\sigma^{2^{\\{t\\}[l]}}$ for each layer. We can calculate the exponentially weighted averages of these values for all mini-batches and use them to compute $z_{\\text{norm}}$ and $z$ for the test example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e76000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
